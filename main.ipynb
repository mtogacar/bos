{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mtogacar/bos/blob/master/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QaFrvOWeGWoE",
        "outputId": "b72063de-24d3-40f8-bdc1-dcd63b0ea3f4",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import sys\n",
        "sys.path.insert(0, '/colab/brainmrnet/')\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from PIL import Image\n",
        "from albumentations import *\n",
        "from skimage.transform import resize\n",
        "\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report, precision_recall_fscore_support, accuracy_score\n",
        "\n",
        "from keras.layers import *\n",
        "from keras.callbacks import *\n",
        "from keras.optimizers import *\n",
        "from keras.models import load_model, Model\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "!ls \"/content/drive/My Drive/colab/brainmrnet/data/\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oY4ffYPhGdIE",
        "colab": {}
      },
      "source": [
        "SHAPE = (224, 224, 3)\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 100\n",
        "N_SPLITS = 5\n",
        "SEED = 1881\n",
        "TRAIN_TEST_RATIO = 0.3\n",
        "\n",
        "BASE_DIR     = ('/content/drive/My Drive/colab/brainmrnet/data/data/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sl3WnfTa4jYm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uGT_8VyTZt2T",
        "colab": {}
      },
      "source": [
        "class DATASET:\n",
        "\n",
        "    \"\"\"\n",
        "    input_shape           --> TUPLE.wanted image size\n",
        "    batch_size            --> INT.yielding data size for every iteration\n",
        "    orders                --> LIST.which images will be used. max=len(all_images). it can be used for K-fold(CV).\n",
        "    base_dir              --> STR.the DIR which is include images.\n",
        "    seed                  --> INT. This allow to dataset generator to more reproduciable and it ensures that x and y are shuffled with compatible.\n",
        "    augment               --> BOOL. Augment data or not.\n",
        "    train_test_ratio      --> How much of data will be used as test set.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, input_shape, batch_size, orders, base_dir, seed, train_test_ratio, augment=True):\n",
        "        self.SHAPE                 = input_shape\n",
        "        self.BATCH_SIZE            = batch_size\n",
        "        self.arr                   = orders\n",
        "        self.SEED                  = seed\n",
        "        self.TT_RATIO              = train_test_ratio\n",
        "        self.AUG                   = augment\n",
        "        \n",
        "        self.BASE_DIR              = base_dir\n",
        "        \n",
        "        \n",
        "    def get_paths_n_labels(self):\n",
        "\n",
        "        x      = []\n",
        "        label = []\n",
        "\n",
        "        img_paths = os.listdir(self.BASE_DIR)\n",
        "        \n",
        "        for img_path in img_paths:\n",
        "           \n",
        "            if (\"men\" in img_path) or (\"MEN\" in img_path):\n",
        "                x.append(os.path.join(self.BASE_DIR,img_path))\n",
        "                label.append([1,0,0])\n",
        "            elif \"gli\" in img_path:\n",
        "                x.append(os.path.join(self.BASE_DIR,img_path))\n",
        "                label.append([0,1,0])\n",
        "            elif \"pit\" in img_path:\n",
        "                x.append(os.path.join(self.BASE_DIR,img_path))\n",
        "                label.append([0,0,1])\n",
        "                \n",
        "        return x, label\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.get_paths_n_labels()[0])\n",
        "    \n",
        "    def get_img(self, img_path):\n",
        "        img = Image.open(img_path)\n",
        "        return np.array(img)\n",
        "    \n",
        "    def augmenting(self, img):\n",
        "        if self.AUG:\n",
        "            augment = Compose([VerticalFlip(p=0.5),\n",
        "                               HorizontalFlip(p=0.5),\n",
        "                               RandomBrightnessContrast(p=0.3),\n",
        "                               ShiftScaleRotate(p=0.5, shift_limit=0.0, scale_limit=0.05, rotate_limit=20)])  \n",
        "        else:\n",
        "            augment = Compose([])  \n",
        "\n",
        "        img = augment(image=img)['image']\n",
        "        return img\n",
        "    \n",
        "    \n",
        "    def resize_and_normalize(self, img):\n",
        "        img = resize(img, self.SHAPE)\n",
        "        return img\n",
        "    \n",
        "    def get_shuffled_data(self):\n",
        "        img_paths, labels = self.get_paths_n_labels()\n",
        "\n",
        "        np.random.seed(self.SEED) \n",
        "        np.random.shuffle(img_paths)\n",
        "        \n",
        "        np.random.seed(self.SEED) \n",
        "        np.random.shuffle(labels)\n",
        "        \n",
        "        return img_paths, labels\n",
        "        \n",
        "    def split_train_test(self, get):  # get=={\"train\",\"test\"}\n",
        "        img_paths, labels = self.get_shuffled_data()\n",
        "        x_train, x_test, y_train, y_test = train_test_split(img_paths, labels, test_size=self.TT_RATIO, random_state=self.SEED)\n",
        "        \n",
        "        if get=='train':\n",
        "            return x_train, y_train\n",
        "        \n",
        "        elif get=='test':\n",
        "            return x_test, y_test\n",
        "    \n",
        "    def data_generator(self):\n",
        "        img_paths, labels = self.split_train_test(get=\"train\")\n",
        "        \n",
        "        while True:\n",
        "            x = np.empty((self.BATCH_SIZE,)+self.SHAPE, dtype=np.float32)\n",
        "            y = np.empty((self.BATCH_SIZE, 3), dtype=np.float32)\n",
        "\n",
        "            batch = np.random.choice(self.arr, self.BATCH_SIZE)\n",
        "\n",
        "            for ix, id_ in enumerate(batch):\n",
        "                # x\n",
        "                img_path = img_paths[id_]\n",
        "                img = self.get_img(img_path)\n",
        "                img = self.augmenting(img)\n",
        "                img = self.resize_and_normalize(img)\n",
        "                  \n",
        "                # y \n",
        "                label = labels[id_]\n",
        "             \n",
        "                # Store the values    \n",
        "                x[ix] = img\n",
        "                y[ix] = label\n",
        "\n",
        "            yield x, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bJ-aVohsqt1F",
        "outputId": "292d715b-1ef5-4430-f8b8-413cfaa1cc94",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "dataset = DATASET(SHAPE, 1, range(4), BASE_DIR, SEED, TRAIN_TEST_RATIO, augment=True)\n",
        "\n",
        "for ix, data in enumerate(dataset.data_generator()):\n",
        "    img, y = data\n",
        "    print(img)\n",
        "    print(img.shape)\n",
        "    print(\"-\"*10)\n",
        "    print(y)\n",
        "    print(y.shape)\n",
        "    print(\"-\"*10)\n",
        "    print(img[0,:,:,:].shape)\n",
        "    plt.imshow(img[0,:,:,:])\n",
        "    plt.show()\n",
        "    \n",
        "    if ix==0:\n",
        "        break"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[[0.         0.         0.        ]\n",
            "   [0.         0.         0.        ]\n",
            "   [0.         0.         0.        ]\n",
            "   ...\n",
            "   [0.         0.         0.        ]\n",
            "   [0.         0.         0.        ]\n",
            "   [0.         0.         0.        ]]\n",
            "\n",
            "  [[0.         0.         0.        ]\n",
            "   [0.         0.         0.        ]\n",
            "   [0.         0.         0.        ]\n",
            "   ...\n",
            "   [0.         0.         0.        ]\n",
            "   [0.         0.         0.        ]\n",
            "   [0.         0.         0.        ]]\n",
            "\n",
            "  [[0.         0.         0.        ]\n",
            "   [0.         0.         0.        ]\n",
            "   [0.         0.         0.        ]\n",
            "   ...\n",
            "   [0.         0.         0.        ]\n",
            "   [0.         0.         0.        ]\n",
            "   [0.         0.         0.        ]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.0047619  0.0047619  0.0047619 ]\n",
            "   [0.00392157 0.00392157 0.00392157]\n",
            "   [0.00392157 0.00392157 0.00392157]\n",
            "   ...\n",
            "   [0.         0.         0.        ]\n",
            "   [0.         0.         0.        ]\n",
            "   [0.         0.         0.        ]]\n",
            "\n",
            "  [[0.00532213 0.00532213 0.00532213]\n",
            "   [0.00392157 0.00392157 0.00392157]\n",
            "   [0.00392157 0.00392157 0.00392157]\n",
            "   ...\n",
            "   [0.         0.         0.        ]\n",
            "   [0.         0.         0.        ]\n",
            "   [0.         0.         0.        ]]\n",
            "\n",
            "  [[0.00532213 0.00532213 0.00532213]\n",
            "   [0.00392157 0.00392157 0.00392157]\n",
            "   [0.00392157 0.00392157 0.00392157]\n",
            "   ...\n",
            "   [0.         0.         0.        ]\n",
            "   [0.         0.         0.        ]\n",
            "   [0.         0.         0.        ]]]]\n",
            "(1, 224, 224, 3)\n",
            "----------\n",
            "[[0. 0. 1.]]\n",
            "(1, 3)\n",
            "----------\n",
            "(224, 224, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD8CAYAAAB3lxGOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nOy9aYxk2XUm9t3Y9y33rKqszFqa7IWC\n2BRJUZ6xpRaoXZZGNKTRD3s2QB7AAmzDgEeyDXgwhgF5vAwMGDAsQ8LMAPZoxtBQEqihRLLFVSSb\nXdXN7q6urjVryaVyjcjY93j+kfXdOnHqvsjMyqruJDsOkMiIeO/d7d2zL9d4nocxjGEMH14IfNAD\nGMMYxvDBwpgIjGEMH3IYE4ExjOFDDmMiMIYxfMhhTATGMIYPOYyJwBjG8CGHZ0YEjDE/Z4y5boy5\nZYz5nWfVzxjGMIbjgXkWcQLGmCCAGwA+C2AVwOsAftPzvKtPvbMxjGEMx4JnJQl8CsAtz/OWPc/r\nAPgjAL/yjPoawxjGcAwIPaN2TwFYEd9XAXza72ZjzDhscQxjePaw43nelP7xWRGBA8EY81sAfuuD\n6n8MY/gQwj3Xj8+KCKwBOCO+n374mwXP834fwO8DY0lgDGP4IOFZ2QReB3DRGLNkjIkA+NsA/uwZ\n9TWGMYzhGPBMJAHP83rGmN8G8JcAggD+0PO8d59FX2MYwxiOB8/ERXjkQYzVgTGM4f2Ay57n/Zj+\ncRwxOIYxfMhhTATGMIYPOYyJwBjG8CGHMREYwxg+5PCBBQuNYQw/7GCMGXn9JBjlgTERGMMYnhgO\nQnLX/RLx9fcPCsZEYAxjEKAR+yBE1fe77j0qsXi/YUwExvChBD/EdCH9YTm4H7HwPO/EcH0XjInA\nGH7ogQjvh/iHQU6XhOC6RxOLo/TxQcGYCIzhhwaIdIFAwCId/48SyZ+GuP6kSH4SJIQxERjDiYfD\ncF15nx83Pkp/uu0nafOwyP1BE4IxERjDiQFjzJEMc++Hwe0oiHzQc37qwVgSGMMPHUhkJhK7EMCP\nw8t7niYcRjXQ43hSLn0U+8NYEhjDDzxoDh4IBI7sWiNognHY/kdJC0dBftd4ngT8VJWTaCwcE4Ex\nHAk0wkvOLTc6CcFgMMBgMIDneU4R2M/qrts9DsJ80JzWD07KuJ44d8AYc8YY81VjzFVjzLvGmP/8\n4e//2BizZoz5/sO/X3h6wx3D+wFEdCKz/JPXtNhPIOLyGdnuQeCnK4/y6x/U3gcRwKPXh+PwG88H\nCceRBHoA/ivP894wxqQBXDbGfPnhtX/med7/cvzhjeFZgQt5/fzpfqItgS45Ij//gsHggYE3o65r\naeEwxrfDwkHP6nEch2j4eRu4Th90MNETEwHP8x4AePDwc9UY8x72S42P4QRDMBgE8LjxToLk+IPB\nAMDjXFm2w/+DwWDIVx8IBDAYDBAKhaxKwHsOIiyHAT9V4mnE50uO/aQEYNQ4TlIo8VOxCRhjFgF8\nHMBrAP49AL9tjPlPAFzCvrRQehr9jOFwQFE8GAwiGAwiHA5bJI1EIgiHw4hGowiHwwiFQkOGvMFg\ngGg0an8HgH6/b9sNBoPo9/sIBAIIhUKWGACPi7y9Xg+dTgfAPlHodDpotVpoNptot9totVpot9sj\n53GUObvgMMbJZ82B/YjtSYFj1xg0xqQAfB3A/+h53r81xswA2AHgAfgfAMx5nvf3Hc/Jcwc+caxB\nfIiBHDcSiVjkjcViiEQiCIVCmJycRCaTQSgUgud5CAaDMMYgm80imUwiHA5bcb7b7Q4Z9YwxiEQi\n8DwP7XYbnuchFoshFAqh2WwiEokgFosNEQUAqNVqMMag3+9jMBggEAig2WxaolCr1dDtdlEqlbC2\ntoZqtYp6vW4JxqgYfNfn4yCz53lDEYZPEw7i/KMCkp4ROGsMHosIGGPCAL4A4C89z/vfHNcXAXzB\n87yXDmjnZFlKTiCQs5N7RyIRpFIphMNhJBIJhMNhTExMIBKJYGJiArlcDr1eD8lkEjMzM4jFYuh2\nu+j3+2i32/Y+AOj1egiHw2g2m0gkEohEIhaBW60WwuEwer0ejDEIhUIwxqDZbMIYg3A4bNvlM71e\nD91uF4PBAMFgEIlEApVKxRKRer2OZDKJarWK7e1tlEol7O3tYXNzE+VyGbVaDc1mE61Wy0ohwNMh\nAn4qw2GfPwq41IlRfXxQROCJ1QGzP7M/APCeJADGmLmH9gIA+FsArjxpHx9mIHePx+OIx+NIp9NI\nJpOIxWKIRqOIRqNIJpOIRqPI5/OIxWKYnJy09yYSCXQ6HSt2h0IhLC0twRiDVqtl+yHyBoNBRCIR\ntNttrK+vIxgMYmJiworxjUYDvV4P7XYb8XjctlOtVlGtVi2nZ1skBtFoFPF43NoWMpkMgsEgQqEQ\nEokEFhYWcPbsWfR6PZTLZZTLZVQqFezt7aFYLGJnZweNRgP1eh3dbtf2oeEwiOxnH9DuzaeFjGzr\npIcPP7EkYIz5GwC+CeAdAIOHP/83AH4TwI9iXx24C+A/FUTBr60PvSRA8TscDiMWi2FiYgKTk5NI\npVKIxWJIJBJIpVJIJBKWGMTjcWQyGavf5/N5K6KHQiFcvXoVX/ziF1EsFhGLxfDxj38c+XwezWYT\nnU4HpVIJ/X4f0WjUcv9er4dSqYTBYID5+Xkr6pMYbG5uIpVKIZlMWhWjXq+j1WohEAggHo+j0+lY\nIgbA2hYoeodCIXS7XXieh2q1img0ikKhgGw2OyRZAECxWMTe3h6q1SpKpRKKxSJ2d3dRr9fRaDSG\nJA7aNLieh8ktOMy148BR23vGRODpqwNPCz5MRMBlJCLyZLNZTE5OIp/PI51OIx6PIxqNIhaLIZVK\nYX5+HtlsFtFoFOl0Gp7nIZ1OW12+Xq9bBO52u7h8+TL+8i//ElNTU5icnEQul0MikYAxBtFoFO12\nG4FAwBIaIlM6nUatVsPU1JRF2nK5jLW1NUxNTSEYDGJjYwMTExPIZDIIBAIWeak20P4gjY4SMQeD\nARqNBu7fv496vY7BYICJiQlrp6A9Ih6PIxwO23VqtVp2LBsbG9jc3ESpVEK1WkWr1UKv17PrepS9\n/bSJwFGlAAnPECefrjowhsOBFjO1Xz6ZTGJqagpTU1PI5/OIx+NDnJ9EIRaLYW5uDuFwGK1WC/F4\nHL1eD7u7u9jY2ECtVkOpVEKpVLJI1Wq1sLi4iMnJSczNzSGXy6HT6aDb7QLYR6put4t2u41KpWK5\nbzQaRaPRQKlUQigUQjKZRKlUwsbGBjzPQzKZxO7uLowx1mCYz+etrp9Op3Hv3j2Uy2Xk83kUCgX0\n+33rTaCHIpFIIJfLWclkMBhYAtDr9awngbYQShbT09OYnZ1Ft9tFvV5HsVjE9vY2NjY2cO/ePatC\nPK13d9xoRQkngelqGEsCzwA0ouvNZIxBPB5HLpfD9PS0NdxRz5+YmMD09DRyuZy1BdCKv7m5iWKx\naC3qe3t72N3dRafTQa/Xs6496t2VSgUAkM/n0e/3LXJwLIFAANFo1FrmSQhIREgwCDQM9vt9JBIJ\na3DMZDLo9/uWaK2srGB3dxehUAjz8/NWYiHRCAaDiEajyOVyVh2hd4GxBe12G/V6HZ7nodPpwPM8\nFItFNBoN692gahSLxVAul/Htb38bvV4P9+/fx8rKilUZ5Hvx2/NPC2FH2R4OA++3JDAmAk8BJKLz\nu597KBAIoFAo4MyZM9aaH4lEkMvlrFV/cXHRGtNisRiazSZu3bqF7e1tFItFtFoti7SuiD8phtIX\nn0qlAMDq8dSdKcozmKff7z8WAET1ggRkMBig3W5bzt3v9+F5HqLRqFVtEokEgsEgut0uotGofY7P\nlMtltFotZLNZpNNpq1rEYjE7DhLGVCqFZrOJer0OYwxqtZoleu12G/1+H+l0GtlsFrdu3cLZs2cR\nj8dx+/ZtrK2t4fbt21hdXUWj0ThQRPez5GsC4hdINMoTcBDx0W0/AxgTgeOCX7CHRkRGyulNRK64\ntLSEbDaLYDBoDWznzp3D4uIiut0uJicnEQqF0Gq1UCwW8d577+HevXsW0cgZ2Q8t5gzmMcYMuQNr\ntZqVDBjEwzHxd62/c2wy9p9IzP6ka7DRaKDdblubANsLhULIZDK2v3A4jKmpKesNiMViAIBCoYBI\nJILBYIB0Oo1Go4FarWY9HfF4fIgwUGWgB6Tb7SIejyMQCFijKcdRrVZx69YtvPfee7h58yZKpZI1\nTPq9S6nTH9cFeYLsAmMicFg4KLJrFNcHHlnDJbdNJBKYnZ3FuXPnkE6nEQwGEY/HsbCwgFwuh3g8\nbpE/Ho/jzp07WF5exoMHD1AqlRCPx60oTklAxuzTek9XHSWAer1ug3fC4fCQhV56JPidRMbzPCul\nBINBK3JTZ+f8+EdiQC7e7Xata7Hf71vOTaJDgx/tH3Nzc8jn89ZjwHlWKhU0m80hlyg9J/F4fChC\nEdhHnnK5bAkZ1QZgXypaW1vD1atXcePGDayvr6PZbD4WGq1VuKPiiN4Tmhkcpr0xEfgAgRybG5q/\n+d3rWj9JBKLRKKanpzE/P49cLodsNotAIIB0Oo35+XlMTU1Z11+hUMDW1hZWVlbwzjvvoFwu2/EA\nQLfbtWJ5t9u1Ojs5IgkAuRd1d/ruZdQgkZkuQv5JQkNJA4B1Q1Lk18FLwCN7AhGHLkdKCnt7e5YD\nt9tta/VnBGM+n8f58+eRTqftXOr1OqrVKnq9HiKRiDUWxmIxzMzMYG5uDtls1koRHBODjhhBmUgk\nkE6nEY1G0e/3USqV8N577+HKlSu4ffs2KpXKY7YDgp/EcBTcOYwd4rjSwyFgTARUn76/Sw476nk/\nis+20+k0zpw5Y0N3w+Gw1YELhQLm5uasVADsh9u+9dZbWFtbg+d51u3GYBkiRLVatbow1Q6OR/5R\nzCeytVqtoXnTFkCLPeMLYrGYlRAogTD0l2J4t9u1hIQxAZFIZIjwSO4siQvbY3xBo9EY8p7k83nk\n83nkcjnrFmSwkufthzfXajX0+31LmGZmZjA5OYlAIIAzZ84gGo2iXq9bTi9jMJLJJFKpFILBIIrF\nIpaXl3HlyhVcvXoVa2trj73nUWqDfO9+zx3ETFztjInA029/5O9aXJPBLaMouB+xoMj98ssvY3Z2\n1kbHZbNZnD592kbihUIhaxu4du0a3nvvPZTLZWuZ39vbQ61WswSAOrgMiNH+aKnTynwCRvExqIYI\nRzuAbJsuPGBfAqAUQU8CxXDgkcpDiYTqA/Vxfpb5DfxeqVTQ6/WsnYEuy2KxiG63i0QiYWMHGGBE\nDwi9DJSCpNtzdnYWCwsLmJ6eRjqdHop4pLQSi8Ws98XzPDSbTWxsbOAP/uAP8ODBgyEi9iRwGDvC\nk147Bnz4iIArMOewz7iMexokQuk28vk8/ubf/JsIhUJIp9PI5XKYn5/H3Nyc1b3z+TyuXLmCu3fv\nYnNzEzs7O2i1WtjZ2cHe3p4NfqEILXVx4NFGIZKSmxPxKapr+wF99pIIEKGow9OyD+wjOIkW2yRR\nCIVCyOVyNmaAEYYcL8cl76fq0e/3rQuT9weDQfR6PVQqFdTrdQQCAUxMTFi7BG0QHLNMVW61WqjV\najbfYWJiAufPn8eZM2eQTCYtYtOwStsDYygAYHl5GV/72tdw6dIlVKtV5/45Ds5IYj0mAnIQT4EI\nHMZyPwo0J6Xriwh3UNvcwNlsFtPT0/Yvl8vZiL3p6Wkb8BIOh1EsFnHp0iXcu3fPBrzs7e2h2WwO\njUun8hIhAFgjG41mFMkBWM5PHzu5brfbte2QCBBoWwBgkVomBslqQUR2Zi1Go1FrpZfrR1WCRIRE\nMJfLWQ4vkSKRSNh043a7bdUUEhWZ7cjPlDiMMTZTsdFoIBAIIJ/P48KFC5iZmRkKdzbGoFKpoFwu\nIx6P49SpU8hkMqjVarh8+TL+6q/+Cvfv33fuAb1fDotHR3nuGeDmDw8RcCH2KN3tIKrrEqllIYyD\nngX2E2NoqGIMf6FQwNLSkhWj5+fnbVDLlStXsLKyAs/zcO/ePdy5c8dm2rnci4wgTCaTFtHJESm+\ny41Fjsl5sB1a8onAJBTyuWq1OsQ16VXgs1R3yFGJiOyfEkkwGLQJTNT7ZZxCLBbD4uKinRMJHo2P\n9Xp9qBaCJBIE2gharZa1fwwGAxuBSDcp13B+fh5nz56175wJTcViEZVKBc8//7wlsPfu3cPXv/51\nXLp0CbVazXcPcByHAf1eRz07JgLD1y33Och1M4oQjLquicBhjIPAPjfMZDJYWlqynCadTiOdTts8\n/r29PZw/fx7nz59HrVbDt7/9bSwvLyOXy2F7extXrlwZQrx+v29160QiMZQ0REu8a7x6fbQEQdWA\nxINGPqnWeJ6HWq1mPRtUBdgOkZFSBACbxEP7BSUO2hUY2KRrGgCwXoFCoYBkMmnVEEnkZL8kQEx4\nolQRiUSQTCZhzH4wEdOR2Ua/37feFEYaZjIZTE1NYXp62kY+Mj9ifn4ekUgE5XIZr732Gl599VU8\nePDgWAjrUhtHPT8mAo+uDSGlrmILjKau2i5wWIvtYaQHRv8tLi5ienraRsqdOnUKp06dsj7+iYkJ\nfOITn0ClUsF3vvMdvPPOOwD2XWp3795Fq9Wy4joRn+G0NMxpsZ0gPQPAcL0/BgVR9Gefcu5a1O31\nemg2mwiHwwiHw5ZoyPtlf/IdDQYDVCoVG9Is26bBUc7DGIOXXnoJp06dspJDMBgcqiWgpSJp1OT/\nwWCATCaDfD5vpQjP289u3NraQqvVGpoDDYWdTgfZbBYXL17E7OysXeuVlRWEQiEsLCxgYmICxhhc\nuXIFf/qnf4rbt28PpTJrBjIKRhEBv/ueMn6ebCIwSneX16gbylLWBBcx4Gf5slwv47C6He9jgMvs\n7CwymYx1/U1NTeHMmTPI5XLY29uzIuf29ja++93v4ubNmzZklmGs0miWTCbtZpbzkoZBPT+J1LK4\npxTdpSFNEw7ZNkV8YNg4ynbYD3+T74CEhi5NujKBR/aJQCBgkTuVSuFzn/uczWzk7+1223pJ2Afb\nAGAlDUoU0iVJr0I6nUYqlbLGzk6nM3Qf7QmsaMQkq/PnzyMWi2Frawvr6+s4e/YsLly4gGAwiDt3\n7uDzn/883nrrraF4DAlPquP7tfN+EIEfuCzCwyKsiyD4PXcU7wHF/zNnzmB6etqKlvl8HhMTE5YA\nsIhHsVjEm2++iTfeeAP3799Hq9WyVm+G5dLnnkqlkE6nrc4vkVtyPj12iZzSKCiRlfdJ7k+iIaWD\nYDCITCaDarU65D7U7cvxECE6nQ7i8Tjy+Tyy2Sza7TaazSaazSYqlcpQpSCqJ8lkEvF43EoBzJAk\nAWLUI+MFPM+z8Q7NZnPIHkH1g7EU4XB4KPSZcQ+cL5/r9/uo1+u4evUq6vU6stkslpaWkEgkbBLS\n888/j+eeew6/8Ru/gUAggDfffPMxr41cFxccZZ/J9X3WjPoHgghIZNBc7LDP+70ovalHAavtLCws\nYG5uzkaizc/PY2JiAtls1gYFRaNRLC8v4/XXX8f3v/99m97KPqjHNptNSxBoudYgx+iat0tVIBLx\nmubi8rlAIDAU0iszEaVI71IDNLGV0YkkbL1eDxMTE6jVaqhWq9YN2ev18Oabb+Ls2bOYnZ21Yjzj\nEIwxVqJgmLUxxvr8GTtBaYGBThwLDYMkZLFYzEpsJL6MVZiZmUGn08HW1hY2Njawvb2NF154Ac8/\n/zxWVlbwxhtv4KMf/Sjm5+fxuc99DsYYvPHGG0P1C+ReO6p6cJT9/LTh2ETAGHMXQBVAH0DP87wf\nM8YUAPxrAIvYry70695TqDjMly2TWtRYHrt/FLjEaz/DYTAYxPT0NM6dO4dCoWB9/6lUCmfOnEGh\nULDJK7FYDDs7O/jCF76A27dvY3d31xrYmF3HpBfP82wGHVUdAEO6t+bArvFpBNWlveklkL/L5yRB\nkLo2ACvCyypDcr34XhhDQKOkDDsmQWBh0t3dXbTbbVy7dg3FYtH68pnVSENpPB63kgQlpHq9jn6/\nj2azOaS+yPwHBhbV63UEg0Gbecg4BXpUqMLQtcn+tra2UC6Xce7cOSwtLWF7exvvvvsuPvrRj+LU\nqVP4tV/7tSFCcFwbk0vKe7/gaVQbvgvgxzzP2xG//VMARc/zfs8Y8zsA8p7n/aMRbYy0Cah7h4xc\nwMEFHSUnlHYBKWZrC7l8NhKJYGZmBmfPnsXk5KQlAHNzc0gkEpiamkImk7Ft3rt3D6+//jq+/OUv\n2/RXGp0Y6TYYDKzbL5/PD1nM9bhdxEDeA2CIMMo5EOnJ1aVPnn1K3Z4uORmrwIQgtiVLjcvkIOYX\nSIIi6xPQYBiPxwHA5gHE43GkUimkUikbeETR3/M8ayhkn7QzcC0Zz0C1ghJVLBazUgPDr0lcyExo\ni2GQFdeUFZ2CwSAmJydx8eJFhEIh1Ot1K7ns7u7ij//4j3Hp0iVLiFz7j23q3/zU0oPuOwa8rzaB\nXwHwkw8//wsAXwPgSwSOAtywR6Wafl4B+btLvA2FQpidncXS0pLV+/P5PFKpFE6fPm03XSQSwebm\nJpaXl/HGG2/g9u3b2NvbQywWs6GpjJunyMp4Au2Ck+PQHFvbBiQnl8/wu4wBADCkIvC7vNbpdB5T\nkaRNgUFDkhhL6UAfSiLnlkwmbTASr0ujYLlcBgBbNQmAzZgkx6cExbRluSdoLGRyEt1+JBaUDqTh\nNBqNIpPJYHp6GplMBsYYWwq9WCwCAHZ2dtBsNnHu3DkkEglcv34drVYLCwsL+MVf/EUAwOXLly1x\nciGtn4R5EuBpEAEPwJfMvpvv//I87/cBzHiPiotuAJjRD5nhcwf4m+9iyd81tz7UIBXnd13XEIlE\ncPr0aSwuLiKbzSKbzaJQKFi/cjqdtuW419bWcPnyZbz99ttYX19HqVRCOBy2BKDX6yGXy9mkmG63\nazmqyxKspQD+5tLL5bNS6tFr5T30RMi29H2acGjLvyTAJCi0IVAKkO1LIkOpStYioIjf6XSQyWRQ\nLpdtZGKv17NI63neUA4Fx0QCRGLKdsn5pQpEu4EcfyKRsEQ9l8thZmYGg8EAFy5cwObmpj0X4ebN\nmygWi3j++eexuLiI5eVldLtdnD17Fr/6q7+KTCaDr371qyMPUzkInrUB0A+eBhH4G57nrRljpgF8\n2RhzTV70PM8zjjiAh8Ti94GD1YGjLM4oY8tBhEA+xzDSs2fP2ow2hpVmMhmr35bLZVv04/bt27hz\n544VOVnXn/HvrOkvrfPSry/Hp/VuHQzkmrOUErRko20Drmua40vEke1LICGQYch6rDL6kO1HIhHL\n4VnTkCoHALtOJASsT8DoS94jQ5EpbVGVkEFFnI9cV0YzyqSt+fl5rK6uYmpqyoaAt1ot3Lx5E81m\nEzdu3EAmk8H58+dx7dr+Vr9w4QJeeeUVtFotfPOb3xwq2qL35Cg46v1PC45NBDzPW3v4f8sY83kA\nnwKwaR6eP2CMmQOwddx+RnG/EWN77NmD2mau/9zcnM34YxXgubk5TE5O2vDU1dVVvPvuu7h16xaK\nxSI2NzdtQszExMSQuBmLxYaMbi7x3vU3arxaOpKfXSK9qx3N7aVIz9+1RKJtJ3q88rNUU+g1oHGQ\nKcwMGWbYtDH77r5cLmej/IwxNsWZfzREyujFeDyOSqViQ4VJCGRNBmlLotuStgkSIxIdBmw999xz\nWFtbQ71ex7vvvotUKoWPfvSjlijk83l89rOfRa1WwxtvvOEMKHK9K9c7djGvZwnHIgLGmCSAgLd/\nIGkSwM8A+CcA/gzA3wHwew///+lxByo3ttZ/1Zh8DSsHSQDk2KdOnbL56dlsFnNzc5ienrZegX6/\nj83NTbz22mu4ffs2dnZ2UCqV0Ol0bGkrWpkLhcJQ3r0c52EQX85bz8VvjqMQVt7jApeNQYrQAB4T\n8fW70fEI8l4irfwM7HP1er1ui4GwuvJgMMDt27ctEaWBk0SAKgj7ZMTg3t6eVTdkRSQiN12Hxhib\nqNTr9bC6ugoA9qi0bDYLz9uvspTJZFAqldBoNPD666/jlVdewUsvvYSVlRV0u11MT0/jl3/5l1Gp\nVHDz5s3H1vwwyOwiEId99knhuJLADIDPPxxsCMD/63neXxhjXgfwb4wx/wDAPQC/fsx+APhTRBeS\njLpHfyf3m56extLSki36OTs7a9WCQqFgy2pfvXoV7777Lq5du2ar//Z6PRvs43keGo2GtR1oBKXe\nTZHV5fI8SHqR4jUwHJGnObxWOzQRkFF8xhhr+NNSgQzdlmvtkmTkPdLzQmlArgfvZxkwFgwJhUI4\ndeoUSqV977J0cwKPbAJSEgBgC6HQEEsiplUlac9gVOHt27cBPKqkxHRj1jmcn5+37sPXXnsNn/nM\nZ7C0tIS7d++i3W7j3Llz+Pmf/3nU63VboMTFqOR7dMFB7/9pwg9E2LAGzWkkjBK19OJzk9P6Pzk5\niZmZGWQyGRsUxLx/AHj33Xfxla98BXfu3MH29jYqlYoNIGJdv2w2i0ajYSsHy75dkoBGTj993YVg\nen7S8i+fI+LIjEE+K/V1vb6yX9mWnIc0FBIhadhju1Jy0JKJHCe5M+sjLiws4MKFCyiVSrhy5Qo8\n71H8gVwL7WHR7cs/fVIypYNqtQrPe5Q8RQMf+6ErmPujWq1iY2MDqVQKFy5cwI//+I+jWCzi1q1b\n+NjHPoZ8Po+vfe1r+PM//3Ps7e059+eo3+Q+8VPnngB+OMKGgaNnEI6itizxzVr/rJE/NTWFXC6H\nWCyGXq+HK1eu4NVXX7XBP/V6HbFYzBoKa7UayuUySqWSjf7TRMdl+PPjCprIuZDSb66aiIzaRC61\nwi8YS6smrvFIoiLb0nNxEQMSGKYKb21t2YSqfD5v3YXsT5ZBpxrgUoVIXGR6tZRwmIvAIqae5w0d\nnc6Eo83NTUus8vk82u02SqUS7t69i3g8jh/5kR/B+fPnsbKygkwmg5/8yZ9Eo9HAl770JdTr9aG5\nj3p/R712XDjRROCgiUtR2A9cBjP+Zwbg5OSkPQuA8eZTU1OIRCLodrt455138Oqrr+L69etWJ2S+\nAOPRKU62221MT08PFa6QG5oYZ+sAACAASURBVJ3iv5YK9Gdt3NPEQnNjec2FrPIaxXLdvhyvtAn4\nETO9trJffWiojD50vR8iK/vjmu7s7NhoSumC5BhpYHSpIQQ5F0YJaqLAEnA0WtI4yGrNzIHY3Ny0\nXg1KgHQhBgIBPP/88xgMBrhx4wY+/vGP46d+6qewvb2N73znO86DVPX6utb1WcOJJgJ+CyA32yh9\ny+9ZRoGRALDqz+LiotVNeVbf5cuX8dWvfhW3bt3C3t4eOp0OkskkCoWCTUYhx5DiqkQUbljNpfhd\nIohLNdCbxM9gJJHW73e9VrptV58u8d3F4WWbLgnEJaEQ6A4E9nV92lY8z7Ohv3pcrNGo7Rx6b8g1\nlq5MGc3Img3yGe2CbDQaaDQaQ9WcCoUCjDHY29vD+vo64vE4PvKRj+Ctt97CjRs38OKLL+JnfuZn\nsLa2hjt37gyt1SjV9f2EE00EDgK9uf2+a2AOwPT0NPL5PGZmZlAoFGweP4/veuedd/C1r30N169f\ntyWpk8mkLXHNdGD6t6PRqK1zl06nH9PjXTH3OriGiCJ1bc3Z9e+S07nukd/52U9ikMjNtnu9Hvb2\n9mwST6/Xs6G/Ms3X8/ZdbrLYqO7b9f7IkclhZZFT+vxJXOUctATjmh/XUuY98I/XWTyVGZWyH0qG\nfFesYcgQ6EQigdOnT+PGjRvY2NhAIpHA5OQkTp06hXv37tmYgs9+9rP4oz/6IxsVOQoO2r9PG35g\niYCk/n6cTgNVgKWlJVsDcGFhAclkEjMzM/aors3NTbzzzjv41re+hRs3bqBSqWAwGNicAWOMPRqb\n48jlcjakVUapSf1fj1H/5trUrvx/fa/8rtsepRq47tPcmhV65SEdPECUlYo8z7PJUK1Wy0pSMhZf\njlH+AbB9SJWA7j3mMEjJQa6pllikmE/k15GNco2NMUP5DrymKyoxuYkSS6VSwe7uLoD90OeFhQUs\nLy9jY2PDqpazs7NYX19HKpXCpz71Kdy7dw+vvvqqjXvweyfvN/zAEgGCFqkBfwMhq8/Ozc2hUChg\ndnYWZ8+eRSwWs1mAGxsbeOutt/D6669jeXkZtVoNnudZIyEP0mCxTh4cEo/HLRHQQS2usRxG1XE9\nfxB39btPEwMtCbiIikQiHujBYBueFkxDHTkuw3xJvKREoK36vJ+pxQBsIVZKG5QKZATjqLlKCYBE\nRZ6wxHGwTZnp6HoXbJfj6vf7NgahUqnY9z45OYlGo4H19XVsbW0hHo8jnU4DgE1L/tmf/VmsrKzg\n6tWrh0L4UfvkacIPDRHQOqaEQCBgT7c5deqU9QJEo1FbDzAcDuPu3bu4dOkS3n77bVv4k1yep+qy\n4CSr16TT6SEOw6O0ZGUgwJ2gJBFRIyjn5lIXZHsu0dFP2tBc/qB1ZNYhi3cwtFYG+/AcAQIJB4Nv\niHyyEg+f43dmAUoLvzTaSWKiD1zR9iGpAlC1oE7vUiHkGnP8XA/dljHGSjiNRsMWiGHfc3Nz6HQ6\nWF1dRTAYxOLiItLpNDY3NzEzM4PFxUX83M/9HLa2trCzs3NkBH9WROHEEIHDTtCPs456PpvN4sKF\nC5ifn8fs7Czm5uawsLCAcDhsLcKrq6u4dOkSvv/97+PevXuoVCq28g3r3lFEJZWnHsl+ZTFPF8d2\nGclc4ryfHu/H/UcRBt2+1IXluLR4zXh9In8oFLIFQogU8oBTrX8Dw3YF9sdrtNLzLAEaZCW31txf\nelU4DpedRapjsgqzJgJ6/XS9Bb2f2BfHQC8CKx0ZYzA7O4vl5WWsra0hGo3iwoULMMZgeXkZhUIB\nL7/8Mm7cuIG/+Iu/GDr23bV3NYF7VnBiiMCTTvKg59LpNJaWljA3N2eryObzeXuUdTAYxObmJr73\nve/h7bffxurqKmq1mo1dB2Ar6JL7M3VYegEA2JwCisws1T1K9Hfp7IcR+Ufd42pXSxT6frn5JTdk\ndB8DjZjY4zrAhKoSawvIepDaiMk+aPgjceVYSVzkd9mOPJGIRExKOZrDS+JHAqPDmuV68D/v5fNS\nsmAbDEVmUtnMzAwePHiA3d1dJBIJXLhwAVeuXMH6+jpOnz6NV155BdeuXcOtW7ecBF8T6INUv+PC\niSECTwpyQ2mIRqNYWFiw2X+nTp0aqglgjMHW1hbeeustvPPOO1hZWcHe3p4NWGG13V6vh3A4jHw+\n/9jhFfIlkggQUSTXcRX9dKkGhINe/GG5hJ8EoS3t2sovxy/danyW0gF1ds6LB4/qQ1CZqCOjAiOR\niA0MItGQ+rqOAdABR/yN9hk5Pr2W8nlZcNRlR2AbbE+elUACRCLT6/VsHUTGjczNzWF3d9dGFS49\nPIr+5s2bmJiYwNmzZ/ETP/ETWF1dtUTwsPAsVAJ3aNgJBZdYB7gjCIPBIE6fPo2FhQUUCgWbFDQz\nM4Pp6WkAwIMHD/D222/j0qVLuH//PqrVKgaDgfUPy4o52WwWqVTKbno5Jolk9CHL8FzJlSQH4nU/\ntcA1VxdoYuJ6Tm921/2u60R0JuLwLx6PW1cgI/Ao3lNaokGOZyewLSJ8u922gTqsH8ix+K2XfN/s\nU54xKNdbEhBp5yChkxxfSgn8Y/vS5kOJh3sMgLVTUAKMRqNYXFy09pTl5WVMT0+jXq/j/v37CAaD\n+NSnPoX5+Xnfd6rhWaoFPxCSgJ8dAHg8go0vampqCouLizbzb2FhwdYFCIfDuH37Nq5evYo7d+7Y\nCjLk4NK95HkeksmkLQYySlSjuMisNVmlVxq8DkJseV2K5vJ+HY+vcw50DoEWO/XGJxAB5EbXyKKJ\ni7xGKYpEtN1u29Bf3hcI7Bcg4QlDJASu6D+JvHoOfkxBqwLaXuFHaF3vQKsNJDiUZkiEKpUKYrGY\nZTDT09MolUpot9vY3Ny0AWobGxs4d+4cFhYW8NJLL2F1ddW6WWXffirKs4AfCEnAxaEk6BeYz+ex\n+DAakEkfNGyFw2Gsra3h2rVrWF1dtb5enu5D6s2qNORs8ow/V58SmCMPPB4m68flXRtYi/xyI0tC\nJdt1SUguvVPq/+xTZuTJlF8Xssln5HPyoJNIJIJ0Oj0kQUipgtIEy60dFiRRJUFxlTrTayClgIPm\nI9eH6pzcFzxrkfkDm5ubqFaraDab2NnZQafTwdTUFIB9Ara5uWndoTz78OWXXx5KMtNzfJbcX8IP\nBBEguF6sRsZ4PI6FhQXMz88jl8shkUjg4sWL9jDQbreLGzduYH19faiYJX3/FPdI7TOZDBKJhFPH\ndKkjLg4tn3F91rkEcq563noj+xFHF9fWoOPsJTL4jVevgWtNSBDYP41psqQ5I/P4uwtZD5oLx0oj\nrbbe63ek100jP/CosIpWF2iEZMk4Jo4NBgOUSiWUSiU0m00kk0nU63V4nmfPjmSBEyad3bt3D4FA\nAC+++CJeeOEFS8Bc71DDKObzpHDiiYCf6OwScwOB/YNBZmdnbTlwVvaZmJhANBrFzs4Otre3baVb\n4FHEmvT5AvuuRboINWik5XMkIlKE1GPUHEj+DgxXUZZqiW5HrpEeh9z0oySpgwjVqHG63oeeowx/\nZplvVgti3r+LCOpxyfWVc3RJOfo+AiWVUXP1A5ZMZxWhfD5vM02pGty6dQtbW1tIJpNWikwmkzay\ndHd3F4VCwZ6EPDs7i1deeQWZTMa3Xxc8bULwxETAGPMRY8z3xV/FGPNfGGP+sTFmTfz+C8cZ4ChR\nyGUMJPJT9J+enkYqlUIwGES9Xsfy8jJ2dnbw4MEDLC8v486dO2i320in07ZqTSgUsiHC5FIuruEy\n6En3lrxfP+/iuGJtLSHRwTFaZPUjKnIsmstqBPITl11cU6+/tKHI3/ksDxkBYAt9VKtVRKNRpFKp\noTnJ8UkEleOVpyXJMejxa9uIVFVchEwSLf07n5UGQuZRkFEMBvsHsl67ds2+s1AoZM+UpAeh1Woh\nGo2iVCrB8zx8+tOfxmc+85nHDp151iqAhCcmAp7nXfc870c9z/tRAJ8A0ADw+YeX/xmveZ73757G\nQF2guUI0GsXk5KQ94IM5AeFwGLVaDVevXsWVK1dw586docIg9HnTak0qL0NKNRLolyQRDxgutuF3\n32EouuZSLq7lQnSXgVMTAgBOQuGan2s8eixa2iAyDwYDm5dfLpctArE9rdrxu8u45xqnJmxyfn5r\npu+R4Hq3kvhKwsLkIxIKHoHGwirpdNqGFDPkOJPJoFgs2uIzv/RLv2Rd1h8EPC114KcB3PY8796T\nNvCkCyBffqFQwNTUFGKxGCKRiC0MOhgMcP36dVy5cgXLy8tYXV19rGwUq9RmMhnMzMzY+H/X+PRm\n1wjKeyToDaTnQPAzXum+/bifvE4uTFen5KJ+YjT/u0TwUQRMEx8+R87L8wYA2FBrvYYug5y87ifJ\n+K39KOLppxq5JDuXBKbfrTy8hAfOMmy6UChY9+GDBw+QTCaxs7Njy9J96lOfwosvvvgDTwT+NoB/\nJb7/tjHmbWPMHxpj8odpwG/TH/YZAJibm7NnAabTaSw+jN2u1Wq4desWbt68aQlAKBSyxSMDgf0a\nc+FwGIVCwdYF1BtTItdBHFNKEC6R3TUXFwFwXdeI7zJG0rgpiQUJgRaZOV4Xp/Tjvprju8ZJ7uh5\n+weH1Gq1ocNXCC71xa9dliCX6yBFfRr2XO9NE42DiC1Bxw0Aj0LE+SxzCjzPs0VIGFOSyWRs7YlS\nqWTtIRsbG2g0GigUCnjppZces/O8X0Th2ETAGBMB8B8C+P8e/vR/AjgP4EcBPADwv/o891vGmEvG\nmEvyd73BNPhdZxHITCaDdDptT8YNBAK4e/cu7t69awOCGMDCABVGuLEwqOsUXvbt0jn1+Pis/K/H\nf9A8/frVREDfS2t5PB4fCuzh+XsuYqPHLueu7R+u8Wsx3u+v1Wo9FsSjn/MbhxyLaz312QYuAuAn\nwcl2/N6bHq/sQxs32+02arWaLX3mefvH0/PAmXv37tk4CnpJzp49a1Wk9xueRrDQzwN4w/O8TQDg\nfwAwxvzfAL7geshTh488aeekwjI8uFAo4PTp0wiHw2g2m3jjjTfswZc0+oXDYVsSLBgMolAoIJPJ\nPOZ3d/Wn5uG8Rk5zlPtH6ahEXC1RSESky41uOJdxkrqpDlCRfbnclX42Dj0nOS8iFtWRRqMxFADk\nao8qhN966TBlroEMHXapFFpSciE7bUDymlaNeJ8kKpQQGArNuRaLRRtmzkChSCSCra0tewozpbO5\nuTlEo1FUq1XnGuv9MWq/HBWehjrwmxCqgNk/bITwtwBcOUpjRxWDAoEATp06hY985CMoFAooFApY\nWFjA5OQkwuEwrl69ijfffBObm5sYDAbW+MfsuFgsZg8U1XXsXeNy6ZYuLiM3nUZYeY++T4uxfmvj\nIi6SEBhjhsJ0OTeWxWISlKtsl4vDy3HpuUsCpV2dzEb0PA+JRMLaB2SJ81FzdElfun8GN/FeuR78\n7Cc5SdBGPxkEJT/rNQdgz5nodDo2dmBnZ8eGRudyOXvwbKfTsWnqwL7qNTk5ORQ67YJnpR4ciwiY\n/QNHPgvg34qf/6kx5h1jzNsAfgrAf3mcPhx9Dm0SBgfxtOD5+XnrEVheXsaf/Mmf4NatW0OlwVg9\nlqnCtO5SlCa30TrwUUR5P1GXIH87jDVb6rue5z1m6OP9RCyp//OzLPhBjit95weJ+y5Pgx/ho02C\nBMDzvKFzGfzWzbVmkihqwsn5uVQXOSY/N61+J677NPLLd0OpgGXImHFarVZRLpdtfUQAKBQK8Lz9\njEnWqwwE9s9PZEi7n3Tk+vy04FjqgOd5dQAT6rf/+JhtAjh4stzEs7OzmJmZGSIAwWAQxWIRX/zi\nF+1psYxfJ2fiseDMFnQdUOEai96QMrbeJcW4XqDsw+853uciRLoteb3f79tEKPajjWZyDFLqkYRC\ncjnZD4AhtcQlGZBIMQKTxFUbS/W68Lr21bvmK9uRc2Gkp5yjXms5Vj8pSN+v75GEghGQmUwGe3t7\nFvFJCKanp20W6tbWlpWGGDTFk46y2axTajwsTjwpnJgEIj8dxyUaczGy2SzOnDljC3/Ozc0hEomg\n3W7jr//6r/GVr3zFJgclk0nLmSiaMWTVpYNSP5SbXP/X4qy+rgNA9H2jrrkkh1FtkABwXHLNXMEu\nkjgMBvu1AmgQlYTA1b8fJydnZkw9Y+y5FlJUl1WZ+ZvfWo/aFwepE67nRoFrr/nNl+sq58hDaiOR\nCKrVKkqlEgaDgQ1GY8k0FqRlklUikbAH2LrmybFp4jtKIj0snBgicNTJhEIhzM3NYW5ubiittdPp\n4M6dO/jqV7+KjY0NGGNscY/BYICJiQkUCoUhPzXdTHo8kgBJnRh4PEtPivDc3DLV9DDz1y+coAnD\nqO+cp/ydYzDGPBazT+MUjYmDwX7VXz8DlZw/+5BjJrFhtR0pNchnSKj8LP5SvZH9yDlJKUAivvys\n3Y5yTV3cXvfp9w5kmzLSlNIA6wzQ+MyANEZJDgYDexoyA4smJiYOJH5PA+k1nBgioOEgSpdOpzEz\nM2PrxS8uLsLzPFSrVXzjG9/AnTt30Ov1hrgdDYeSA7GvUaIhQW8Kba2Xben7XfNzta2JgN84/TaI\n33pxvCR4lFKYHcmsQeq3rBEoawPK+XDuenxsm94HKYEAw0lLfhxNIq+eg4v7u665EFivs0uKGLUP\n/BBUSlaZTMZKo/QSVKtV5PN5xGIxZLNZlEolRKNRqzZwnaanp23Z88NKI08DTiwR0KA5wNTUlOXo\nxhjMzMxgMNg/+eXNN9+0Z8txM/FAUck9gEcvWFq35YJLTqUlBq3bErT4xs9yPKNgFDLL9jkGlwoh\n+5PfyYGZQUmbiKwCRI7FwqrU7V3zkaoRxXym2TLrjve4pAc9X5cO75KK9Hxd6+FCfhnW7adyHaQy\n6DFRKqTxOZFIWH2/Xq/bkGlKYTzynASWwU9+hsGjjOVJ4ERmEWpKrCESidgMLsZvRyIRrK+v4xvf\n+AbW19etFBAI7GcW6qPB/PqRG0FH+snrwLDFWV8DHi+EoTflKClCjkf/uQxnLpDEiGKnDDriRpVG\nw2AwaDMvZclv2b/ug54AedgnJQ0ducj1l+nGsk05Tr+QaL2G+r3pdXWtJ0GHA7uekZ4EF9Jxr5BY\nEpk9bz9+YW9vz7qnmTEojbCUwmKxmG9a8UE4cRw4kURgFGUzxlhLKguBnD59GsFgEG+99RauXr1q\nD8pgZODMzIyN63YZxqgPyxfgQlrtVtIbbpQO6RegMmq+euP7qRwc2ygpYzAY2NgIgvRs0GhKFYAW\nfVlvXxITOS+OhwbBZrNpLebMIJQg15Jt8k+uh5Rc5Jz173pdtb1Gq1GauPgRC1doNtvQkgr3Ew9f\nkQVq1tbW7MnE9AYwmIv2A4Kf/egg6fA4cCKJwCiKFwqFbN3AUCiEmZkZ5PN5dLtdvPbaa7aCC4Nk\nJiYmbGkr2b7cMCQE+gW7xqX/XFzBD1xcXf5+mOf1/X7PyfHJe2T8Pb/zaC0aslgxWNpTZB+aGEkE\noVdFHvbhMg668heklCMlFhfy6/WUa+EKLnKpbboNF+Fge/zNrx3Oj2oWxX2qBHt7e9je3h46nMbz\nPKyvr9t5sbala41dfT0tOJFEYBQwBDMWiyEQCGBxcRGxWAzNZhN37tyxGzAWi9kS4a4EHqmfukRK\nCa5nNUL4vRQ/TnYUAqC5vvT363vkmDVw48miJwwhrtVqqNfr9hRefpcEQJcY1+vHqETqubpPjlOW\nCnfNw8WtNcFx/c7n/Z51qRT62VHSh58aKfdXr9eztQcpfbGADb0yPNswGAzi7t271j17+vTpx/IH\nDoPsxyUIJ8oweJjJsGQTF3F2dhbGGFy5cgXVatX6baWOxReoxXbNySVx0IinOSuvy9JYEqT1XHMq\n+ZnWdD9DlS5VLuMX9L0kEJqzaUSRa0PklElTOueA6hJDqmX5NXIu9sdozE6nY4Ow+J9x9TzBmV4D\nv3wNzdnlO9Pz91OFRhEa2R7fr4y10M/7SX68LqsOS0lGrjmv8TvtBbVaDRcvXsRzzz2H3d3dI0mY\nx4UTJwmMEnWMMUin00ilUuj3+5iYmEAymUSpVMLVq1etGpBKpaxOO4pzuTiDS7eXv2kx1YXgEiQi\n6z9ZgUZyMOrSkpvKebgImUR8v/UjAWE7dGHxdCVZSZfxArymD1uV85U+cdleMLh/WAlDs0mkWGmn\n3W5bdUxLY/q9+BE+v3XX71dLCC7Do+vdHiRF6HWnpCORn6HornL0JACdTgeJRAJLS0tOSdQFT4sw\nnChJABhtFGT5MCZrpNNp9Pt93LhxAzdu3AAAewoMcwMIcsE0d+CL18g/alx+NgEXyMAYV7sSKeWm\nkv513bdug5KR6x5jHp33R0LJmH4tsUgiI5FEJvzwPhoTSbjkxg8Gg5iamsLk5OTQeYV7e3vY29tD\npVKxx7VJL4HfxnYRCr0G+rq0Rei14tylQdUl6en2dfaiJiAA7DH2lKAymYytN8joQBLAer1u6xIy\n0cgleRxmnz0pnDgiQJDiGoFFQBKJhDWibG5u4vbt21hZWbFUl75qisaynj3BFRUmv0sDlgv8OK6L\nU7A9nYkn1Q9ak3mvdqGNIlD8TQfisK9Wq2WTVVh6TVqhpWRCoH9fvgeuBdUFuT4yhj6RSGB2dhbn\nz58HAHuu4+zsLM6cOYO1tTVrMOMzGgld8/P7PApRNFLL/1rk16qgvCaf0+vCtmhcpXGQe67b7eLm\nzZtDoe2DwcCGGK+uruLUqVM2VkD27TevpwknlghoCIfD9lBRYwyee+45JBIJfPOb38TXv/51PHjw\nAOfOnbP6Jmu+d7td6+YC/K2tvAYM+4UpQrvuc3EarTZQLeE1iUwUj4n0sna+dJlJGBXSSlFeb3QW\nGaEfu9vtDiUOsQ3ZL+dGewV1d4rx2lZhjEEqlcLFixdx5swZZDIZ7O7uYnd3F71eD5FIBOVy2YbP\nnjt3Ds1mE4sPIz07nQ46nQ663S6KxSL29vZsSS5pxJXgRzA4Julp4PPyfoZMu9yrkvhKO4E8vITr\n2+v1UCwWbWAQVbxoNIrBYID19XVbuGZ2dhb5fB4PHjxAu922Kd3AvpT0sY99DIlEwjds2w8OK5W6\n4AeGCKRSKczOztrwyqWlJRSLRaytraFYLA5t/FQqZe0BcjPwOuB2d+mF1N9dIrMErcNKYiLHpyPn\n+Kc5vxyrRm6dfAO48xmIxDw0k+viEr21BCGlFRIFulMlAeQBLTz2jeoHc+tpFfc8zwYgMS6Dadw0\nGvZ6PWSzWayvr2N3d9d6KHTkphy/3zuUIrqfWkUkl3Mmckv1RktkbLfX69mjxpmDIT0D7XbbHrjC\nxKF0Om3HxPlRgj1z5owtiaeljWcFJ4YIjDKGGGPsaULyxJp79+7h5s2baDabluoyl8AYYw+loKFK\n+q4lEvvpf5oAyHHJzeXiOFL8d81Hcm+54TQC+oE2lMlxaV2VEXi02Ou2dZCOHAd/51ykbcCY/XoO\nSw9PfWZAVqfTsfYM+sSlkZb3AY/ciqlUyq5HPB7HYLCfgbi1tWURwqUr+81f3sd1cBFv7W3hfXSd\ndrtdS7QomfC+aDSKbrc7VLSW9SmZHchYgUajga2tLaytrWFxcdF6sVqtFpaXl/HSSy8hGo0ikUhg\ncnISDx48sGM8EeqAMeYPAfwSgC3P8156+FsBwL8GsAjgLoBf9zyvZPZX+H8H8AvYL0P+dz3Pe+Mo\ng9IiWzQatWcK0rpcq9Vw48YNbG1tWf0KgCUA0qUmN50mAn5cV4PcRFrXl9e1fqn75TNaQtH2Av2M\nJkIkHDLqj/dx01M/1fq7XB/2rXV/F5GUKgSNXhcuXMD58+etMZaEhuvOyDh6EKT7kdw/Go1aoyVd\nifF43CZ7ra+vW445iju67Cca9DXOXerxgUDA+vapOsk1lKHGoVAI09PTQxGWJLyNRgM7Ozuo1Wpo\nt9uoVqtYXV3F6dOnrQoQi8WwsrKCYrGIqakpxONxxOPxZ474Eg4rCfxzAP8HgH8pfvsdAK96nvd7\nxpjfefj9H2G/5uDFh3+fxn7h0U8/6QA9z0M+n8f09DQ8z7OlsXZ2dnDnzh2Uy2UrknIDSYpPRPGT\nNFxEQfatN51LvPc8z+q9mojIsehnJAGiyMzr3GyjIuckZ9drxrwAEgu2r2Pl9ZikeCw9JpIQsGDp\n5OQklpaWkEgkrEhLlQHY3+A8kpxt0A3JdhhMw1qPvJfSQD6fR7/fx8rKis1M5Ph1kI+exyhi6nmP\nPB4M5ZXvngFNnA8RXtpvuIb8XcY9kCAYs+/1aTabqNVqVnVgPctoNIrNzU0Ui0WrOkxNTT1zFUDC\noYiA53nfMMYsqp9/BcBPPvz8LwB8DftE4FcA/EtvfwbfNcbkjDFznuc9wCFBLkAwGEQ+n8fExASC\nwSCy2SwSiQQuX76M9fV1a/gLBoP2pKGHYx5CJD9C4LIRaF2b49Ccm5uIhTQpCkqQ3FPHFgCwKoo+\n8lxueOmm4xjZr5YGKIqyxj/HIH3U0vIvxWCCnzrE8USjUQSDQVy8eNH6/CnW016gCQ/wKCBJr0W9\nXreIQ+Qn52QR2FqthgcPHgyNW45TIr6W2nSf2uDK9SGB4lw4JpdtR0pTsk6jHAerWaVSKXvWZb1e\nx/r6ui1tz+hCnmAcjUbxyU9+El/60pfQbrfxfsBxbAIzArE3AMw8/HwKwIq4b/Xhb4cmAnKhI5GI\nXTBjjC2aeffuXezs7NiXRveXRBzgceu+S5fUqoDmKmxH+pMBWL2VoiTv47PcKMyvlwYtyWH4x6Io\nMrafm5YiJrBPkGT9fa37MoGKSMma+JpA+qkeep4SAYgcMzMzOHv2LGq1mkUULU1QnJZRhRIJZbYg\nAOu9YFET2hZisRhyuZw1vgGPEpqI9CQ6klhLD42WqKRXQ6o48n3ItXCpGlLa0JKV9K7IAqLNZtMe\nT07iHY1GbZJRs9nEoZsoKgAAIABJREFUZz7zGZw6dQrLy8uP7VWJHxqeVHp4KoZBz/M8c8Sy4caY\n3wLwWz7X7OdEIoFCoQBgf6Gz2SxarRY2NjasvktEozgpEUy+ZJc4r/uT3zUndOmTMvlIX+Nz0u1G\nvVOOm0YoYDj1Vj7Pschiobxf9sHvrCysMyQpkrvmrFUebe8gQrTbbcTjcUuwqA6wohAAm00IYKiy\nDucnYxL4O0VzmWJL4sUqycVi8TH/viT4LtVJEgz9BzyK6vPbD9p9qNeMbWj1kWNj6DTnxwQtlsRP\nJBJYX1+3h+WeO3cOL7zwAu7evTukkj0rOA4R2KSYb/bLjG89/H0NwBlx3+mHvw2Bd8C5A1zMRCJh\nj4AuFAqYn5+3h4pygRgYJI02+mW7JAD+19xfcmptOZcvhBxO5ifI9vnStTgp+yTici5STJYcjfOk\nGiIlDCIVx5hKpazrTY5dcitNECWx1Osi14KIzwM2gsGgrbArS2fRPkMJCMBQZp20a8h1prFQqi40\n1HU6Hezt7dnCsbwm3XKcm6xhQI4s94PcJ5IYuzg+10OqmH77R6438IioU3qTBlHmEqRSKWs7KZfL\nmJ+fxyc/+Um8+uqrQ1Km3rtPy2ZwHCLwZwD+DoDfe/j/T8Xvv22M+SPsGwTLh7UHaEQLBAK2XJMx\n+3kDyWQSW1tbKJfL+xN4yC2kzip1Nc0t2K4Evki+JPlSpToh75fP0Qeux87f5IYkEPmlnqzXQY9P\nfiZx0f5sxuqT88hnKIFoN5/UraWqI3+XhUNodwgEAojH4zZAi+I9DZLa0MnxkThIAxtTl+URcHzv\n9HI0m00YY4YOAJWh4ZLAcj00M5BSGNfGJTHKNl12AC1NSAIkn2fRFtphSLiazaaNDaCqt7q6ipmZ\nGfR6Pbz88stDa+uCp0UMDusi/FfYNwJOGmNWAfz32Ef+f2OM+QcA7gH49Ye3/zvsuwdvYd9F+Pee\nZGB82dPT0/Yct0AggGKxiNu3b1tVgFWFpJGML4ufpT4okYkgX5zmAnqBXdc1wknkZ/9ax5fIKImI\nJDgSMeU8gGHxk6IzD8X081LIPrQbUIOURBqNxhACUXfmxqaqoQkHgKGjz0igWRhW6tvUnenipfoQ\nDAZtngGPAteEWs5R2jOkK9aFyPyupSON2LoP3YZs2+XF4V7mMWya6JL57OzsoNPpYHt7G3Nzc5id\nnbXFSA4LrnU5CA7rHfhNn0s/7bjXA/CfHWkUPpBMJoesqNw8pVLJJp9IMU+DFvNcCKbv10TCpQbI\n54nE5HrcwPxMLqd1VD4vk3607qrHIefAubE9njsoD/uUbbikEkpOGqT9ge4tydVJ0CQx8DO+cn1Y\nZ1/fR6MlOSSJRTQafUz1kUU3dF8uxNbf9fz0+sj18PMwyLbl/VqSksS63+9bVUlKA9wH7XYbqVTK\nJhW1Wi3Mz89jenoa165dc74bTeDltaPCiYkYBB6fGAuDUI/iCTYMOpHBGS73m9QHaVV2cXbZtx8x\n4TX9PDe1tg9IXV8X2NC2C4ngmptpEVNuaIro5KLSHeeaqx43OZdLKur1etZ1R1cVNy6Nd7TFcCyc\ng1afwuGwDZ6pVCpDtg6+Y8Z58HRoSn80NHqeZ0VjjeBERk0UXMjr0vfle/RDZtmGH6PQQCmJIdvS\n7UgvTiQSQaVSQS6XQ6VSwd7ennWHMwT7WcOJqSegJ+t5+8c1MQ2VNQJarRYajYZ9abFYbMiPLkVe\neQqO1vlcRIPPyY2kg2tcY5bIKuv0UdSTXEWL+OxTjl0iviQOUgz1PM/qyDxJSbq4tHgriQvnJomS\n7EtWw6F+z2AX5gSwmjMwXBdPIpO09tNLQU+ILGnebDZRr9ft3JvNJqrV6lCwU71et9KfZhZ+Bk0X\n4XT99xP55bpJW4Du108SJRHwPM+qM2RuDENmWjdVgt3dXSshnD9/fqj2wCjQxOoocKIkAWCYM1Mn\nJCdtNpuoVCo2VhvAEJcHHm1IIhbFSY0EWjUgSCObS4z048hSz+d3WYFHIqcGWdVHjoEcTksivV7P\nlv+S4iXH7BqjXFs/kIjLTEG2oQ2YlUoFtVoNhULB2mJIBEmQ6UmQ3E9LXnKujUYDmUwGwWAQzWbT\nzrVarQ7lD2gk11KARmxJFOV1zcldBNq1Rn6/u9QHvjtKrSy8UqvVYIwZiipMJBKo1+v2YNOzZ8/a\n/ISD+pfXj6oWnBhJwAUUmwFYrlAqley578CjGgMyCo6JHuRoFGulJOD3p6/rDQcMb2CJ9NJX3+12\n7YGUErQEQGkFwGPShuZILgSQuqxr8/pxKN0PrfR6/tKgKdUrHqrB52VAECWHarVqJQe+Az0eqV5U\nKhW0Wq0hn3q73bZBNOzLtU5+BNpvzroNv7XX3gC/9uVa6zHK/Ai6RRmqLGM5AoEAGo0GWq0WWq0W\n5ubmHiuS6wcuAnRYODGSgMvgQd+wPDVYBuaQ+6XT6aFNorkWRU9mGvJZ3Z+2J2gLutTVgUcbmEhI\nDkgdkPq63EiyeAglGBZNleK5FCX5rLS+aylEc0m5ITSCSOCGlYFWMriIXJ7RmJ7n2Xu3t7cxPz9v\n2yEhIcGW5w/KMw/0WIFH0XrlctkSAmMMarUadnd3h6QlPwQdZd85DHcfxWXlPXJd5X7R/UsDcDQa\ntR4thoRzbWVxVqph7XYbExMTmJiYwNbW1oHI/STITzgxRMAF3EySczJWgIsI7AfHsAiFjF2XB2BI\noxLwiHtJkdCFNKOQS4bu0udLKzh/Z7izi1LTck6ruNzcvO7H2SntSE4qVQ5yUYqc2qUlbQCUXtg/\n3XoMY5YJNPV63T63vr6Oj3zkI0gkEpYYknBLQywlOi0J0LNAwyAJODljo9HA7u6ujaaU7fkhuvys\nxX2XOqa5+yhkcjEMlxQp26FRmwxCGlrz+fxQ4pQx+y5vqgy5XA4TExPPlAAAJ1wdAIZFq1arhenp\naRQKBRizr39XKhUYY2xWFjevtAPwpVAXazQaQ9FhrkWUCOL6k1F6/I2JOwxukb502S6RnJud89Tz\n5n+XSuJ5niVi7JP3cF1KpZKtd6fnRSLF37Qey0IhTNjiONLptOXaDx48wNramiUylIA4bhl8pf8z\ncCibzdrTjqLRqD1Ypt1uY2Njw1belYxAShO6Xb8/eR//dDCR7McFcn21GiltTHotw+Ew6vW6JZB8\nHzx7UBL+fn//aHn2wToLo8ZzXDjRkoA00nmeZ4uHnDlzxr6sYrFojx8HYBOKGAvOF0QOzbrvLuSU\n4LIDyI1IDiz1Plq92SeRQ45DbjgZLquRnYgiVQQ5JnIPxiE0m027YVhzQVa08YuGk31rXRZ4xD3p\n6iLxCQaDtqbDqVOnbPFMcj0taREZpFRBNU8eEZ9IJCwRo3SnEVPbalz2EBdRdUkKWrSX0oPeH657\nNRHgc9JtHQgErF3E8/arK1FSyuVy2N7etmtFCU/u92cNJ4YIuF4IF5dBMUy1PHv2rN2c9GfL+HFy\nMp4Bz+OgBoPB0DHmGun0WFxuREmx6ftlv3z59FDIYBd+1xxe2ijIPXldjkeOiWtCo52rOhG5t25D\n1gzUJw67xsd1JvekPhuJRJBIJLC5uYnLly/jYx/7GJLJpJUE6BWQwVDkvkR8BiBRnWi32wiFQrY2\nIYOLRtkztC6uVSpNyF37TV5zSQIuKUy37eoPGD6olfNjBSGuK/cxiSP3Mn9/1nBiiIDrhUixmQUu\nY7EYzp8/b90pRMZMJmPFcXJCchOKv9Q95cbUG8hlfCNyUW9j0Ae/080DwFY9IpAIsB8ioJ/+6RJJ\nNRfTiCsJpbxf+u+lv1kiu7QFuFQPiVDUWXkPkffWrVsAgBdffNFKBFwfxnHICkexWGwowpJS1O7u\nLkqlEnZ2dlAulx8jwloC4PNSxZL75yBwcX0/ZqTvcb0vP7VF2qRoI4jFYojH46jVakOivzHGxkPw\nRCgJfuK/HMNR4cQQARfQ8EYRHwDq9boNJabvuNVq2So1zDAbDAYol8u2wi2tsMDwS5RcWop1mqt4\nnjcUqgzgsXoARJJmszlkGNOILTPe5Hj8OJ5UAQhS5KQoLTeplFhkG7IPmV7MubuCYvgM14G5Gozs\nq1aruHPnDhqNBhYXF5FKpYbGJdUCZsXJvAp6Ukqlkg0Skt4AqXtLInaQ/s7/LglAI7VfW6PeiW5H\nEnoZN0I1EdgngDxMV6Yv0+jNfRaLxbC9vY2dnR3fOerxSCLux2RccGKJgOQQvV4P6XQapVIJpVIJ\nuVwOU1NTdoFojU8mk5bqNptN63NlsEqtVkM6nbaZarIiD1+kjuUnp+fGldyVG1IGBFE9kK47IoRr\ns+nv2nXpkpDYHwOppAQj2/EjMpJzulQGP6TgvPg8vRtc8/v372Nvbw9TU1OYmppCJpOxdhgZPsvM\nOqoUlPKKxSLK5fJQSrLeyJIou7iw3vyaGEiJwU8S84sm5XV9jxyDDplmIVIidqFQsPUypTdH7h9K\nuIy1eNZwYokAALs5uNm63S52d3exuLiI+fl5XL9+HYPBwN5XKBQQDAaxubk5FPLKl9lsNrG1tTV0\nAIfmKjr9mFIE7Qwse0UpRQbJEKS47xJTdbSi3EhELo3AkvOTqPDwShaqYNVerS4QXMihpQdNADQn\nlnOkGsK6BvTzN5tNFItFzM7O2mPjWAaehj9ywlarhe3tbdRqNZRKpaHgJM5Zjk+7OvnfT1x3cWl5\nzaVq6fn69ScJgh4H9flqtWqt/YlEAtPT08hkMvY0Ih7vRqmAOTHMtZCBcfJ9aDisCuSCE0kEuJD9\nft+e304uUi6XkU6n8dxzz+F73/uejWdntZZQKIREImETT2gAYxvdbhczMzPI5XJDG0z+53PkXqVS\nCa1Wy8a+0y2py2bJnAEAQ1zTNUf5MklsuCldYc1EBGbTyc3JghSDwQDJZHLITSrXVCKDS8T1U0XY\nhzZy0tai1YdKpWKz4zKZjPV5M3zW8zwUi0WsrKxgZ2cHgUDAGs60CiPfiVZV+F8js45+lJKRRFap\nEnFd/KQAl/qgVQCqgYyaLBaLVnXKZDKYnJzExMQEEokEKpWKDXKTUgFL5FM1cq2FC57EHgCcUCJA\n6Pf7j1mJaeibn59HKpVCpVKxdgEiDt2ALFTJzSqNfozc0xZx/o9Go4jH46jX61hbW7PEhkYtRvk1\nGg1r6JHJMix0IrmXazMSGeVnORYtzkrLMQCblEIiKUVP7SHQ/RJcnFPeL9dNum0pKZEwaeTlsVyV\nSgWbm5uIRqNWAjDG2GPQPc+z3gKum4swy7Zd78x1nwbN5V3EUM5XE3BJOFz2I8ZsSHVWB0SxwhLt\nXYwsNcbYg0kk02Ifo5D8SQkAcEKJgHwp1WrVIngmk7FBFxcvXsT58+exvr5uuX0gEEAymbTUmKIn\njTIUs2h1Ze03SQhYnCOdTsMYg2KxOKRacNPLWAG2TdcjDZncDJK7S1EdgE3JdUX0cS30ZpNrxPHS\nI0IbCPVKWWSUm04jNDcgub3uV4LmfMAjqUkHzEivSqvVsolfUvVgvEa/37fuW7lOBK1WudQt1zil\nBCR1eY5LGlW1mK9VNBexlPdwLWmj4u/SqxSLxYbOYeAYksmklQpmZmaQSCSsUftJ4SDCQTiQCBj3\nwSP/M4BfBtABcBvA3/M8b88YswjgPQDXHz7+Xc/z/uGTTIBAMZ8+fopcMzMzeOGFF/Dd737Xvtha\nrTZ0/kAul7Nnve3t7VkKzAy4fD4/5FqT6bzkuvF4HLOzs/A8D5lMBp7noVwuW/Gf1m/mJshNTO7s\nct/Jz9KwyP8ukVQimZQuaHhjngU3FyUd6ZPn+kmJQtohJGIDj+vBciySW0oxnl4GiXRahCZSSWMq\nv8sxaGIgiaokrq4NL4mFvqbHIKUx3adcA/mbHi8AK8kwcYq2LLYri612u13s7OzA8/bP00ilUqjV\napaBbG9vH0gEXOM8KhxGEvjnePzgkS8D+F3P83rGmP8JwO9i/8wBALjted6PHnUg+iVykWVCBd2A\njCKbmZmxKgE5M9NrPW/fj80N1mq1bBQdKbEsP8ZoOCnGkUvMzs5aokLjV6VSsS9JHupBBJCGRFbJ\ncW1SzWV00I/cyBL4XXNIzk8G5kiRUlb25Tj1+lMi0CqMC5l4j6z3x/sYmCSf1XYYtk2k0MekyU2u\nw3zZlyQ6sn/O36Xna6lFSw7yd/2sJuLyupTqaORrNBrwPA/VahXlchm5XM7uYyJ9OBxGNptFo9FA\nKBRCpVLBysrKULDQYTn7UeFAIuA5Dh7xPO9L4ut3AfxHxx2I3+QYEksLarVaxe7uLs6dO2dPv2Wq\nKRc/k8kAeFR5h4VIAoEA8vm8FZ2pS2tdnS9ZSgZ0YVF05nnzNEDKRCBuJOp3Mp9ebkgZYyD7BeDk\noFJKkLo5xyhrAOozCUgYqJ8yzZpSiiYmkhi5pAGNCIFAwKbL6nsJMutSz0+mVEuiKkHq6Fpclwgp\nP2vbh7bky88upJZJTy7CKyUCtk8bCQkxCXO9XkepVMKZM2esysA+M5kMQqEQarUaOp0Orl+/jrt3\n7zqJruz7aRCFp2ET+PvYP5OQsGSMeRNABcB/53neN10PmRHnDhAoUpdKJetbDgaDKJfLNnJwYWEB\n9+/ftxuZXI4SwIMHD7C5uWn90zzjjgvoErf0BuGGpUvn4fgRi8Xsxub5cfIlEeHkOXdycwGwSCg3\noouDSYR2cW1pZJJ1+7kRZaVhGaMvIxrlZndJZnqN5FwBd0lz10aVm1+ewKRVI83dXUZcPR6NoC5V\nRHptdDtSInOpBfId6Gf0WnBuNEJ3Oh3UajVrG6lUKta7QobCYqrXr1+3qoILDov8h5EejkUEjDH/\nLYAegP/n4U8PACx4nrdrjPkEgD8xxrzoeV5FP+upcwf8BjsYDGxhUVmPnsdsLy4u4lvf+pbdWLzG\nwzGr1arNG+CLZVKGDP19OI6hfsVYLSLK+ACKcdLAQ27AZyKRiCUePKSDm57taVsAx6JtA1R55Iam\nWEn1BniUm08fvhShSSSMMdZPT0KgjZMuFUa9Q0t0ZVGSUdxXqiFSiuBYddKMlgY0sWIfo3R49iGN\ndZLIuIyyZDq6L1f/rn4lgZPFbplh2mg0UCwWrTGbam02m0WhUMDOzs5QjMCzUAMIT0wEjDF/F/sG\nw5/2Ho7Q87w2gPbDz5eNMbcBPAfg0nEGKY+EDgaDVrRPJBKYmpqy4m8wGLRFGlmlhi+ZxIFGGvmC\nXZZflzQAPKLwfJ6iK33ckotzc9LVSJ1XXuNvfjYD/udGJuGSf1L/p27u4nIcv+d5Q6c3uTL1tNTB\n/1Ic1xGULlGd74ySENdREjQih0vtkPMgIZfE0WXT0G1IYiXXVX6WhFarPK534geSAHKPuELWGUmY\nyWSsMdvzPJtGvbu761sc92nDExEBY8zPAfivAfwHnuc1xO9TAIqe5/WNMeewfzLxsk8zBwInL4tK\n8Dt99hMTE9YCy2cYPERiQZCbR5fN8utbzA0ALJIxSo7uHUY2Ss7LNrgJarWadYNxA3NzSGOd5p7s\nXyMSQSKGVCf8EEP+UX/l+vjd5+KqkrPLdZJ/ek1JIKXNge4yKSFI5Nbqj5bSdB96zbhGkji59H/5\nnD6WTP5pY65cI3kPvTOyDmYsFrMMajAYYHJyEpOTk/b9FwoFGxmridazIgaHcRG6Dh75XQBRAF9+\nOEC6Av99AP/EGNMFMADwDz3PKzobPgLwRFcmY0g9fGJiwqYNMzCFZZoYykpxjPn20uCnX6Zj/kPI\nIV1a5IJ0PfJF0kouKx3zHtbwZ5v8T7FeBwMR5OaXG5G/602qVRyNHPo3ficRkeqTbpMHnVJcddkC\nNGGQBEz656WUx/WT/el1OEhF0e9Oj8kvOEg/pz9LouanBmiVjglWjEmhkbpYLFpmkk6nMTExYZlK\nOp3G1tYWSqXS0Bz054PmfBQ4jHfAdfDIH/jc+8cA/vhIIxDgmoDn7fvli8UiZmZmrB5OKjs5OYl8\nPo+9vT3U63UrbgcC+yfdMPCi1WphcnIS3W4Xe3t7tkQZ8ChYhv1pcVaOaTAYWK4lN2sqlbJIL3P9\ngUcBI+xTqjZEOmnRl2XVpP4qjy+XNgh6T9iWdH/KeUmuLfV3Ijt/k/UQSJCoI6fTaeRyOQSD+8dp\ns/yX1PH5LrXVnoSA1nKeGsVxsx+ts0uCLd+DzOKU85DvjX86/kG2I4mdPtxEqyIuCUkSX3lvMLh/\nrBptUsB+8BsR/sKFC9btvba2hnw+j1gshm9+85vY3Ny0cziKJHAUgkE4MRGDcjH14Hu9nj2N1hiD\nUqmEer2OM2fOWGs9kYsvnBuZB5pubW1ZatxsNq2BETh6HL3rGW5kaZzjeLTeLP3nbEu2AQyfRsR2\ntRtKcmeKl1w/F0d1qT5SvOfG1XOm+4+cjc/J485dKgE/69/k+GXotpRo+IwfUuvfZF/aMKnFfq2q\n+KkvfsB3qj0ZrnlSuqGUR4bA0PZcLgdj9kOoz507h52dHdy5c8e6Ul1relhiwHkfRBBOTI1BrZdJ\nGAwG2N7eRrfbRTQaRbVaxerqKnq9HqamppBMJgE8qj/AQg3AvioRi8WQyWSGSnvzvzag6Q0hOYmf\nnkwgokpbgOY8BMnppIGNfbM0mE4E4j3SCEjxfBTCaLVGi/o6dFa+i0AgYI1XkkiREGiEl/NwIZ5U\nq9gWPRzScOi3cV3qhmxfShLsT9dO0Agm11m/e9e75hilGsg/6aplOjsAW1EI2A8zLxQKmJ6etoFo\n/397XxvbWHae9xxKosQP8VPf0uxoNB+7O7veL6+D/WEHhmPYiZPAjYGk6Z+4RdA2QII2QArUSfoj\naH+1aFKkaBEgQYImhWsnSNyuY7t27aTNbnZnd9Zez3o0Ozsz0mj0LVESJZIiJVIST3+Qz5mXZ86l\nSEkzknb4AgTJe88999xzz/ue9/vt7e3F8vIyZmdnHyCI8n00SqwahRPDCQD1KRxTTrGMM5NohsNh\nJBIJAPcTVgD3We29vT1sbm7WONcwCs/eLQG3vOzabeR4JSLxON2IgdrKxva9JCtu7yauBcnzNpvL\nuAh7DuVisuVpOQ4SEZvDIaGgMpPXy4zCLk8+184lRQLOF6097M9rF7f1IZIVl1yb6/3YXKYtatkK\nOFvscAGJGHU5FBE5FyzdTh0V41H4noLBIOLxOOLxONbX1xEIBFAoFHDr1i1TcVtCs3J+M9ecKCJQ\nj23JZDJYXV3F8PAwCoWCkYM5uQCMe7E0mZHVZaQWAFPwgS6aNmX1+m8jiUv5xXbSCUdq7m2k5jki\nhLyHiwAAtV5scuG5EIHtJYGp94x2P9KnX5pVqXylY5atyLPHLscrEVcmbGF9CDnX9rjlOTkvJMBe\nRJPfMoeE7b/gEptcnIw997yeoilQWa8sl0dTdjweN56C1K3wvv39/chkMhgfHzebmYubahSx7bb1\nrj0x4kA94M5KkaC7uxurq6vY2NgwSSuA++IAcD/fu1T0kHoHAgETgmsvJrloJXvosrvb10l2kN9S\nrnft4HIhSRlT9i/bu2R6XiOfVYLUwMsMxK6+JNIx6o11DsktcNfmopdZhF3z5JpDucgZXiznUOpr\nbDGH8+ti010ikfzNJB7s22blOQbbF8MWByXnI7maYrGI1dVVk3MikUjg4sWLOHfuHBKJhFEQM7V6\nOp1GNBrF4OAg5ubmMDMz48l9NMsN2OKEF5wKIgBUXlg6nUahUDDppzKZDNrb25FMJgHAyGFA7cID\n7suJDLslkZC7+X4Liv3abbhwyHHwOJ2TpLwvdzECx1lPuy3/2+fkgpRsPU1SUslEJV8oFHqgiKms\nachisPxIgkguTI7dtdhcREE+g+R4SGyYTMMLydmHS2SyOTbeQyI33wlFD+lybVsY7LHKdOA2p8EP\nYzLa2toQjUZx7tw5XLp0CdFo1HCqoVAIY2Nj8Pl8yGazOHv2LPb29nDjxg1sbGw88CzNwn6crQ0n\nRhzweuESWIyUOgB66fX19T1gq5dsrM3qcse13UIle24jqWuhyQUi2VdJfBgYYxMcKX9KUyHB5Szk\nmi+5m0lWXZ6X8QmyRBgRneOnuYpjIidFTowIQCcsL5ZfIoVNuGx5nm1ofqQsvbu7i0QiUVM/whYp\n5G8p29vcGc8xv4Jsy7mWviPkqqTJT86dnBe2p9WJ2YEDgQAikQj6+vpMwhkWIY1EIjh//rzhAnp6\nenDjxg3cvHmzJpr1oNAs4TgxRABwm+UkUNHCl7a8vIx0Om0Wi0xLzpdms6pcAKTqXvIh4EY8Hrd1\nAECtR6JcnDabbu+M0u/fBTYBcrGwHJN8Dq11TZUjzgkJE01V0lxKbkIqESn/U1aVxE6+Ny/kl+Pi\nf5sg8h1Fo1EAlazSzAUhiS3nWRIBW+Fp30e+Exk3AqCmVJ1UHrvWoVL3ox1lQppyuWyyJFH3RM6G\n3BbFJ7/fj6GhIVNodWxsDIVCAePj4w3VHHwYcKKIwH5QKpWwuLiICxcuoKurC0tLSygUChgcHEQk\nEsHi4iK2t7dNhSHKc5TXuLhJBOzFLBV4ErzEApciDEDN7mMTBJcc7mI/Xfcn0suiIbZPui3TunZs\nytXA/UKhtPkzgIemWMrQ8jnlc7jkTsmF2ITBnkN7LhnhKPUDkmCwT5c/w356Apv4sH/OI5XEfD5b\nPKMIyWzWZPu5yzMegwRAZrVeW1tDLpdDMpnE6OgolpaWEAgEEI/Hsbi4iGvXrhkPzEa4gHqKvmbb\nnioioLXGwsICNjc3EYlEsLm5abIHRyIRzM/PG52BTKghkZ/f0vddEgGXfdqLQ7EXN49JJOG1cgf1\nIgRyLHI3pfgiCRr7dbkYS32IbfuW46WjDnBfLGGCUBZRITdBXYpXX/YzE2wNvy02sA0/3GH5Du33\nIJWGLuJm/3cRK47Bttp4yeH2c5LwstgN/QHK5bJR+nV3dyMej6OzsxOzs7P44IMP0NHRgaGhIQSD\nQaysrGB4eBh9TtbtAAAgAElEQVRaa9y4cQN37tzZN4tQPXCtT9f7cMGJUgzupxfQupKhdnl52ch3\ns7Ozhrr6fD5sbW2ZF0Olj5Sb7f+SDax3b4lUErG8FGNSpHAtVvmxtdT2OWrjKVrIaEV5PxfLbXMC\nQK3CjvfY2dlBPp83CzqdTteYAL2UcOzXNZ82wfN6XmlV4T2kGOMy5cnnlO9BPqf9fiRhtcUotncR\nDkk0qDjlnJArAGCyAyUSCRPdOjU1hbfffhtaa1y4cAFPPfWUqRsZiUQwNzeHt99+2+kbsB/Y7+Sg\ncKKIgAtsxKRIwF1xbW0NbW1tePHFF01QD4OHuLBkklDpnWdrh6VJz4vFdS0sVzvX2F0EwN6dJBss\nr5ccAPCgO6zNjdRbHHYfkjgyryIXOtlk23XZxS7b93Cx7a52rr5s3YatAJVETyKonAMJNiG27+8a\ni6ud7FcmCw0Gg8ZxjZGAk5OTuH37NsrlMp5++mlcvnwZkUgEuVwOw8PDUErhvffeq1EINsriHyWc\nSHFA61oHG3l8b28Pq6urJgsLS3A/99xzxjxISkulku23bZvzuMBlBh4uLLkYbYR1yVr2rmvLwvZO\nZosN8jg/MhGnfY1cuHI3ZFtG7tmIL4OEbLlXKs/s/ARyt/VSoLrOy/vb82Sb5Xhvea3W2iRAYfyC\nzZlJPxGZ+FTOld2nPS4v0cJ+NmlqZKAQq1wVCgWsr68bRWcymcSlS5cwMDCAXC5najDcvXsXb7/9\nNrLZB3Lu1NzLa45d7Q4CJ44I2DKj6wUwgCgQCKBUKmF+fh5nz5418uL29jby+bzJ/AqgBpGkgg14\n0NvMZjddu7ft0FGPc5D9UEa2lWsumdpGOheRkOYtqSuQeg6bS7AJmj3Xkt2V+f5s7sMGe06kPkCy\n33IuZa4IEn++G9Z+1FqbXZfPLJV40tmK+gyZ5dfmEOQ823K4i7jL3yQ0hUKhppgo619orU14cCKR\ngM/nw/DwMEZGRox1ZXh4GKVSCVevXsXt27drkona0Ag3tx8B8NJ1EE4cESDYi15CPp83NtZQKIRy\nuYxCoWDytGldsWVHIhEopUzabyp1uHi4kLwCfTgGlzLPZl3txSNZckkQbDnT6yU2IufJfr0QXbLP\n8ho5dpvrksgl2XkvltqeL9cYJYGgCEbFGu3u0p5PsyT74DG2sUUA5vDju6YdX4p49jNIfQSfV45Z\nEgtykYVCwaSsp8ckx9bT04OhoSH09vaiXC4jl8thYGAA58+fN+MbHR2F3+/HjRs3cPXq1ZoUYvsh\ntNdz1ING1tFB6w78DoB/CmCl2uy3tNbfqp77TQC/DGAPwL/QWn9n31E0ADY1ZsktvoBisYhnnnkG\nV69ehc/nQ6lUMsUwmcwDuG8XljubLd/bVNZWuLl2CdfLcFkJ7LZeLH6j4gbvI4mYF1GxkdTmLCSR\nkr4VPO/aNW2Ox7XbA/c5AWbdZYw9oxAlQjKGgDsuzxMZ7YItDOCRuSSpy6BilUjOcdpKTK4D6kZK\npRK01jXKSRYI5a6vVCX2IRKJ4MyZM+jv7zdp5jOZDAYGBnDp0iUAwPr6Oi5cuIB4PI6pqSl8+9vf\nxr179x7gnuqBa316tZHvdT84aN0BAPhPWuv/aA3gMoBfBPAMgCEA31NKXdJaH9j24UKE3d3dmuKV\n8/PzGBwcxKc+9SncunXLvKT19XXjGccFRWWXTQCABxFa7iaSSNgsJY9J8HJgkYjhRST22xEkskpF\nnZdsLq+xf7uUaVIMsftpFEg82SezQtGeTkcl7rCcZ6moBVDjQUeEZKp0AMYaRE9DJpUJBoM1uzVF\nRUmk5HNKZSzXCt8P5X9Ww9JaIxAIIBwOI5lMGj8V7v5bW1vo7+/H888/j/b2dqyurmJkZAR9fX1Y\nW1vDG2+8gWvXrhlCY7/bZub4MOcJB6o7UAc+D+CrupJwdEopNQHgxwBcafB6ed+6E5JOp02p8ZmZ\nGYyOjuL555/Hiy++iL/7u78DALMwEomEMYO1t7cbMUEqlmQYrs0V2AtH7m5sb4/ZHr8LqexQXPbl\nWhhyTDxmm8Tse9lmSpvLkGO3+3bt8vJeXhyMPR4SAsrrNHfKYxwvd3977mUwEa0/RHaJeLYIxHx+\nJEhdXV1G3yCzPBPRqYMgsaCViXqLcrniMkwnn2QyabIBkXAVCgX09/fjIx/5CAKBANLpNGKxGEZG\nRlAqlfDGG2/gtddeMz4YB4F6m8RB+jyMTuDXlFK/hEom4d/QWq8DGEalGAlhrnrsAVAN1h2wFz8h\nnU6blEzFYhETExP45Cc/iZ/4iZ/A5OQk5ubmTKy6TO7J8GFpBpOLR4Is2CEDR+T45G+XXbwe+w3c\n18Bbc+P5bRMqW3Sx+5cEhGPcb6eXyCG19FJ+lmOy/RvkmKW4xvqIPG87bhFZpW6AzkNbW1smsSvZ\nf2rnmYOS53Z3d7G6ulqD8DQZy2eU+hKe393drUlSIyEQCJgQ4EQiYYrSciwsbXf58mUEAgEsLCwg\nGo3izJkz0FpjfHwcr7/+uhnbw4RGdQbAwYnAHwD4dwB09ft3USlC0jBoq+6Adc5zgfPhdnZ2cO/e\nPYyNjSEQCGB2dhZaa7z88su4e/cuvvzlLxs/72w2a5I58j8Vg1wMpP5yMVL2JBGQY5NjJUglmwR7\nZyNI85uXWFCP1a9HFFxtpRWkXnt7nHw2XiuJhOxP2u5JQBjPwQSxrMzLrEkyKQdld96f74biA0UC\nrbURE4j0MmfCzs6O8SWhopA6IhaOpb5AVry278fNgroL1gcgYVLqfiyBUgoDAwPGF+DevXtoa2vD\n4OAggsEgJiYm8K1vfQvT09MP6FbqyfiHhf1ES+CAREBrvSxu8kcAvlH9Ow/gjGg6Uj12aHBN0NLS\nEqampvDUU0/B7/djcXER58+fx6c//WncunULb731Fvb2KvUJmc5JKYVCoYBcLmcqCLF/G1nJCchd\nwwZpGeB/Lih77FJOtl1pZTsXgnpxFV7cgn18P92Blwhg9+vyymN7OwcAkZ1ElESVHBj7oY6HuSJ4\nD7L+kriS0Ngh2iTW0pefCCrLoTMuguMmIeZ4SYza29tNuHVbW5tBfioYpQ9JqVTC8PAwzpw5g2g0\nirW1NXR0dGBsbAzd3d1YWFjAd77zHbz//vsmBNue+4OC147fjFhw0LoDg1rrxerfnwMwXv39dQD/\nQyn1e6goBi8CuHqQe9TjBnieNduSyST6+vpw69YthEIhDA8P42d+5meQy+Vw48YNlMtloySMx+Po\n6OgwvgRcHLwH++Y32Ux53EZGKbvacrHXC3YhnEtet3USbCfnwr6H1/jk3Lme176H3acXByQ5ATtN\nmd2nPE7kZurtUChkkr1w7unrId3BARg9DuMaZJ0ARjwS2traTIZkmUmKxJieprlczjgihcNhBAIB\nQ1g6OzuNTkn+pvs6i+NSEXnx4kWT/Oa73/0urly5UpP+7qjgKPo7aN2BTyqlXkBFHLgH4J9XB3RD\nKfUXAN5HpTzZr+pDWAbk4vdCppWVFdy+fduw+uFwGB/72Mfw7LPP4md/9mdRKpVw584daF2xFvj9\nfgSDQWxtbSGTydToC6S/gO2vLllU107ZyHPY7fdj4W12W47Lhv3YvnpEyZ5nFyGT5+QuKPu0kcvu\nG0ANQZXcA1PEsV8q4UhUZJQfxQISXdtLkvfr6OgwgTwM5mFYryQmAIxc39XVBQDG45QmP4ob0jlp\ne3sbPp8PY2NjJmFoNptFPB43AW5vvPEGXn/9deTz+SMnAM1AXVHxOAdmBmHpBKxzNd8uCAQC+MhH\nPoInn3wS4XAYzzzzDC5cuIC9vT1cuXIFX/nKVzAzMwOgkvE1kUgYj7J4PI7u7m5orWsWljRTybhx\nO/jGxYZJmZtjt3ddL7u+vFay1S7ixH7suXHpF+w5lQTItiIQuGvLe9nH5Luxz9nEQ86XJArynM01\ncGx0f6aCkDoHmZOQmvt0Oo2dnR0kk0lEIhFEIhGjE+J4ZBGbfD5fY7YkZ7G9vW0SgpAj5H2ojHzm\nmWdw/vx5dHR0IJvNwu/3o6enBz6fD1euXMFf/uVfIpVKPfCe60EzOoL9+rTe5w+01i/bbU58AJEE\n12LXWpssrdTyTkxMIJVKIRAI4GMf+xi+8IUvYGRkxDiirKysGCUUS5ZxUbiCR2wfeMmiSzOjlBfl\n+PjbHrcX8rONC1nldfbOaxMdLy9I+7eN/HKMcsf3Gq8t1rj6s4mnnE9Xn9JjUSo0iejc1aXtPxAI\nGD1PKBRCd3e3yUHJRJ+2GzR1C+w3HA4jGo2aTSCfz6NUKhlOBIBxdhobG8O5c+fQ0dGBQqEArTUi\nkQj29vbw3nvv4dvf/jZWVlacm0U9cM2f6914tXWtpXr3P1VEoJ5YkMlkkEqlzIu7fv06Jicn4ff7\n8corr+Cnf/qn0d/fD6UqGt21tTXji769vW3i6Lm72DnwbWSwX4JcWDwuv3md65kIXs8miYatgZfE\nQY7H9bG95iShIUhzH9u4CKMcr93G9Zz2OCT34LVApQXHJkgEicQ+nw/d3d01BVJs5KdCkjoh7vB0\nKmPsCdeG1trEIgAwa+TixYt4+umn0dbWZnwJ6H9y69YtfPOb38T09PSBRAB7DvdD4nriZCP3P7Gx\nAzbYsqXr/K1bt+D3+zEwMID19XXcvHkT4XAYsVgMn/jEJ7C9vY1XX30VGxsbKJVK2NjYMBFgm5ub\n2NraQiwWM8hCMxBQW/aL95M+55J9d42bUI/K221tTsLmQlwEpxn2kP26rnU9h2v38eKY6j2rzVHJ\n55aihxQV5LXyndB3QJYzj0QipibF8PAwcrkcNjc3AVSQnx6GFCskB8ecCnQ97uzshN/vNybGvb09\njIyM4PLly6aUugxOm5mZwfe+9z1MTk7WJXBeUO8duObwKODUEAHAvXPJBTQ3N4dAIIBQKIRIJIJM\nJoOJiQk899xzCIVC+MxnPgOfz4e//uu/xtraGvb29rC+vo54PI5yuYxMJoPt7W10dnYiFovVFA6V\nSO8aF7+9kNQWC7zkPpvIyHtwwQK1LscuttwlNrgIEsHl/egSCeRO7iJS9ntyvS+v6+Rcyf6lj7+8\nRppww+GwiRORGv5wOIxMJoPl5WXjoixzSmitTcbq3d1dUyyko6MDsVgMnZ2dyOfzyGaz6OzsxOjo\nKJ599ll0d3djbm7OiA/pdBp3797Fa6+9hvHxcRPB6AVeSOwiri6Ry6utnKNG4VQRAcBb0w5UFvLc\n3JzJ8NrW1oaZmRmEQiGMjo4iEAjgs5/9LDo7O/GNb3wDCwsL2NnZwerqqmH5mHGXOQlc95djoCeh\ndHKRCGsjlQsx5MK37yOfjcfldTxny9qyH/s+Elw7tWTt+dv2FLR3bxfrKXd1l37DvpecF56Xmnxy\nWzzGa+geTnfieDyO4eFh9PT01GRl2tzcNGY9qVAslUrI5XLmXQYCAYyMjCAej2N1ddVUsBoeHsZz\nzz2Hrq4upNNpFItFk/l6dXUVf//3f4/x8XEjShwGGuUW67W123iN6dQRAaA+lcvn85iYmEAwGERf\nXx8AYGZmBn6/H0888QT8fj8+/vGPIxqN4mtf+xomJyeN11koFEI8HjdRiAxHlg5ALvaZpiuXEo0y\ntktRZiO2RA6bLZQOM/vpG7yQ0QYv4iN/e314vdyl6AgjMxDZzlguLoAuwmxHRaDLlMe2vF+xWMT2\n9jZWVlaMv8HIyAh6e3vR2dmJTCaDzc1NwzHQCYj5AJRS5l2zVkAgEIDWGouLi1haWoLWldRgzz//\nPDo7O5FOp7G3t4f+/n6T4u7KlSu4du1awwTgoETChfD7Efr94FQSAYKLzQaA5eVljI+P4/Lly+jt\n7UU+n8fNmzehtUZfXx+y2Sw++tGPwu/349VXX8XNmzeNf/nGxgaSyaTZOQCYXPW8J+/Hj8tJyG7j\nUtLZCrh6HIEXGy/b1ZsTFwfgtXhcLLqt3XeNgcE/JJiMy5BilItjkNfTQYgBQrZoQCUcLT1k7Tc2\nNtDX14ednR1kMhlj15fEgkpjKgK3trbMM7W1tZkkocViEffu3UOxWER3dzcuXbqES5cuob29HXNz\nc1BKmWSi6+vreOutt/Dee+8dCQdwWGiE07PhxBMBrwXHcwS5iPf29rC4uGhkQCZ9vHfvnrENx2Ix\nvPDCC+js7MQ3v/lNw8axDDoXYjqdNg5GTMstA1wA1HgdunZMCdzpad6SbKmdIlzunvJ5bcJgE5tG\n55Xftisw542/eY62eq93IEubyczIUqTgHEiORz4nI/Eomsl5LJfLRqNP7ovJOtjH+vo61tfXTdLP\nvr4+dHR0GLFA64rTmEwZHovFDBeYy+WQy+Xg8/lw9uxZjI2NYXBwEHt7e7h3755xQKI/wptvvomr\nV68il8s9wCUdFFwinzx3mL5dcOKJgA0udscVtFMul7G4uAitKwqk3t5es0OQbezr6zNxB8lkEteu\nXTPy38bGhkF6RsDRg036vdMX3VUH0MWWy2N0VyWCuDT0+9n6Zb9erLo9b/W4Crnz7gf2vSSR4A5M\nt996YoWcDzrpMIW3dBJiNKLPdz8Potx9GfDDKD0ZXCQJUKlUMpWABwcHjbfi3t4e1tbWkEwm0dPT\ng7GxMUSjUWQyGaytrUFrjXg8jvb2dkMA3nzzTWxsbBzIEiCh3pw3069rDfG4F5wKImDveq7zEth2\nd3cXCwsLNUkhqT2enJzE9vY2BgYGMDo6it7eXpw/fx6vv/465ufnDeIrpWqqyND9k0jLctOMjqNu\ngIvc9ujjb7K0XOh2/QAXS28flx+X15/URdhstcvTkO1sgiTvIcNv64kHbMNxySg/AqPv7PmhC/jW\n1pZR5FF+Z19k72Uk5t7entmlGecvw8W3t7eRy+VMeu9IJGLy/a2vryOXy2F0dBRnzpxBLBZDd3c3\nUqkUFhYW0NnZiZ6eHrS1tSGVSuGtt97CO++84yQAjezUh5HjGxEP693PhlNBBGzwemhb8cSFmEql\njAy5u7uLeDwOrTWmpqawu7uLkZERdHd34xOf+AQGBwfxzjvv4N1338Xi4qKJSGNJqWAwCKWU2ZVy\nuRyAyi4krQkSMW19AnC/tJfW2vRpIy1Q3ypg34f/bT2DyxIhA3lc88nj5FTssUhiIs2WPC8Vea5+\nmKknFAo9IALJilEcK+8nnXNoGqSyjwjf29trck/Sfr+1tWVEBb/fj5GREQwODqJYLJosVaFQCM8+\n+6ypHDQ/P4/V1VXEYjHjCLSwsIA333wT7777ruEa661JL6jXfr++jlo5eOJjB0SbB/43IhbYkxOJ\nRHDu3DmcOXMGoVAIXV1dJgw0EokgFAoZ9n5hYQF/+7d/i6tXr2J9fd1omTkWZjtmJFs4HEY8Hq9B\nNOmIYrPmJCBcYPXehdy9+U1zmQvRXXPmmiP7Wlu3YJvwyFrLHIRebtI2p0I/fWbq6erqMmG90iuQ\nCkKy8rQW0MTn8/mQTCYRjUaNdj8YDCIajZp5LRaLiEQi2N7exsbGBtbW1jA8PIzLly+jo6MDqVQK\nmUwGfr/fyP3hcBgzMzPI5XKIRCKGm8jn87hx4wa+//3vY2pqyuQybEb+b+T91rt2P9gPj6vrxRk7\ncGo4AZsF8nro/WQrOhAppUwZqI2NDbMQL168iEgkgq2tLSQSCfz8z/88hoeHceXKFSwtLSGbzdYU\nOFGqEsseDAaNrbm7u9t4G8rAE0kYyPYWi8Ua7bnkBuTzumztEjkbVRi5kNw1rzzmctmtx4m55p8E\njP73JJx2lmf53HLnZ2ah3d1dJJNJExbM2n8UAchdkEtYWVkxeQefeOIJvPTSSwiFQshmsxgaGsLo\n6CiCwSASiYRZG5ubm6ZScHt7OzKZDH70ox/hnXfewezsbI0TUDO7f7Ny/X59NHrvRtqdGiLQCHBx\nS8cdF+TzeUxNTUHrSg74rq4uEy12584djI2NIRKJmNoGL7/8Mnw+H27cuGFyG/KbWmwApvjE1tYW\nAoGAqUTj5XlIVlUSCMnC24o3+Zz287G9KzrR1Sev4XmbQ3CJC65d3nUfOR67LRWFZPvt/uV/Eg3u\nvDLfAN11I5GIie+nO3B7e7tJ5d3T04NoNGrSfi8vL6O/vx/xeNy8N4oKxWIRMzMzSCQSxpfghz/8\nIX7wgx8glUrVpEGT8+cF9fRYXm3td9fIfZq9lw0fOiJgg2tySPXp303EiUajyGazmJ6eNh5jdCN+\n+umnjesoq+BsbGyYMNPNzU0TgMIoMzodURkpYw9oHmSqs2AwWDM+G3HtHVw+k4sNd8n6Xte7EJfn\nbWWebONSHrKN18JlrEa5XH7AosI+qEQk11AqlYxlhuJPLBarMRMWCgUUCgWTaERrjXPnziEajcLv\n96O3txfLy8vo6+tDMpmE1pWCIVtbW8YfJJFIYHp62uQJuH37NsbHx7G+vt40+/8woBEO4SBw0LoD\nfw7gyWqTGIANrfULSqlRADcB3Kqee0tr/SuHGmGTwAmxbd/2LkjZcXp6Gn6/3/gPMOaAtQ3oY8AE\nkuFwGDs7OyaRZDabRT6fBwCjeOQuStfSbDZrdizgvilrc3PTKKX6+/sfyHpjiwVS1yCfYz8Zv9F5\ncxFMW9Tw4irkuGwXY9mW74XKP+knQX0AiQ85AKb4Yo7/cDhs2krTIf0HmEpuYGDAiAudnZ24ePEi\n9vb2jMmvVCqhu7vb9NnW1obnn38ehUIB77zzDu7evWuqDB0EDoOcB+UiHhYn8N9g1R3QWv9DMYDf\nBSBLqk5qrV9oeiQNQDNyb6NeatlsFrdv3zbJKorFImKxmPEnl/cdGBhAJBLB+vo6BgYGEI1GsbCw\nYFxLqdCiLBsMBo2jCsUEKruYqZaLWZoLeU/5PK5d3N59pTjkQlL5bRMb6cEo29opwKXjD58TqHUA\nsp/FpeOQbQHUFISVtn2Kal1dXYhEIggGgyYVGKsBMZS4t7cXTzzxBDY2NkxgTzAYRCwWg9YVJyGG\nkANALBYzbuLlchmrq6tYWFjAxMQE5ufnTY6J44R6nFsz19aDQ9UdUJW7/AKATzV0t0cM9cQDnidH\nMDU1ZRJG7uzsmCqz9Ekvl8sIBAIIBAKGVY3FYhgYGMDt27cxOTlp5Fem1vb7/UZJuLOzY0JRC4VC\njZNQLpeD3+83iS2lM5Ict43wLuLmIg71np9gIzfP2WZL1/3sbxIBSYTtPlxehNT0s44kdQCM5mPC\n0nw+b0Sujo4OhEIhXLhwAX19fRgdHcW9e/fQ3t5uOIFCoYDl5WVTA2BoaAjBYBBaa+Mqvra2htnZ\nWbzxxhvmPdWLAmwEjlJ8cK1ll97gIFzBYXUCnwCwrLW+I46dU0r9EEAWwL/RWr9+yHscCJpRrGhd\nSWQ5MzODYrGIgYEBhEIhbG5uGu1+Nps1XoZKKWO6ohIRAGZnZ80C3tzcNMSAC1gmrmAkG9tyhyoW\niya/nRQP+Bxesr7Xc9rIbLP3thXA/khFaz252OY8JCGwCZP8Tyef7e1t4wJMghgKhQwBKJcr9SbT\n6bRB/r6+PgSDQeP0Q6UjvQDL5TLW1tawslKpljc8PIxEIoFSqWSOr6+vY3l5GUtLS1hbWzNFTFzi\nUTNr6lGAlyWmWTgsEfhHAL4i/i8CeEJrvaaU+iiA/6WUekZr/UDtZdVA8ZGHBV5UtVQqYW5uDrlc\nDr29vUgmk+Y8F480bzHnQE9PD1544QUEAgFMT0+b9Ne5XA7lctl4qNFlmQgeCASMeMGcdVRWBQIB\nwxVQoUiEkkVVXToCiXw8Zj8z8/hL06Tsw0ZaCa7FZ9+HOhaaR6VLsbxXJpMx8QAAzHPT54JuxPl8\nHplMBqVSCSMjI3jyyScRCoVw/fp1kxWY8R1KKSN+lUolYyHQWmN5eRnT09OYm5vD8vIyNjc3DcFm\nghJ7Tg8KR0koGhWDXb/3gwMTAaVUO4AvAPiouHERQLH6+wdKqUkAl1CpUmQPuKb4iNfCOgpwyc31\n2KtsNlvjv85ilz09PchkMgiFQub/2NgYyuUyzp49a3Lazc7OYn193diyiWgbGxvGwtDe3o5oNGqS\nWmqtjYssFz0AQzT8fr+5jlwFZVkpPthKO4mY0rPPxRVwjuzd2/Zd8NJNyHtRzyFjB3g951QG6/A5\nI5EIEokEotEo2trajO2eXNO5c+dw6dIl9PT0mIxQ1NXQm5CJQehA1NbWhtXVVdy5cwe3bt3CysoK\nNjY2DPfhlVvSRfwaXWv12nu1Ocj95LUHxZ/DcAKfBvCB1nqOB5RSvQDSWus9pdQYKnUH7h7iHg+A\nvWibvbbRdqVSCalUytSWY/xBd3e32aFnZmYQi8UQi8UAVEyML7zwAoaGhrC8vIzr16+jUCgY92CK\nHdvb26Z8FncumcpKltbijkqk8fl86OrqMkk1pclRIjU/cnGT0NhzaLO/9rVec6f1/WKtNiGgv393\nd7cxBVIvsr29bbTuoVDIELVkMmm09WTlfT6fCRHu7+83Nn6O58knn4Sq+g1sbW2ZOU0mk/D7/Vhf\nX8fMzAxu3LiBu3fvIpPJYGtrqwbx7V1fRlE2CwfdzA5zzWE3zwPVHdBa/zEq1Ye/YjX/cQD/Vim1\nA6AM4Fe01ulDjfAQ4OIACPVelqSqTEBK+zPt28lkEuFwGLlcroZdp1/6wMAAOjs7jca5WCyawpVU\nHtI1ltfR/4DEgCHKdM1lAkwSpHw+bzgDWhskAbBdlclp2Ao72/znIhIuqBc5R4LHZB225YSJQWme\ni0ajxuknm81iZ2cH0WgUFy5cQH9/P1KplEkUQo5IKYV8Pm+qD1MRGwqFoJTC0tISxsfH8f7772N+\nft7oaGzCaMNhiADner/zzSKwvZaPkms+MbEDzVDQw8patv3aiw2TPvodHR0YHh7G0NCQKUoZi8WQ\nTCaNBpv2a+7qVCBOTExgenoam5ubJkKxWCwinU4brkApZRyPuMBZ7YY+9sD9aDxG0pEwEDnISdjl\n18lyU1GtfIQAABEGSURBVMcgnYDseoiS/a/nIsvxSo5Ea43NzU1sbGwYF166/nJ8khCMjY0ZJyCK\nTm1tbRgaGsJTTz2FaDQKACZ6U4on5DhIREOhEPx+P9LpNKampjAxMYHbt29jeXm5po4Ex2kjFn9T\nzDpocNBRwFEo/BzgjB14LIkA+/AyfdlEgIuN+ev6+/vNrkRfdmqyqdiSqarK5bJROE5OTuL69evo\n6ekxbD7ZUzoPcaeXUYhc6EQoWSSFWXN5jruvVMoR8WXgj0wDJot0EtG4c5OgkahRRCkUCtja2jLj\nInHK5/MoFotQSploQCpL+eGckTjJACwS1a6uLsNNcJ6oAGRxUM6vUgobGxtIpVKYmprC5OQk5ufn\njX5gP3lfgq1wfdRE4KjYfAec7gAiCYfRC7j6sU1BgNsExpLXzD4bj8eNwwoXcSQSQSwWqwkP7urq\nMtxDb2+vCVHu7u7G2bNnTdKMtbU19PT0GPmWHAO94ZiDQC5o7uJ0UabzEZNvcCGTEFCGB2rz9skU\n65JDooKPRIP9y2y9DKml7E/kJzfT3t5uNP5UhhLRuru7jemOir1CoYDV1VUjvhQKBfh8PqPlp/dl\nW1ul9HwqlUIqlcLs7CwWFhawuLhosgfJcdrvvJ4ocxTg4jTsNeW656PemE8UJwA0PgFHbX6xX5Rk\no8vlcg0ryUXOCLSenh6jie7o6DDBQxcvXkQ0GjXXU16dnp5GKpXCzs4OBgYGjBgxPT2NpaUlU4k3\nm80il8vV7IYy/z2RXo7Njukn0vKcZKX3m3NZBpxEQIJdh0FyG2T/WQEoEAgY0yp9JLq7uzE6Omqs\nKKwlSRNpX1+f8RGQvhNaa6TTaWxsbGBhYQFzc3PGzp/NZk0MgZfc38haOwprleu+XgrWRwQfHk7g\nYYALKWzlGoHIRJs+C5v29PQgHo+bgqe9vb2GrSYx2N7eNiLE7u6uKd3N7Mj5fN6UtMpms8ZhaWVl\nxRAOBsswbp4iAVlrsvgyQEl6wElXX5clQOoQiNySY2I/3NHJ4tO9lyW9gsEgfD6fUX7SosHxMU+f\nz+dDKBTCyMiIKezZ3t5u9ACsEEXT7dbWltn9U6kUVldXDeLzOfdTWjayHux10eg1rvYPQ6F3VNDi\nBBx9SYWazFMvTWnS1567b2dnJ3p7ezE8PIxIJIInnnjCOLwMDQ2ZnZD18og8rHVny+lk6aldT6fT\nxp99e3sbqVQK6XQa6+vrWFlZMUo67rrlchmzs7NYXl42jk4u8cdenDLfgdQhMMkmlXt0zpG5F2U1\nJgIz+dJVlyJJOBw2YkIikTAiDVOAMbyXnBGderLZLDKZjKkkJU2wruc56JpohIi4WHrX/J4QOPmK\nQaC5CTsqQuDqx94NZZBPvXv7fD5Eo1EMDQ1hbGzMKAuZ8qqtrc1Uu6FMLIlLMBg0ufeYRLOtrc3k\nwucYWG0HqJTWmpmZweLiItLpNHK5HHZ3dxEOh+Hz+bC0tIR0Om3MknKnlIguFYhMl8YQZxI67vSy\nVDdwH/GkUk2mCSPHw1Rg/f39xpZPhF5YWDC5/uggRILHAiMsMiIjDl0EwPaVeNjwkLT5Rw0tItAM\ncLfnDk2bfqPQ2dmJF198Eb29vaZWXiAQMDIy8w309PSY8OSdnR309PQYDoC6B+D+7ixNe7u7u4YF\n5w5MrmBubg75fN7spnwG27zGHV9aDIg80slHEj9JMID7ocOSFee5eDxuPiwVTpNmsVjEBx98gJmZ\nGczNzdUQKipC7QKxtggjE7EAhw/a8bq+nl7hJOBQg9AiAs0AFxpZ3P2IgGvxxONxDA4Ooqenx9TH\nYzARbflkk+knQDaemnqZgKOrqwsdHR0Ih8MmCpFuuXQyIsHZ3t5GV1cXPvjgA1y/fh2ZTMYo2eio\nQ4TyijOQOgL6LjAFO6/lOCgOsEZDIpFAZ2cnhoeHEQ6HjSiyt7eHubk5bG5uYnV1FVeuXDGxE0zA\nIh165HzarLfLjOeFrPu9N1cf+ykHTwLuNAmngwgAjU3uw7bdsn+p0ZYZc21wEQHuppFIBMlk0pjL\niPhklWW0IDX91EXQ1i9jBhhHwEQYfr8fPT09ZmePRqPGzbajowNzc3OYn59HLBbD9PS0SadF6wL9\nAeRzSBOfZOW1rngC0reART74jNQZhEIhY1lYXV01xV7X1tYwMzODVCqFtbU1Yy7l3Mp8jPVk70aR\n8yDr5GGbDo8RPlxEoHpds/dpmsAQEcg+N2risccmd1LmxqOnnUQyKcsCqCEUwWDQOMcAMNwCM+kE\nAgFTiXdvbw/BYBD9/f3w+/3I5/MIBoM1JbqXl5dNMg1W0KHTUjgchlLK+P4zRDoYDJpsP4ODgybm\nn4SqVCqZoB8GCKVSKaPRt/UTlOvtd+Ta0W3Z/mGt3ROo0DsqOD1EADhebsDu1ytD0X52ZtnONjXS\nA1CW15bneD2JA4lAMpk0qdJtRR5NcPz09fVhaGjIaPO561Nbz2AbuiDTWWdzc9MoAHlvmWqcadOG\nh4eNw45USGazWaTTaayuriKbzWJtbQ2bm5vIZDImMpPPK4lAIwTWxfY3uvO7NoFTotA7KvjwEYHq\ntQ9jSDX976ev8DruNTbKurJOX73rpBmSTjjRaBThcBi9vb0mXJaL3OerlOmKRCImi44sqcb0ZhwH\nkZLKQ4oJJFwkWLJiEKMcKR7Qjs+4/42NDRM/wLRqrnmoJ2I1CvWIgNe6+hDv9vWgRQQOAuxfsqny\nGOBeUK7r7PE2Kp64xkOrBfUBiUTCJNSk4o8igTTVAffdhKXzEIkRzZJ2BKJSysQQlMtlwzHQeYm2\ne0ZIUrPvckiyPw9bwUtw6RoeM/hwEoHq9Uc9pJq+Zf8ywk62cSmyXGaketyB3We9c9KJiHoBcgjx\neByhUKimyjERm1YBmauAWnvpFEVLhDRR0qxnf5gjgbu6/dwcvxfXU09DXw/2EwMaFd0eI2i5DR8W\nbHOafe5R3J9gh/vSoSaXy2FpaclwBAxFpnJTlvumuzF3dqbjko5A0j+BxIL2eTsZpwxIcu26zSJ2\nM1zCY8reHwk0klTkDCrpxvsBaAB/qLX+faVUAsCfAxgFcA/AL2it11Xlrf0+gM8BKAD4x1rrdx/O\n8B8N2CKAXHCN7lxHQSRspPBSalFW54dmPgAmZTfNdxQBCDLgiM8qLRa2iCCf3wv57WtdY270+GHb\ntuBB8O3fBLsAfkNrfRnAKwB+VSl1GcCXAPyN1voigL+p/geAn0IlrdhFVBKJ/sGRj7pJOKpd2tYF\nEOot5GZ0GzZi2fdttE/bxEglJBNylstlo8CjEo+ae9sV17635D4kVyD/e3FKzbwHl0ghP66cgC04\nGDRSd2ARlSzC0FrnlFI3AQwD+DwqaccA4E8B/D8A/7p6/M905e28pZSKKaUGq/08FNiPbeT5gywY\nu2+5OL2IgYtYHHaxupSQrudyjXdvb8/kKFhbW6sJS6Y7rtcz2Pc7KDR6bUuOf/TQlE5AKTUK4EUA\nbwPoF4i9hIq4AFQIxKy4bK567KERgUagHtJ6gW0RqCcW1OvD1ozLc3Zf+0Gjuyl3ZobeAjA7NV1z\nm5XRDwv7Eayjvl8LGoOGiYBSKgzgrwD8utY6a+1yWinV1NtTx1B34Ch2MmrZG4lXt2VoewxeO7ls\nf5BnkOz59va28/4uK0ez92kUXMRSjqMFxwuN6ASglOpAhQB8WWv9terhZaXUYPX8IIBU9fg8gDPi\n8pHqsRrQWv+h1vpll8mi2mdjT/AQoR5r3Oj1Un4Far0BXX15mbq82PF6Y3HJ0ATpobjf/Q8K9rhl\nmHFLnj85sC8RqGr7/xjATa3174lTXwfwxervLwJ4VRz/JVWBVwBkHqY+4FGAlxa+kevqKckaUQQ2\nAs2Y0eoRFLY5DNj987f0QWjByYJ9nYWUUh8H8DqA66jUEgCA30JFL/AXAJ4AMI2KiTBdJRr/BcBP\nomIi/Cda6wcqEFn3eMBZiHDUsvJBwcvM1cj4bES39QJ2dtv9wKWAfNTI9bCISAseKpwuj0FCM+Nr\nlBA045Um29e7bj/lIL9d7aQjjlefLiJyGMLn1Xcz15+EtdOCpqDlMUiwEWC/xSxZW9tBptH71buP\nl5LOCzml1cKrrZcYUq/9QXUfLTjd8FgSgaMA20LQCGHwMiu6jjfCfUhoxNzpai8JSgvpH0947InA\nQXQOLpfZen3VY7f3Q3KXudHus1l7v9fvFjye8NgTgWbhMJr7w8jhB0HWFoK3oBFoyE+gBffhIL4C\nrmMHkem9+vcyy7WgBY3Ah4oTkMqtR32fh410jZrkWsjfgmbhxBOB47CBu0Aivh3hZh9zXSvbeHkF\neukYTsoctODDCSeeCJwUcAUQ1WvnAhei2+dd51oEoAUPEx47InAUu6qXTb8ZLX0LsVtwUuCxIwIH\nRT7brGYHAnm1a0ELTjo8dkTgqMCW86XjUAv5W3Ca4LElAs26/hJcO3293AItaMFJh1NBBJqR4xs1\nEx7G+aaZ6sQtaMFJh1NBBB4FtHbyFjyu8KEkAo2E2raQvgUtqMCHkghIaCF7C1pQH05s7MBRuP+2\nCEALWrA/nFgiYMNJSDzaghZ8GOGkiAOrWus8gFUeOIW7eA/E+E8pnPZnOO3jBx7uM5x1HTwROQYB\nQCn1fVf+s9MCp338wOl/htM+fuB4nuHUiAMtaEELHg60iEALWvCYw0kiAn943AM4JJz28QOn/xlO\n+/iBY3iGE6MTaEELWnA8cJI4gRa0oAXHAMdOBJRSP6mUuqWUmlBKfem4x9MoKKXuKaWuK6WuKaW+\nXz2WUEp9Vyl1p/odP+5xSlBK/YlSKqWUGhfHnGOu1pL8z9X38iOl1EvHN3IzVtf4f0cpNV99D9eU\nUp8T536zOv5bSqnPHs+o74NS6oxS6v8qpd5XSt1QSv3L6vHjfQd2ttpH+QHQBmASwBgAP4D3AFw+\nzjE1MfZ7AHqsY/8BwJeqv78E4N8f9zit8f04gJcAjO83ZgCfA/C/ASgArwB4+4SO/3cA/CtH28vV\n9dQJ4Fx1nbUd8/gHAbxU/d0N4HZ1nMf6Do6bE/gxABNa67ta6xKArwL4/DGP6TDweQB/Wv39pwD+\nwTGO5QHQWr8GIG0d9hrz5wH8ma7AWwBiqlqK/rjAY/xe8HkAX9VaF7XWUwAmUFlvxwZa60Wt9bvV\n3zkANwEM45jfwXETgWEAs+L/XPXYaQAN4P8opX6glPpn1WP9+n4Z9iUA/ccztKbAa8yn6d38WpVd\n/hMhgp3o8SulRgG8iEp172N9B8dNBE4zfFxr/RKAnwLwq0qpH5cndYWfO1Wml9M4ZgB/AOA8gBcA\nLAL43eMdzv6glAoD+CsAv661zspzx/EOjpsIzAM4I/6PVI+deNBaz1e/UwD+Jyqs5jLZtep36vhG\n2DB4jflUvBut9bLWek9rXQbwR7jP8p/I8SulOlAhAF/WWn+tevhY38FxE4F3AFxUSp1TSvkB/CKA\nrx/zmPYFpVRIKdXN3wA+A2AclbF/sdrsiwBePZ4RNgVeY/46gF+qaqhfAZARLOuJAUtG/jlU3gNQ\nGf8vKqU6lVLnAFwEcPVRj0+CqoTC/jGAm1rr3xOnjvcdHKe2VGhAb6Oivf3t4x5Pg2MeQ0Xz/B6A\nGxw3gCSAvwFwB8D3ACSOe6zWuL+CCsu8g4p8+cteY0ZFI/1fq+/lOoCXT+j4/3t1fD+qIs2gaP/b\n1fHfAvBTJ2D8H0eF1f8RgGvVz+eO+x20PAZb0ILHHI5bHGhBC1pwzNAiAi1owWMOLSLQghY85tAi\nAi1owWMOLSLQghY85tAiAi1owWMOLSLQghY85tAiAi1owWMO/x9cbhGbgW1+mwAAAABJRU5ErkJg\ngg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ck1I9ub34jYw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NZygcwwp0Sry",
        "colab": {}
      },
      "source": [
        "# credits: https://stackoverflow.com/questions/43547402/how-to-calculate-f1-macro-in-keras\n",
        "def recall(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Recall metric.\n",
        "    \n",
        "    Only computes a batch-wise average of recall.\n",
        "    \n",
        "    Computes the recall, a metric for multi-label classification of\n",
        "    how many relevant items are selected.\n",
        "    \"\"\"\n",
        "    \n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision(y_true, y_pred):\n",
        "    \"\"\"Precision metric.\n",
        "    \n",
        "    Only computes a batch-wise average of precision.\n",
        "    \n",
        "    Computes the precision, a metric for multi-label classification of\n",
        "    how many selected items are relevant.\n",
        "    \"\"\"\n",
        "    \n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "    precisionx = precision(y_true, y_pred)\n",
        "    recallx = recall(y_true, y_pred)\n",
        "    return 2*((precisionx*recallx)/(precisionx+recallx+K.epsilon()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bBbVuHAfgHnh",
        "colab": {}
      },
      "source": [
        "class SGDRScheduler(Callback):\n",
        "    '''Cosine annealing learning rate scheduler with periodic restarts.\n",
        "    # Usage\n",
        "        ```python\n",
        "            schedule = SGDRScheduler(min_lr=1e-5,\n",
        "                                     max_lr=1e-2,\n",
        "                                     steps_per_epoch=np.ceil(epoch_size/batch_size),\n",
        "                                     lr_decay=0.9,\n",
        "                                     cycle_length=5,\n",
        "                                     mult_factor=1.5)\n",
        "            model.fit(X_train, Y_train, epochs=100, callbacks=[schedule])\n",
        "        ```\n",
        "    # Arguments\n",
        "        min_lr: The lower bound of the learning rate range for the experiment.\n",
        "        max_lr: The upper bound of the learning rate range for the experiment.\n",
        "        steps_per_epoch: Number of mini-batches in the dataset. Calculated as `np.ceil(epoch_size/batch_size)`. \n",
        "        lr_decay: Reduce the max_lr after the completion of each cycle.\n",
        "                  Ex. To reduce the max_lr by 20% after each cycle, set this value to 0.8.\n",
        "        cycle_length: Initial number of epochs in a cycle.\n",
        "        mult_factor: Scale epochs_to_restart after each full cycle completion.\n",
        "    # References\n",
        "        Blog post: jeremyjordan.me/nn-learning-rate\n",
        "        Original paper: http://arxiv.org/abs/1608.03983\n",
        "    '''\n",
        "    def __init__(self,\n",
        "                 min_lr,\n",
        "                 max_lr,\n",
        "                 steps_per_epoch,\n",
        "                 lr_decay=1,\n",
        "                 cycle_length=10,\n",
        "                 mult_factor=2):\n",
        "\n",
        "        self.min_lr = min_lr\n",
        "        self.max_lr = max_lr\n",
        "        self.lr_decay = lr_decay\n",
        "\n",
        "        self.batch_since_restart = 0\n",
        "        self.next_restart = cycle_length\n",
        "\n",
        "        self.steps_per_epoch = steps_per_epoch\n",
        "\n",
        "        self.cycle_length = cycle_length\n",
        "        self.mult_factor = mult_factor\n",
        "\n",
        "        self.history = {}\n",
        "\n",
        "    def clr(self):\n",
        "        '''Calculate the learning rate.'''\n",
        "        fraction_to_restart = self.batch_since_restart / (self.steps_per_epoch * self.cycle_length)\n",
        "        lr = self.min_lr + 0.5 * (self.max_lr - self.min_lr) * (1 + np.cos(fraction_to_restart * np.pi))\n",
        "        return lr\n",
        "\n",
        "    def on_train_begin(self, logs={}):\n",
        "        '''Initialize the learning rate to the minimum value at the start of training.'''\n",
        "        logs = logs or {}\n",
        "        K.set_value(self.model.optimizer.lr, self.max_lr)\n",
        "\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        '''Record previous batch statistics and update the learning rate.'''\n",
        "        logs = logs or {}\n",
        "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
        "        for k, v in logs.items():\n",
        "            self.history.setdefault(k, []).append(v)\n",
        "\n",
        "        self.batch_since_restart += 1\n",
        "        K.set_value(self.model.optimizer.lr, self.clr())\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        '''Check for end of current cycle, apply restarts when necessary.'''\n",
        "        if epoch + 1 == self.next_restart:\n",
        "            self.batch_since_restart = 0\n",
        "            self.cycle_length = np.ceil(self.cycle_length * self.mult_factor)\n",
        "            self.next_restart += self.cycle_length\n",
        "            self.max_lr *= self.lr_decay\n",
        "            self.best_weights = self.model.get_weights()\n",
        "\n",
        "    def on_train_end(self, logs={}):\n",
        "        '''Set weights to the values from the end of the most recent cycle for best performance.'''\n",
        "        self.model.set_weights(self.best_weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IvvEuAAKfkV5",
        "colab": {}
      },
      "source": [
        "# copied from https://github.com/kobiso/CBAM-keras/blob/master/models/attention_module.py\n",
        "def cbam_block(cbam_feature, ratio=8):\n",
        "    \"\"\"Contains the implementation of Convolutional Block Attention Module(CBAM) block.\n",
        "    As described in https://arxiv.org/abs/1807.06521.\n",
        "    \"\"\"\n",
        "    \n",
        "    cbam_feature = channel_attention(cbam_feature, ratio)\n",
        "    cbam_feature = spatial_attention(cbam_feature)\n",
        "    return cbam_feature\n",
        "\n",
        "def channel_attention(input_feature, ratio=8):\n",
        "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
        "    channel = input_feature._keras_shape[channel_axis]\n",
        "    \n",
        "    shared_layer_one = Dense(channel//ratio,\n",
        "                             activation='relu',\n",
        "                             kernel_initializer='he_normal',\n",
        "                             use_bias=True,\n",
        "                             bias_initializer='zeros')\n",
        "    shared_layer_two = Dense(channel,\n",
        "                             kernel_initializer='he_normal',\n",
        "                             use_bias=True,\n",
        "                             bias_initializer='zeros')\n",
        "    \n",
        "    avg_pool = GlobalAveragePooling2D()(input_feature)    \n",
        "    avg_pool = Reshape((1,1,channel))(avg_pool)\n",
        "    assert avg_pool._keras_shape[1:] == (1,1,channel)\n",
        "    avg_pool = shared_layer_one(avg_pool)\n",
        "    assert avg_pool._keras_shape[1:] == (1,1,channel//ratio)\n",
        "    avg_pool = shared_layer_two(avg_pool)\n",
        "    assert avg_pool._keras_shape[1:] == (1,1,channel)\n",
        "    \n",
        "    max_pool = GlobalMaxPooling2D()(input_feature)\n",
        "    max_pool = Reshape((1,1,channel))(max_pool)\n",
        "    assert max_pool._keras_shape[1:] == (1,1,channel)\n",
        "    max_pool = shared_layer_one(max_pool)\n",
        "    assert max_pool._keras_shape[1:] == (1,1,channel//ratio)\n",
        "    max_pool = shared_layer_two(max_pool)\n",
        "    assert max_pool._keras_shape[1:] == (1,1,channel)\n",
        "    \n",
        "    cbam_feature = Add()([avg_pool,max_pool])\n",
        "    cbam_feature = Activation('sigmoid')(cbam_feature)\n",
        "\n",
        "    if K.image_data_format() == \"channels_first\":\n",
        "        cbam_feature = Permute((3, 1, 2))(cbam_feature)\n",
        "    \n",
        "    return multiply([input_feature, cbam_feature])\n",
        "\n",
        "def spatial_attention(input_feature):\n",
        "    kernel_size = 7\n",
        "    \n",
        "    if K.image_data_format() == \"channels_first\":\n",
        "        channel = input_feature._keras_shape[1]\n",
        "        cbam_feature = Permute((2,3,1))(input_feature)\n",
        "    else:\n",
        "        channel = input_feature._keras_shape[-1]\n",
        "        cbam_feature = input_feature\n",
        "    \n",
        "    avg_pool = Lambda(lambda x: K.mean(x, axis=3, keepdims=True))(cbam_feature)\n",
        "    assert avg_pool._keras_shape[-1] == 1\n",
        "    max_pool = Lambda(lambda x: K.max(x, axis=3, keepdims=True))(cbam_feature)\n",
        "    assert max_pool._keras_shape[-1] == 1\n",
        "    concat = Concatenate(axis=3)([avg_pool, max_pool])\n",
        "    assert concat._keras_shape[-1] == 2\n",
        "    cbam_feature = Conv2D(filters = 1,\n",
        "                    kernel_size=kernel_size,\n",
        "                    strides=1,\n",
        "                    padding='same',\n",
        "                    activation='sigmoid',\n",
        "                    kernel_initializer='he_normal',\n",
        "                    use_bias=False)(concat)\t\n",
        "    assert cbam_feature._keras_shape[-1] == 1\n",
        "    \n",
        "    if K.image_data_format() == \"channels_first\":\n",
        "        cbam_feature = Permute((3, 1, 2))(cbam_feature)\n",
        "        \n",
        "    return multiply([input_feature, cbam_feature])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "F4Ndx2vm6NZ3",
        "colab": {}
      },
      "source": [
        "# copied from https://gist.github.com/mjdietzx/5319e42637ed7ef095d430cb5c5e8c64\n",
        "def residual_block(y, nb_channels, _strides=(1, 1), _project_shortcut=False):\n",
        "    shortcut = y\n",
        "\n",
        "    # down-sampling is performed with a stride of 2\n",
        "    y = Conv2D(nb_channels, kernel_size=(3, 3), strides=_strides, padding='same')(y)\n",
        "    y = BatchNormalization()(y)\n",
        "    y = LeakyReLU()(y)\n",
        "\n",
        "    y = Conv2D(nb_channels, kernel_size=(3, 3), strides=(1, 1), padding='same')(y)\n",
        "    y = BatchNormalization()(y)\n",
        "\n",
        "    # identity shortcuts used directly when the input and output are of the same dimensions\n",
        "    if _project_shortcut or _strides != (1, 1):\n",
        "        # when the dimensions increase projection shortcut is used to match dimensions (done by 1×1 convolutions)\n",
        "        # when the shortcuts go across feature maps of two sizes, they are performed with a stride of 2\n",
        "        shortcut = Conv2D(nb_channels, kernel_size=(1, 1), strides=_strides, padding='same')(shortcut)\n",
        "        shortcut = BatchNormalization()(shortcut)\n",
        "\n",
        "    y = add([shortcut, y])\n",
        "    y = LeakyReLU()(y)\n",
        "\n",
        "    return y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EVUWz9lzfm6Y",
        "colab": {}
      },
      "source": [
        "def create_model():\n",
        "    \n",
        "    dropRate = 0.3\n",
        "    \n",
        "    init = Input(SHAPE)\n",
        "    x = Conv2D(32, (3, 3), activation=None, padding='same')(init) \n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(32, (3, 3), activation=None, padding='same')(x) \n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x1 = MaxPooling2D((2,2))(x)\n",
        "    \n",
        "    x = Conv2D(64, (3, 3), activation=None, padding='same')(x1)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = cbam_block(x)\n",
        "    x = residual_block(x, 64)\n",
        "    x2 = MaxPooling2D((2,2))(x)\n",
        "    \n",
        "    x = Conv2D(128, (3, 3), activation=None, padding='same')(x2)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = cbam_block(x)\n",
        "    x = residual_block(x, 128)\n",
        "    x3 = MaxPooling2D((2,2))(x)\n",
        "    \n",
        "    ginp1 = UpSampling2D(size=(2, 2), interpolation='bilinear')(x1)\n",
        "    ginp2 = UpSampling2D(size=(4, 4), interpolation='bilinear')(x2)\n",
        "    ginp3 = UpSampling2D(size=(8, 8), interpolation='bilinear')(x3)\n",
        "    \n",
        "    hypercolumn = Concatenate()([ginp1, ginp2, ginp3]) \n",
        "    gap = GlobalAveragePooling2D()(hypercolumn)\n",
        "\n",
        "    x = Dense(256, activation=None)(gap)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Dropout(dropRate)(x)\n",
        "    \n",
        "    x = Dense(256, activation=None)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    y = Dense(3, activation='softmax')(x)\n",
        "   \n",
        "    model = Model(init, y)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "w2V3AUW7fm-n",
        "outputId": "22bdffa9-ee5b-42ee-fb65-a535be46556d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = create_model()\n",
        "model.summary()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 224, 224, 32) 896         input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 224, 224, 32) 128         conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 224, 224, 32) 0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 224, 224, 32) 9248        activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 224, 224, 32) 128         conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 224, 224, 32) 0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2D)  (None, 112, 112, 32) 0           activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 112, 112, 64) 18496       max_pooling2d_7[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 112, 112, 64) 256         conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 112, 112, 64) 0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_7 (Glo (None, 64)           0           activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling2d_5 (GlobalM (None, 64)           0           activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "reshape_9 (Reshape)             (None, 1, 1, 64)     0           global_average_pooling2d_7[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "reshape_10 (Reshape)            (None, 1, 1, 64)     0           global_max_pooling2d_5[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_15 (Dense)                (None, 1, 1, 8)      520         reshape_9[0][0]                  \n",
            "                                                                 reshape_10[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_16 (Dense)                (None, 1, 1, 64)     576         dense_15[0][0]                   \n",
            "                                                                 dense_15[1][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_9 (Add)                     (None, 1, 1, 64)     0           dense_16[0][0]                   \n",
            "                                                                 dense_16[1][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 1, 1, 64)     0           add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "multiply_9 (Multiply)           (None, 112, 112, 64) 0           activation_19[0][0]              \n",
            "                                                                 activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lambda_9 (Lambda)               (None, 112, 112, 1)  0           multiply_9[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_10 (Lambda)              (None, 112, 112, 1)  0           multiply_9[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_7 (Concatenate)     (None, 112, 112, 2)  0           lambda_9[0][0]                   \n",
            "                                                                 lambda_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 112, 112, 1)  98          concatenate_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "multiply_10 (Multiply)          (None, 112, 112, 64) 0           multiply_9[0][0]                 \n",
            "                                                                 conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 112, 112, 64) 36928       multiply_10[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 112, 112, 64) 256         conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_9 (LeakyReLU)       (None, 112, 112, 64) 0           batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 112, 112, 64) 36928       leaky_re_lu_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 112, 112, 64) 256         conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_10 (Add)                    (None, 112, 112, 64) 0           multiply_10[0][0]                \n",
            "                                                                 batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_10 (LeakyReLU)      (None, 112, 112, 64) 0           add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2D)  (None, 56, 56, 64)   0           leaky_re_lu_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 56, 56, 128)  73856       max_pooling2d_8[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 56, 56, 128)  512         conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 56, 56, 128)  0           batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_8 (Glo (None, 128)          0           activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling2d_6 (GlobalM (None, 128)          0           activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "reshape_11 (Reshape)            (None, 1, 1, 128)    0           global_average_pooling2d_8[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "reshape_12 (Reshape)            (None, 1, 1, 128)    0           global_max_pooling2d_6[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_17 (Dense)                (None, 1, 1, 16)     2064        reshape_11[0][0]                 \n",
            "                                                                 reshape_12[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_18 (Dense)                (None, 1, 1, 128)    2176        dense_17[0][0]                   \n",
            "                                                                 dense_17[1][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_11 (Add)                    (None, 1, 1, 128)    0           dense_18[0][0]                   \n",
            "                                                                 dense_18[1][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 1, 1, 128)    0           add_11[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "multiply_11 (Multiply)          (None, 56, 56, 128)  0           activation_21[0][0]              \n",
            "                                                                 activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lambda_11 (Lambda)              (None, 56, 56, 1)    0           multiply_11[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lambda_12 (Lambda)              (None, 56, 56, 1)    0           multiply_11[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_8 (Concatenate)     (None, 56, 56, 2)    0           lambda_11[0][0]                  \n",
            "                                                                 lambda_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 56, 56, 1)    98          concatenate_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "multiply_12 (Multiply)          (None, 56, 56, 128)  0           multiply_11[0][0]                \n",
            "                                                                 conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 56, 56, 128)  147584      multiply_12[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 56, 56, 128)  512         conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_11 (LeakyReLU)      (None, 56, 56, 128)  0           batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 56, 56, 128)  147584      leaky_re_lu_11[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 56, 56, 128)  512         conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_12 (Add)                    (None, 56, 56, 128)  0           multiply_12[0][0]                \n",
            "                                                                 batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_12 (LeakyReLU)      (None, 56, 56, 128)  0           add_12[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2D)  (None, 28, 28, 128)  0           leaky_re_lu_12[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_7 (UpSampling2D)  (None, 224, 224, 32) 0           max_pooling2d_7[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_8 (UpSampling2D)  (None, 224, 224, 64) 0           max_pooling2d_8[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_9 (UpSampling2D)  (None, 224, 224, 128 0           max_pooling2d_9[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_9 (Concatenate)     (None, 224, 224, 224 0           up_sampling2d_7[0][0]            \n",
            "                                                                 up_sampling2d_8[0][0]            \n",
            "                                                                 up_sampling2d_9[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_9 (Glo (None, 224)          0           concatenate_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_19 (Dense)                (None, 256)          57600       global_average_pooling2d_9[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, 256)          1024        dense_19[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 256)          0           batch_normalization_29[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 256)          0           activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_20 (Dense)                (None, 256)          65792       dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_30 (BatchNo (None, 256)          1024        dense_20[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 256)          0           batch_normalization_30[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_21 (Dense)                (None, 3)            771         activation_24[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 605,823\n",
            "Trainable params: 603,519\n",
            "Non-trainable params: 2,304\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "schfFpcIfzZy",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MSHMA1gtMQ6e",
        "outputId": "9c12f42e-945c-4bd3-9a9a-2e0feffdc416",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "kf = KFold(n_splits=N_SPLITS, random_state=SEED, shuffle=True)\n",
        "\n",
        "for ix, (train_index, test_index) in enumerate(kf.split(range(len(dataset.split_train_test(\"train\")[0])))):\n",
        "                                               \n",
        "    tg = DATASET(SHAPE, BATCH_SIZE, train_index, BASE_DIR, SEED, TRAIN_TEST_RATIO, augment=True)\n",
        "    vg = DATASET(SHAPE, BATCH_SIZE, test_index , BASE_DIR, SEED, TRAIN_TEST_RATIO, augment=False)\n",
        "        \n",
        "    schedule = SGDRScheduler(min_lr=1e-6,\n",
        "                             max_lr=1e-3,\n",
        "                             steps_per_epoch=np.ceil(EPOCHS/BATCH_SIZE),\n",
        "                             lr_decay=0.9,\n",
        "                             cycle_length=10,\n",
        "                             mult_factor=2.)\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-3), metrics=[precision, recall, f1, 'acc'])\n",
        "\n",
        "    model_ckpt = \"BRAIN_TUMOR_FOLD_\"+str(ix)+\".h5\"\n",
        "    callbacks = [ModelCheckpoint(model_ckpt, monitor='val_loss', mode='min', verbose=1, save_best_only=True, save_weights_only=False),\n",
        "                 TensorBoard(log_dir='./log_'+str(ix), update_freq='batch'), \n",
        "                 schedule] \n",
        "                                               \n",
        "    model.fit_generator(tg.data_generator(),\n",
        "                        steps_per_epoch=len(train_index)//BATCH_SIZE,\n",
        "                        epochs=EPOCHS,\n",
        "                        verbose=2,\n",
        "                        validation_data=vg.data_generator(),\n",
        "                        validation_steps=len(test_index)//BATCH_SIZE,\n",
        "                        callbacks=callbacks)\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            " - 249s - loss: 0.7498 - precision: 0.7095 - recall: 0.6220 - f1: 0.6626 - acc: 0.6731 - val_loss: 0.8384 - val_precision: 0.6436 - val_recall: 0.5885 - val_f1: 0.6147 - val_acc: 0.6068\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.83836, saving model to BRAIN_TUMOR_FOLD_0.h5\n",
            "Epoch 2/100\n",
            " - 107s - loss: 0.6151 - precision: 0.7659 - recall: 0.7007 - f1: 0.7316 - acc: 0.7356 - val_loss: 4.0614 - val_precision: 0.3072 - val_recall: 0.3047 - val_f1: 0.3059 - val_acc: 0.3047\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.83836\n",
            "Epoch 3/100\n",
            " - 136s - loss: 0.5251 - precision: 0.7949 - recall: 0.7488 - f1: 0.7709 - acc: 0.7770 - val_loss: 2.5663 - val_precision: 0.3246 - val_recall: 0.3177 - val_f1: 0.3211 - val_acc: 0.3177\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.83836\n",
            "Epoch 4/100\n",
            " - 101s - loss: 0.4806 - precision: 0.8162 - recall: 0.7800 - f1: 0.7976 - acc: 0.7957 - val_loss: 3.7377 - val_precision: 0.3057 - val_recall: 0.2995 - val_f1: 0.3026 - val_acc: 0.3073\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.83836\n",
            "Epoch 5/100\n",
            " - 75s - loss: 0.4423 - precision: 0.8283 - recall: 0.7915 - f1: 0.8093 - acc: 0.8095 - val_loss: 1.6968 - val_precision: 0.2094 - val_recall: 0.2005 - val_f1: 0.2049 - val_acc: 0.2135\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.83836\n",
            "Epoch 6/100\n",
            " - 74s - loss: 0.4471 - precision: 0.8274 - recall: 0.7806 - f1: 0.8031 - acc: 0.8029 - val_loss: 0.9454 - val_precision: 0.6407 - val_recall: 0.5443 - val_f1: 0.5882 - val_acc: 0.5990\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.83836\n",
            "Epoch 7/100\n",
            " - 71s - loss: 0.3671 - precision: 0.8594 - recall: 0.8335 - f1: 0.8462 - acc: 0.8492 - val_loss: 3.5101 - val_precision: 0.5521 - val_recall: 0.5521 - val_f1: 0.5521 - val_acc: 0.5521\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.83836\n",
            "Epoch 8/100\n",
            " - 70s - loss: 0.4073 - precision: 0.8319 - recall: 0.8095 - f1: 0.8204 - acc: 0.8245 - val_loss: 2.1973 - val_precision: 0.4279 - val_recall: 0.4062 - val_f1: 0.4167 - val_acc: 0.4349\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.83836\n",
            "Epoch 9/100\n",
            " - 69s - loss: 0.3884 - precision: 0.8448 - recall: 0.8167 - f1: 0.8304 - acc: 0.8317 - val_loss: 3.0465 - val_precision: 0.4332 - val_recall: 0.4141 - val_f1: 0.4234 - val_acc: 0.4297\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.83836\n",
            "Epoch 10/100\n",
            " - 69s - loss: 0.3593 - precision: 0.8612 - recall: 0.8383 - f1: 0.8495 - acc: 0.8492 - val_loss: 0.3731 - val_precision: 0.8536 - val_recall: 0.8203 - val_f1: 0.8365 - val_acc: 0.8385\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.83836 to 0.37315, saving model to BRAIN_TUMOR_FOLD_0.h5\n",
            "Epoch 11/100\n",
            " - 69s - loss: 0.3097 - precision: 0.8826 - recall: 0.8642 - f1: 0.8732 - acc: 0.8738 - val_loss: 2.8096 - val_precision: 0.5234 - val_recall: 0.5234 - val_f1: 0.5234 - val_acc: 0.5234\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.37315\n",
            "Epoch 12/100\n",
            " - 69s - loss: 0.2544 - precision: 0.9108 - recall: 0.8948 - f1: 0.9027 - acc: 0.9032 - val_loss: 2.0007 - val_precision: 0.5260 - val_recall: 0.5260 - val_f1: 0.5260 - val_acc: 0.5260\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.37315\n",
            "Epoch 13/100\n",
            " - 69s - loss: 0.2722 - precision: 0.8980 - recall: 0.8798 - f1: 0.8888 - acc: 0.8894 - val_loss: 2.4297 - val_precision: 0.6224 - val_recall: 0.6224 - val_f1: 0.6224 - val_acc: 0.6224\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.37315\n",
            "Epoch 14/100\n",
            " - 68s - loss: 0.3015 - precision: 0.8963 - recall: 0.8816 - f1: 0.8888 - acc: 0.8918 - val_loss: 2.8173 - val_precision: 0.2708 - val_recall: 0.2708 - val_f1: 0.2708 - val_acc: 0.2708\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.37315\n",
            "Epoch 15/100\n",
            " - 68s - loss: 0.2614 - precision: 0.9039 - recall: 0.8876 - f1: 0.8956 - acc: 0.8984 - val_loss: 0.4051 - val_precision: 0.8512 - val_recall: 0.8333 - val_f1: 0.8421 - val_acc: 0.8542\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.37315\n",
            "Epoch 16/100\n",
            " - 67s - loss: 0.2612 - precision: 0.8934 - recall: 0.8816 - f1: 0.8874 - acc: 0.8882 - val_loss: 1.3289 - val_precision: 0.6742 - val_recall: 0.6432 - val_f1: 0.6582 - val_acc: 0.6615\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.37315\n",
            "Epoch 17/100\n",
            " - 67s - loss: 0.2965 - precision: 0.8796 - recall: 0.8660 - f1: 0.8727 - acc: 0.8726 - val_loss: 5.4888 - val_precision: 0.5156 - val_recall: 0.5156 - val_f1: 0.5156 - val_acc: 0.5156\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.37315\n",
            "Epoch 18/100\n",
            " - 67s - loss: 0.2208 - precision: 0.9250 - recall: 0.9105 - f1: 0.9176 - acc: 0.9171 - val_loss: 3.4534 - val_precision: 0.5651 - val_recall: 0.5651 - val_f1: 0.5651 - val_acc: 0.5651\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.37315\n",
            "Epoch 19/100\n",
            " - 67s - loss: 0.2195 - precision: 0.9165 - recall: 0.9044 - f1: 0.9104 - acc: 0.9093 - val_loss: 1.1053 - val_precision: 0.6170 - val_recall: 0.6094 - val_f1: 0.6131 - val_acc: 0.6094\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.37315\n",
            "Epoch 20/100\n",
            " - 67s - loss: 0.2634 - precision: 0.9070 - recall: 0.8966 - f1: 0.9018 - acc: 0.9014 - val_loss: 2.9688 - val_precision: 0.4491 - val_recall: 0.4453 - val_f1: 0.4472 - val_acc: 0.4453\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.37315\n",
            "Epoch 21/100\n",
            " - 66s - loss: 0.2487 - precision: 0.9095 - recall: 0.9002 - f1: 0.9048 - acc: 0.9056 - val_loss: 2.1824 - val_precision: 0.4535 - val_recall: 0.4453 - val_f1: 0.4493 - val_acc: 0.4557\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.37315\n",
            "Epoch 22/100\n",
            " - 66s - loss: 0.1880 - precision: 0.9315 - recall: 0.9231 - f1: 0.9272 - acc: 0.9279 - val_loss: 1.0776 - val_precision: 0.6886 - val_recall: 0.6797 - val_f1: 0.6841 - val_acc: 0.6849\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.37315\n",
            "Epoch 23/100\n",
            " - 66s - loss: 0.2271 - precision: 0.9210 - recall: 0.9111 - f1: 0.9160 - acc: 0.9147 - val_loss: 1.2854 - val_precision: 0.5970 - val_recall: 0.5703 - val_f1: 0.5830 - val_acc: 0.5833\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.37315\n",
            "Epoch 24/100\n",
            " - 66s - loss: 0.1965 - precision: 0.9219 - recall: 0.9135 - f1: 0.9176 - acc: 0.9165 - val_loss: 1.2411 - val_precision: 0.5181 - val_recall: 0.5156 - val_f1: 0.5169 - val_acc: 0.5182\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.37315\n",
            "Epoch 25/100\n",
            " - 65s - loss: 0.1737 - precision: 0.9359 - recall: 0.9219 - f1: 0.9288 - acc: 0.9285 - val_loss: 3.3998 - val_precision: 0.6120 - val_recall: 0.6120 - val_f1: 0.6120 - val_acc: 0.6120\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.37315\n",
            "Epoch 26/100\n",
            " - 65s - loss: 0.2205 - precision: 0.9190 - recall: 0.9075 - f1: 0.9132 - acc: 0.9123 - val_loss: 2.0395 - val_precision: 0.7135 - val_recall: 0.7135 - val_f1: 0.7135 - val_acc: 0.7135\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.37315\n",
            "Epoch 27/100\n",
            " - 65s - loss: 0.1950 - precision: 0.9364 - recall: 0.9291 - f1: 0.9327 - acc: 0.9333 - val_loss: 0.7744 - val_precision: 0.7684 - val_recall: 0.7370 - val_f1: 0.7522 - val_acc: 0.7500\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.37315\n",
            "Epoch 28/100\n",
            " - 65s - loss: 0.1896 - precision: 0.9285 - recall: 0.9213 - f1: 0.9249 - acc: 0.9231 - val_loss: 0.3199 - val_precision: 0.8712 - val_recall: 0.8620 - val_f1: 0.8666 - val_acc: 0.8620\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.37315 to 0.31993, saving model to BRAIN_TUMOR_FOLD_0.h5\n",
            "Epoch 29/100\n",
            " - 65s - loss: 0.1720 - precision: 0.9265 - recall: 0.9231 - f1: 0.9248 - acc: 0.9243 - val_loss: 0.3964 - val_precision: 0.8362 - val_recall: 0.8125 - val_f1: 0.8240 - val_acc: 0.8229\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.31993\n",
            "Epoch 30/100\n",
            " - 65s - loss: 0.1808 - precision: 0.9347 - recall: 0.9285 - f1: 0.9315 - acc: 0.9309 - val_loss: 0.3757 - val_precision: 0.8558 - val_recall: 0.8490 - val_f1: 0.8524 - val_acc: 0.8542\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.31993\n",
            "Epoch 31/100\n",
            " - 65s - loss: 0.2249 - precision: 0.9159 - recall: 0.9093 - f1: 0.9126 - acc: 0.9129 - val_loss: 0.9912 - val_precision: 0.6809 - val_recall: 0.6615 - val_f1: 0.6710 - val_acc: 0.6667\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.31993\n",
            "Epoch 32/100\n",
            " - 65s - loss: 0.1884 - precision: 0.9339 - recall: 0.9243 - f1: 0.9290 - acc: 0.9279 - val_loss: 0.5852 - val_precision: 0.7966 - val_recall: 0.7839 - val_f1: 0.7902 - val_acc: 0.7917\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.31993\n",
            "Epoch 33/100\n",
            " - 65s - loss: 0.1431 - precision: 0.9487 - recall: 0.9435 - f1: 0.9461 - acc: 0.9465 - val_loss: 0.4322 - val_precision: 0.8798 - val_recall: 0.8568 - val_f1: 0.8681 - val_acc: 0.8724\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.31993\n",
            "Epoch 34/100\n",
            " - 66s - loss: 0.1465 - precision: 0.9515 - recall: 0.9429 - f1: 0.9471 - acc: 0.9489 - val_loss: 0.3637 - val_precision: 0.8727 - val_recall: 0.8438 - val_f1: 0.8578 - val_acc: 0.8516\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.31993\n",
            "Epoch 35/100\n",
            " - 65s - loss: 0.1593 - precision: 0.9397 - recall: 0.9351 - f1: 0.9374 - acc: 0.9381 - val_loss: 1.4283 - val_precision: 0.6262 - val_recall: 0.6120 - val_f1: 0.6190 - val_acc: 0.6302\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.31993\n",
            "Epoch 36/100\n",
            " - 65s - loss: 0.1488 - precision: 0.9516 - recall: 0.9435 - f1: 0.9475 - acc: 0.9471 - val_loss: 1.3006 - val_precision: 0.5790 - val_recall: 0.5755 - val_f1: 0.5772 - val_acc: 0.5807\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.31993\n",
            "Epoch 37/100\n",
            " - 66s - loss: 0.1623 - precision: 0.9383 - recall: 0.9327 - f1: 0.9355 - acc: 0.9345 - val_loss: 1.8767 - val_precision: 0.5780 - val_recall: 0.5651 - val_f1: 0.5714 - val_acc: 0.5677\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.31993\n",
            "Epoch 38/100\n",
            " - 65s - loss: 0.1367 - precision: 0.9494 - recall: 0.9465 - f1: 0.9479 - acc: 0.9477 - val_loss: 0.2635 - val_precision: 0.8880 - val_recall: 0.8880 - val_f1: 0.8880 - val_acc: 0.8880\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.31993 to 0.26349, saving model to BRAIN_TUMOR_FOLD_0.h5\n",
            "Epoch 39/100\n",
            " - 65s - loss: 0.1516 - precision: 0.9420 - recall: 0.9369 - f1: 0.9394 - acc: 0.9387 - val_loss: 0.4745 - val_precision: 0.8363 - val_recall: 0.8255 - val_f1: 0.8308 - val_acc: 0.8307\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.26349\n",
            "Epoch 40/100\n",
            " - 65s - loss: 0.1041 - precision: 0.9638 - recall: 0.9603 - f1: 0.9620 - acc: 0.9621 - val_loss: 0.3821 - val_precision: 0.8476 - val_recall: 0.8411 - val_f1: 0.8443 - val_acc: 0.8411\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.26349\n",
            "Epoch 41/100\n",
            " - 65s - loss: 0.1274 - precision: 0.9515 - recall: 0.9459 - f1: 0.9487 - acc: 0.9501 - val_loss: 0.2275 - val_precision: 0.9242 - val_recall: 0.9219 - val_f1: 0.9230 - val_acc: 0.9245\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.26349 to 0.22747, saving model to BRAIN_TUMOR_FOLD_0.h5\n",
            "Epoch 42/100\n",
            " - 65s - loss: 0.1300 - precision: 0.9565 - recall: 0.9507 - f1: 0.9536 - acc: 0.9555 - val_loss: 2.2907 - val_precision: 0.5861 - val_recall: 0.5729 - val_f1: 0.5794 - val_acc: 0.5781\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.22747\n",
            "Epoch 43/100\n",
            " - 65s - loss: 0.1833 - precision: 0.9355 - recall: 0.9321 - f1: 0.9338 - acc: 0.9339 - val_loss: 0.7675 - val_precision: 0.7806 - val_recall: 0.7135 - val_f1: 0.7454 - val_acc: 0.7448\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.22747\n",
            "Epoch 44/100\n",
            " - 65s - loss: 0.1633 - precision: 0.9445 - recall: 0.9405 - f1: 0.9425 - acc: 0.9405 - val_loss: 1.7899 - val_precision: 0.4778 - val_recall: 0.4740 - val_f1: 0.4758 - val_acc: 0.4766\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.22747\n",
            "Epoch 45/100\n",
            " - 65s - loss: 0.1413 - precision: 0.9544 - recall: 0.9441 - f1: 0.9492 - acc: 0.9489 - val_loss: 0.2946 - val_precision: 0.8637 - val_recall: 0.8594 - val_f1: 0.8615 - val_acc: 0.8594\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.22747\n",
            "Epoch 46/100\n",
            " - 65s - loss: 0.1281 - precision: 0.9535 - recall: 0.9495 - f1: 0.9515 - acc: 0.9507 - val_loss: 0.3758 - val_precision: 0.8590 - val_recall: 0.8438 - val_f1: 0.8512 - val_acc: 0.8490\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.22747\n",
            "Epoch 47/100\n",
            " - 65s - loss: 0.1271 - precision: 0.9517 - recall: 0.9471 - f1: 0.9494 - acc: 0.9501 - val_loss: 0.4248 - val_precision: 0.8382 - val_recall: 0.8359 - val_f1: 0.8371 - val_acc: 0.8359\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.22747\n",
            "Epoch 48/100\n",
            " - 65s - loss: 0.1214 - precision: 0.9509 - recall: 0.9435 - f1: 0.9472 - acc: 0.9459 - val_loss: 1.0257 - val_precision: 0.7145 - val_recall: 0.7031 - val_f1: 0.7087 - val_acc: 0.7057\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.22747\n",
            "Epoch 49/100\n",
            " - 65s - loss: 0.1571 - precision: 0.9444 - recall: 0.9375 - f1: 0.9409 - acc: 0.9405 - val_loss: 3.6025 - val_precision: 0.5892 - val_recall: 0.5859 - val_f1: 0.5875 - val_acc: 0.5859\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.22747\n",
            "Epoch 50/100\n",
            " - 65s - loss: 0.1790 - precision: 0.9388 - recall: 0.9309 - f1: 0.9348 - acc: 0.9357 - val_loss: 1.3200 - val_precision: 0.6874 - val_recall: 0.6719 - val_f1: 0.6794 - val_acc: 0.6771\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.22747\n",
            "Epoch 51/100\n",
            " - 65s - loss: 0.1347 - precision: 0.9494 - recall: 0.9459 - f1: 0.9476 - acc: 0.9471 - val_loss: 0.8365 - val_precision: 0.7836 - val_recall: 0.7734 - val_f1: 0.7785 - val_acc: 0.7839\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.22747\n",
            "Epoch 52/100\n",
            " - 65s - loss: 0.0949 - precision: 0.9692 - recall: 0.9657 - f1: 0.9674 - acc: 0.9675 - val_loss: 0.2465 - val_precision: 0.9131 - val_recall: 0.9036 - val_f1: 0.9083 - val_acc: 0.9036\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.22747\n",
            "Epoch 53/100\n",
            " - 65s - loss: 0.0950 - precision: 0.9710 - recall: 0.9645 - f1: 0.9677 - acc: 0.9681 - val_loss: 0.5258 - val_precision: 0.8177 - val_recall: 0.8177 - val_f1: 0.8177 - val_acc: 0.8177\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.22747\n",
            "Epoch 54/100\n",
            " - 65s - loss: 0.1221 - precision: 0.9518 - recall: 0.9489 - f1: 0.9503 - acc: 0.9501 - val_loss: 0.7811 - val_precision: 0.7995 - val_recall: 0.7995 - val_f1: 0.7995 - val_acc: 0.7995\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.22747\n",
            "Epoch 55/100\n",
            " - 64s - loss: 0.1260 - precision: 0.9488 - recall: 0.9453 - f1: 0.9470 - acc: 0.9471 - val_loss: 0.3583 - val_precision: 0.8774 - val_recall: 0.8594 - val_f1: 0.8682 - val_acc: 0.8646\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.22747\n",
            "Epoch 56/100\n",
            " - 66s - loss: 0.1526 - precision: 0.9444 - recall: 0.9387 - f1: 0.9415 - acc: 0.9417 - val_loss: 2.3624 - val_precision: 0.4909 - val_recall: 0.4870 - val_f1: 0.4889 - val_acc: 0.4870\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.22747\n",
            "Epoch 57/100\n",
            " - 66s - loss: 0.1145 - precision: 0.9625 - recall: 0.9555 - f1: 0.9590 - acc: 0.9591 - val_loss: 0.5183 - val_precision: 0.8229 - val_recall: 0.8229 - val_f1: 0.8229 - val_acc: 0.8229\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.22747\n",
            "Epoch 58/100\n",
            " - 65s - loss: 0.1006 - precision: 0.9638 - recall: 0.9615 - f1: 0.9627 - acc: 0.9621 - val_loss: 0.3542 - val_precision: 0.8642 - val_recall: 0.8620 - val_f1: 0.8631 - val_acc: 0.8620\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.22747\n",
            "Epoch 59/100\n",
            " - 65s - loss: 0.1018 - precision: 0.9590 - recall: 0.9561 - f1: 0.9575 - acc: 0.9579 - val_loss: 0.2046 - val_precision: 0.9187 - val_recall: 0.9115 - val_f1: 0.9151 - val_acc: 0.9193\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.22747 to 0.20463, saving model to BRAIN_TUMOR_FOLD_0.h5\n",
            "Epoch 60/100\n",
            " - 65s - loss: 0.1124 - precision: 0.9596 - recall: 0.9543 - f1: 0.9569 - acc: 0.9579 - val_loss: 0.2459 - val_precision: 0.9059 - val_recall: 0.9036 - val_f1: 0.9048 - val_acc: 0.9036\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.20463\n",
            "Epoch 61/100\n",
            " - 65s - loss: 0.1255 - precision: 0.9547 - recall: 0.9489 - f1: 0.9518 - acc: 0.9507 - val_loss: 6.3548 - val_precision: 0.3438 - val_recall: 0.3438 - val_f1: 0.3437 - val_acc: 0.3438\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.20463\n",
            "Epoch 62/100\n",
            " - 65s - loss: 0.1403 - precision: 0.9505 - recall: 0.9465 - f1: 0.9485 - acc: 0.9483 - val_loss: 0.3279 - val_precision: 0.8824 - val_recall: 0.8620 - val_f1: 0.8720 - val_acc: 0.8724\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.20463\n",
            "Epoch 63/100\n",
            " - 64s - loss: 0.1168 - precision: 0.9590 - recall: 0.9561 - f1: 0.9576 - acc: 0.9573 - val_loss: 0.3051 - val_precision: 0.8750 - val_recall: 0.8750 - val_f1: 0.8750 - val_acc: 0.8750\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.20463\n",
            "Epoch 64/100\n",
            " - 64s - loss: 0.1069 - precision: 0.9627 - recall: 0.9609 - f1: 0.9618 - acc: 0.9615 - val_loss: 0.1919 - val_precision: 0.9268 - val_recall: 0.9219 - val_f1: 0.9243 - val_acc: 0.9245\n",
            "\n",
            "Epoch 00064: val_loss improved from 0.20463 to 0.19188, saving model to BRAIN_TUMOR_FOLD_0.h5\n",
            "Epoch 65/100\n",
            " - 65s - loss: 0.0995 - precision: 0.9608 - recall: 0.9579 - f1: 0.9593 - acc: 0.9591 - val_loss: 0.1754 - val_precision: 0.9193 - val_recall: 0.9193 - val_f1: 0.9193 - val_acc: 0.9193\n",
            "\n",
            "Epoch 00065: val_loss improved from 0.19188 to 0.17543, saving model to BRAIN_TUMOR_FOLD_0.h5\n",
            "Epoch 66/100\n",
            " - 65s - loss: 0.0711 - precision: 0.9777 - recall: 0.9754 - f1: 0.9765 - acc: 0.9772 - val_loss: 0.4371 - val_precision: 0.8623 - val_recall: 0.8464 - val_f1: 0.8542 - val_acc: 0.8542\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.17543\n",
            "Epoch 67/100\n",
            " - 65s - loss: 0.1142 - precision: 0.9626 - recall: 0.9585 - f1: 0.9605 - acc: 0.9603 - val_loss: 0.5653 - val_precision: 0.7991 - val_recall: 0.7786 - val_f1: 0.7887 - val_acc: 0.7839\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.17543\n",
            "Epoch 68/100\n",
            " - 66s - loss: 0.1201 - precision: 0.9517 - recall: 0.9465 - f1: 0.9491 - acc: 0.9477 - val_loss: 0.7503 - val_precision: 0.7202 - val_recall: 0.6979 - val_f1: 0.7088 - val_acc: 0.7161\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.17543\n",
            "Epoch 69/100\n",
            " - 66s - loss: 0.1307 - precision: 0.9535 - recall: 0.9483 - f1: 0.9509 - acc: 0.9501 - val_loss: 0.3081 - val_precision: 0.8903 - val_recall: 0.8880 - val_f1: 0.8892 - val_acc: 0.8906\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.17543\n",
            "Epoch 70/100\n",
            " - 67s - loss: 0.0849 - precision: 0.9741 - recall: 0.9700 - f1: 0.9720 - acc: 0.9718 - val_loss: 0.2001 - val_precision: 0.9181 - val_recall: 0.9036 - val_f1: 0.9108 - val_acc: 0.9062\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.17543\n",
            "Epoch 71/100\n",
            " - 65s - loss: 0.0912 - precision: 0.9656 - recall: 0.9633 - f1: 0.9645 - acc: 0.9645 - val_loss: 0.3428 - val_precision: 0.8902 - val_recall: 0.8854 - val_f1: 0.8878 - val_acc: 0.8906\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.17543\n",
            "Epoch 72/100\n",
            " - 65s - loss: 0.1223 - precision: 0.9582 - recall: 0.9531 - f1: 0.9557 - acc: 0.9561 - val_loss: 0.5857 - val_precision: 0.8724 - val_recall: 0.8724 - val_f1: 0.8724 - val_acc: 0.8724\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.17543\n",
            "Epoch 73/100\n",
            " - 65s - loss: 0.0907 - precision: 0.9674 - recall: 0.9639 - f1: 0.9656 - acc: 0.9651 - val_loss: 0.5795 - val_precision: 0.8248 - val_recall: 0.7839 - val_f1: 0.8038 - val_acc: 0.7995\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.17543\n",
            "Epoch 74/100\n",
            " - 65s - loss: 0.0980 - precision: 0.9686 - recall: 0.9645 - f1: 0.9665 - acc: 0.9663 - val_loss: 1.4636 - val_precision: 0.6701 - val_recall: 0.6667 - val_f1: 0.6684 - val_acc: 0.6693\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.17543\n",
            "Epoch 75/100\n",
            " - 65s - loss: 0.0701 - precision: 0.9747 - recall: 0.9730 - f1: 0.9738 - acc: 0.9742 - val_loss: 0.3105 - val_precision: 0.9026 - val_recall: 0.8958 - val_f1: 0.8992 - val_acc: 0.8958\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.17543\n",
            "Epoch 76/100\n",
            " - 65s - loss: 0.0731 - precision: 0.9728 - recall: 0.9700 - f1: 0.9714 - acc: 0.9730 - val_loss: 0.1584 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n",
            "\n",
            "Epoch 00076: val_loss improved from 0.17543 to 0.15837, saving model to BRAIN_TUMOR_FOLD_0.h5\n",
            "Epoch 77/100\n",
            " - 65s - loss: 0.0671 - precision: 0.9789 - recall: 0.9754 - f1: 0.9771 - acc: 0.9766 - val_loss: 0.1504 - val_precision: 0.9401 - val_recall: 0.9401 - val_f1: 0.9401 - val_acc: 0.9401\n",
            "\n",
            "Epoch 00077: val_loss improved from 0.15837 to 0.15044, saving model to BRAIN_TUMOR_FOLD_0.h5\n",
            "Epoch 78/100\n",
            " - 64s - loss: 0.0541 - precision: 0.9837 - recall: 0.9814 - f1: 0.9825 - acc: 0.9832 - val_loss: 0.2056 - val_precision: 0.9192 - val_recall: 0.9141 - val_f1: 0.9166 - val_acc: 0.9193\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.15044\n",
            "Epoch 79/100\n",
            " - 65s - loss: 0.0579 - precision: 0.9843 - recall: 0.9820 - f1: 0.9831 - acc: 0.9826 - val_loss: 0.3087 - val_precision: 0.9080 - val_recall: 0.8984 - val_f1: 0.9031 - val_acc: 0.8984\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.15044\n",
            "Epoch 80/100\n",
            " - 65s - loss: 0.0639 - precision: 0.9778 - recall: 0.9766 - f1: 0.9772 - acc: 0.9778 - val_loss: 0.3368 - val_precision: 0.8856 - val_recall: 0.8698 - val_f1: 0.8776 - val_acc: 0.8828\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.15044\n",
            "Epoch 81/100\n",
            " - 65s - loss: 0.0833 - precision: 0.9723 - recall: 0.9712 - f1: 0.9717 - acc: 0.9712 - val_loss: 0.1754 - val_precision: 0.9323 - val_recall: 0.9297 - val_f1: 0.9310 - val_acc: 0.9297\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.15044\n",
            "Epoch 82/100\n",
            " - 65s - loss: 0.1601 - precision: 0.9480 - recall: 0.9435 - f1: 0.9457 - acc: 0.9471 - val_loss: 1.6511 - val_precision: 0.6831 - val_recall: 0.6797 - val_f1: 0.6814 - val_acc: 0.6797\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.15044\n",
            "Epoch 83/100\n",
            " - 67s - loss: 0.1581 - precision: 0.9427 - recall: 0.9381 - f1: 0.9404 - acc: 0.9399 - val_loss: 1.2852 - val_precision: 0.6210 - val_recall: 0.6094 - val_f1: 0.6151 - val_acc: 0.6198\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.15044\n",
            "Epoch 84/100\n",
            " - 66s - loss: 0.1096 - precision: 0.9584 - recall: 0.9555 - f1: 0.9570 - acc: 0.9567 - val_loss: 2.6033 - val_precision: 0.6094 - val_recall: 0.6094 - val_f1: 0.6094 - val_acc: 0.6094\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.15044\n",
            "Epoch 85/100\n",
            " - 67s - loss: 0.1164 - precision: 0.9549 - recall: 0.9537 - f1: 0.9543 - acc: 0.9537 - val_loss: 0.3724 - val_precision: 0.8795 - val_recall: 0.8724 - val_f1: 0.8759 - val_acc: 0.8724\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.15044\n",
            "Epoch 86/100\n",
            " - 66s - loss: 0.1013 - precision: 0.9656 - recall: 0.9621 - f1: 0.9639 - acc: 0.9633 - val_loss: 0.3555 - val_precision: 0.8851 - val_recall: 0.8828 - val_f1: 0.8839 - val_acc: 0.8854\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.15044\n",
            "Epoch 87/100\n",
            " - 66s - loss: 0.0846 - precision: 0.9675 - recall: 0.9663 - f1: 0.9669 - acc: 0.9675 - val_loss: 0.2288 - val_precision: 0.9345 - val_recall: 0.9271 - val_f1: 0.9308 - val_acc: 0.9297\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.15044\n",
            "Epoch 88/100\n",
            " - 66s - loss: 0.0640 - precision: 0.9747 - recall: 0.9706 - f1: 0.9726 - acc: 0.9742 - val_loss: 0.2328 - val_precision: 0.9178 - val_recall: 0.9036 - val_f1: 0.9106 - val_acc: 0.9141\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.15044\n",
            "Epoch 89/100\n",
            " - 65s - loss: 0.0752 - precision: 0.9745 - recall: 0.9700 - f1: 0.9722 - acc: 0.9712 - val_loss: 0.1393 - val_precision: 0.9659 - val_recall: 0.9531 - val_f1: 0.9594 - val_acc: 0.9635\n",
            "\n",
            "Epoch 00089: val_loss improved from 0.15044 to 0.13931, saving model to BRAIN_TUMOR_FOLD_0.h5\n",
            "Epoch 90/100\n",
            " - 65s - loss: 0.0577 - precision: 0.9813 - recall: 0.9784 - f1: 0.9798 - acc: 0.9802 - val_loss: 0.1314 - val_precision: 0.9425 - val_recall: 0.9401 - val_f1: 0.9413 - val_acc: 0.9427\n",
            "\n",
            "Epoch 00090: val_loss improved from 0.13931 to 0.13142, saving model to BRAIN_TUMOR_FOLD_0.h5\n",
            "Epoch 91/100\n",
            " - 65s - loss: 0.0502 - precision: 0.9861 - recall: 0.9826 - f1: 0.9843 - acc: 0.9844 - val_loss: 0.2657 - val_precision: 0.9209 - val_recall: 0.9062 - val_f1: 0.9134 - val_acc: 0.9115\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.13142\n",
            "Epoch 92/100\n",
            " - 65s - loss: 0.0700 - precision: 0.9759 - recall: 0.9736 - f1: 0.9747 - acc: 0.9754 - val_loss: 0.1076 - val_precision: 0.9501 - val_recall: 0.9427 - val_f1: 0.9464 - val_acc: 0.9479\n",
            "\n",
            "Epoch 00092: val_loss improved from 0.13142 to 0.10758, saving model to BRAIN_TUMOR_FOLD_0.h5\n",
            "Epoch 93/100\n",
            " - 65s - loss: 0.0667 - precision: 0.9754 - recall: 0.9736 - f1: 0.9744 - acc: 0.9736 - val_loss: 0.6222 - val_precision: 0.7964 - val_recall: 0.7943 - val_f1: 0.7953 - val_acc: 0.7969\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.10758\n",
            "Epoch 94/100\n",
            " - 65s - loss: 0.0758 - precision: 0.9693 - recall: 0.9675 - f1: 0.9684 - acc: 0.9681 - val_loss: 1.3552 - val_precision: 0.7080 - val_recall: 0.6875 - val_f1: 0.6975 - val_acc: 0.6927\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.10758\n",
            "Epoch 95/100\n",
            " - 65s - loss: 0.0874 - precision: 0.9686 - recall: 0.9633 - f1: 0.9659 - acc: 0.9657 - val_loss: 1.2111 - val_precision: 0.6920 - val_recall: 0.6901 - val_f1: 0.6910 - val_acc: 0.6927\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.10758\n",
            "Epoch 96/100\n",
            " - 65s - loss: 0.0759 - precision: 0.9747 - recall: 0.9724 - f1: 0.9735 - acc: 0.9730 - val_loss: 0.7099 - val_precision: 0.8126 - val_recall: 0.8021 - val_f1: 0.8073 - val_acc: 0.8073\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.10758\n",
            "Epoch 97/100\n",
            " - 65s - loss: 0.1006 - precision: 0.9638 - recall: 0.9597 - f1: 0.9617 - acc: 0.9615 - val_loss: 0.1333 - val_precision: 0.9626 - val_recall: 0.9401 - val_f1: 0.9512 - val_acc: 0.9479\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.10758\n",
            "Epoch 98/100\n",
            " - 65s - loss: 0.0768 - precision: 0.9699 - recall: 0.9681 - f1: 0.9690 - acc: 0.9694 - val_loss: 0.7719 - val_precision: 0.7864 - val_recall: 0.7734 - val_f1: 0.7798 - val_acc: 0.7734\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.10758\n",
            "Epoch 99/100\n",
            " - 65s - loss: 0.0586 - precision: 0.9796 - recall: 0.9784 - f1: 0.9790 - acc: 0.9784 - val_loss: 0.1230 - val_precision: 0.9556 - val_recall: 0.9505 - val_f1: 0.9530 - val_acc: 0.9531\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.10758\n",
            "Epoch 100/100\n",
            " - 65s - loss: 0.0564 - precision: 0.9807 - recall: 0.9790 - f1: 0.9798 - acc: 0.9802 - val_loss: 0.0775 - val_precision: 0.9792 - val_recall: 0.9766 - val_f1: 0.9779 - val_acc: 0.9792\n",
            "\n",
            "Epoch 00100: val_loss improved from 0.10758 to 0.07747, saving model to BRAIN_TUMOR_FOLD_0.h5\n",
            "Epoch 1/100\n",
            " - 74s - loss: 0.1891 - precision: 0.9359 - recall: 0.9297 - f1: 0.9328 - acc: 0.9327 - val_loss: 0.9210 - val_precision: 0.6871 - val_recall: 0.6615 - val_f1: 0.6739 - val_acc: 0.6745\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.92104, saving model to BRAIN_TUMOR_FOLD_1.h5\n",
            "Epoch 2/100\n",
            " - 62s - loss: 0.1775 - precision: 0.9339 - recall: 0.9267 - f1: 0.9303 - acc: 0.9309 - val_loss: 2.2570 - val_precision: 0.6205 - val_recall: 0.6172 - val_f1: 0.6188 - val_acc: 0.6198\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.92104\n",
            "Epoch 3/100\n",
            " - 65s - loss: 0.1283 - precision: 0.9512 - recall: 0.9489 - f1: 0.9500 - acc: 0.9495 - val_loss: 0.6550 - val_precision: 0.8315 - val_recall: 0.8229 - val_f1: 0.8272 - val_acc: 0.8255\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.92104 to 0.65499, saving model to BRAIN_TUMOR_FOLD_1.h5\n",
            "Epoch 4/100\n",
            " - 65s - loss: 0.1598 - precision: 0.9380 - recall: 0.9291 - f1: 0.9335 - acc: 0.9339 - val_loss: 0.2464 - val_precision: 0.9160 - val_recall: 0.9089 - val_f1: 0.9124 - val_acc: 0.9167\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.65499 to 0.24638, saving model to BRAIN_TUMOR_FOLD_1.h5\n",
            "Epoch 5/100\n",
            " - 65s - loss: 0.1159 - precision: 0.9572 - recall: 0.9543 - f1: 0.9557 - acc: 0.9561 - val_loss: 0.7083 - val_precision: 0.8554 - val_recall: 0.8307 - val_f1: 0.8428 - val_acc: 0.8464\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.24638\n",
            "Epoch 6/100\n",
            " - 65s - loss: 0.1059 - precision: 0.9638 - recall: 0.9597 - f1: 0.9617 - acc: 0.9615 - val_loss: 0.3654 - val_precision: 0.8670 - val_recall: 0.8646 - val_f1: 0.8658 - val_acc: 0.8672\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.24638\n",
            "Epoch 7/100\n",
            " - 65s - loss: 0.1356 - precision: 0.9487 - recall: 0.9447 - f1: 0.9467 - acc: 0.9477 - val_loss: 1.0977 - val_precision: 0.7137 - val_recall: 0.6875 - val_f1: 0.7002 - val_acc: 0.6979\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.24638\n",
            "Epoch 8/100\n",
            " - 65s - loss: 0.1278 - precision: 0.9572 - recall: 0.9531 - f1: 0.9552 - acc: 0.9537 - val_loss: 0.7683 - val_precision: 0.8034 - val_recall: 0.7969 - val_f1: 0.8001 - val_acc: 0.7969\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.24638\n",
            "Epoch 9/100\n",
            " - 65s - loss: 0.1557 - precision: 0.9463 - recall: 0.9429 - f1: 0.9446 - acc: 0.9447 - val_loss: 0.7266 - val_precision: 0.7648 - val_recall: 0.7526 - val_f1: 0.7586 - val_acc: 0.7552\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.24638\n",
            "Epoch 10/100\n",
            " - 65s - loss: 0.1317 - precision: 0.9524 - recall: 0.9489 - f1: 0.9506 - acc: 0.9513 - val_loss: 1.8789 - val_precision: 0.4715 - val_recall: 0.4688 - val_f1: 0.4701 - val_acc: 0.4740\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.24638\n",
            "Epoch 11/100\n",
            " - 65s - loss: 0.1280 - precision: 0.9477 - recall: 0.9471 - f1: 0.9474 - acc: 0.9471 - val_loss: 0.7555 - val_precision: 0.8056 - val_recall: 0.7995 - val_f1: 0.8025 - val_acc: 0.8047\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.24638\n",
            "Epoch 12/100\n",
            " - 65s - loss: 0.0856 - precision: 0.9716 - recall: 0.9675 - f1: 0.9696 - acc: 0.9688 - val_loss: 0.1758 - val_precision: 0.9296 - val_recall: 0.9271 - val_f1: 0.9283 - val_acc: 0.9297\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.24638 to 0.17579, saving model to BRAIN_TUMOR_FOLD_1.h5\n",
            "Epoch 13/100\n",
            " - 65s - loss: 0.0832 - precision: 0.9687 - recall: 0.9651 - f1: 0.9669 - acc: 0.9663 - val_loss: 1.4074 - val_precision: 0.6744 - val_recall: 0.6641 - val_f1: 0.6692 - val_acc: 0.6719\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.17579\n",
            "Epoch 14/100\n",
            " - 65s - loss: 0.1138 - precision: 0.9555 - recall: 0.9537 - f1: 0.9546 - acc: 0.9555 - val_loss: 1.3556 - val_precision: 0.6992 - val_recall: 0.6901 - val_f1: 0.6946 - val_acc: 0.7031\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.17579\n",
            "Epoch 15/100\n",
            " - 65s - loss: 0.0779 - precision: 0.9699 - recall: 0.9694 - f1: 0.9696 - acc: 0.9700 - val_loss: 0.9259 - val_precision: 0.8021 - val_recall: 0.8021 - val_f1: 0.8021 - val_acc: 0.8021\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.17579\n",
            "Epoch 16/100\n",
            " - 65s - loss: 0.0844 - precision: 0.9765 - recall: 0.9736 - f1: 0.9750 - acc: 0.9754 - val_loss: 0.2612 - val_precision: 0.9089 - val_recall: 0.9089 - val_f1: 0.9089 - val_acc: 0.9089\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.17579\n",
            "Epoch 17/100\n",
            " - 65s - loss: 0.1129 - precision: 0.9650 - recall: 0.9621 - f1: 0.9636 - acc: 0.9633 - val_loss: 0.3248 - val_precision: 0.8915 - val_recall: 0.8750 - val_f1: 0.8831 - val_acc: 0.8880\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.17579\n",
            "Epoch 18/100\n",
            " - 65s - loss: 0.0851 - precision: 0.9698 - recall: 0.9669 - f1: 0.9684 - acc: 0.9681 - val_loss: 0.1644 - val_precision: 0.9397 - val_recall: 0.9349 - val_f1: 0.9373 - val_acc: 0.9401\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.17579 to 0.16436, saving model to BRAIN_TUMOR_FOLD_1.h5\n",
            "Epoch 19/100\n",
            " - 65s - loss: 0.1017 - precision: 0.9615 - recall: 0.9591 - f1: 0.9603 - acc: 0.9615 - val_loss: 0.6087 - val_precision: 0.7946 - val_recall: 0.7865 - val_f1: 0.7905 - val_acc: 0.7943\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.16436\n",
            "Epoch 20/100\n",
            " - 65s - loss: 0.0911 - precision: 0.9644 - recall: 0.9621 - f1: 0.9633 - acc: 0.9621 - val_loss: 0.7812 - val_precision: 0.6911 - val_recall: 0.6797 - val_f1: 0.6853 - val_acc: 0.6901\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.16436\n",
            "Epoch 21/100\n",
            " - 65s - loss: 0.1032 - precision: 0.9643 - recall: 0.9615 - f1: 0.9629 - acc: 0.9633 - val_loss: 0.7866 - val_precision: 0.7861 - val_recall: 0.7656 - val_f1: 0.7757 - val_acc: 0.7734\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.16436\n",
            "Epoch 22/100\n",
            " - 65s - loss: 0.0887 - precision: 0.9687 - recall: 0.9663 - f1: 0.9675 - acc: 0.9669 - val_loss: 0.5909 - val_precision: 0.8091 - val_recall: 0.8047 - val_f1: 0.8069 - val_acc: 0.8047\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.16436\n",
            "Epoch 23/100\n",
            " - 65s - loss: 0.1121 - precision: 0.9590 - recall: 0.9555 - f1: 0.9572 - acc: 0.9573 - val_loss: 4.2242 - val_precision: 0.5301 - val_recall: 0.5286 - val_f1: 0.5294 - val_acc: 0.5286\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.16436\n",
            "Epoch 24/100\n",
            " - 65s - loss: 0.0710 - precision: 0.9753 - recall: 0.9730 - f1: 0.9741 - acc: 0.9754 - val_loss: 0.1185 - val_precision: 0.9635 - val_recall: 0.9635 - val_f1: 0.9635 - val_acc: 0.9635\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.16436 to 0.11854, saving model to BRAIN_TUMOR_FOLD_1.h5\n",
            "Epoch 25/100\n",
            " - 65s - loss: 0.0762 - precision: 0.9729 - recall: 0.9700 - f1: 0.9714 - acc: 0.9712 - val_loss: 1.3626 - val_precision: 0.6536 - val_recall: 0.6536 - val_f1: 0.6536 - val_acc: 0.6536\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.11854\n",
            "Epoch 26/100\n",
            " - 65s - loss: 0.1025 - precision: 0.9639 - recall: 0.9633 - f1: 0.9636 - acc: 0.9639 - val_loss: 4.3609 - val_precision: 0.3750 - val_recall: 0.3750 - val_f1: 0.3750 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.11854\n",
            "Epoch 27/100\n",
            " - 66s - loss: 0.0837 - precision: 0.9681 - recall: 0.9681 - f1: 0.9681 - acc: 0.9681 - val_loss: 0.7776 - val_precision: 0.8082 - val_recall: 0.7969 - val_f1: 0.8024 - val_acc: 0.7995\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.11854\n",
            "Epoch 28/100\n",
            " - 65s - loss: 0.0516 - precision: 0.9831 - recall: 0.9802 - f1: 0.9816 - acc: 0.9808 - val_loss: 0.1512 - val_precision: 0.9505 - val_recall: 0.9505 - val_f1: 0.9505 - val_acc: 0.9505\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.11854\n",
            "Epoch 29/100\n",
            " - 65s - loss: 0.1087 - precision: 0.9643 - recall: 0.9591 - f1: 0.9617 - acc: 0.9627 - val_loss: 0.5366 - val_precision: 0.8041 - val_recall: 0.7891 - val_f1: 0.7964 - val_acc: 0.7995\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.11854\n",
            "Epoch 30/100\n",
            " - 65s - loss: 0.0834 - precision: 0.9687 - recall: 0.9669 - f1: 0.9678 - acc: 0.9675 - val_loss: 1.0444 - val_precision: 0.7797 - val_recall: 0.7734 - val_f1: 0.7766 - val_acc: 0.7760\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.11854\n",
            "Epoch 31/100\n",
            " - 65s - loss: 0.0928 - precision: 0.9657 - recall: 0.9645 - f1: 0.9651 - acc: 0.9651 - val_loss: 0.1864 - val_precision: 0.9164 - val_recall: 0.9141 - val_f1: 0.9152 - val_acc: 0.9141\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.11854\n",
            "Epoch 32/100\n",
            " - 65s - loss: 0.0686 - precision: 0.9765 - recall: 0.9748 - f1: 0.9756 - acc: 0.9748 - val_loss: 0.2407 - val_precision: 0.9271 - val_recall: 0.9271 - val_f1: 0.9271 - val_acc: 0.9271\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.11854\n",
            "Epoch 33/100\n",
            " - 65s - loss: 0.0459 - precision: 0.9831 - recall: 0.9820 - f1: 0.9825 - acc: 0.9826 - val_loss: 0.1775 - val_precision: 0.9349 - val_recall: 0.9349 - val_f1: 0.9349 - val_acc: 0.9349\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.11854\n",
            "Epoch 34/100\n",
            " - 65s - loss: 0.0487 - precision: 0.9856 - recall: 0.9826 - f1: 0.9841 - acc: 0.9838 - val_loss: 0.1469 - val_precision: 0.9737 - val_recall: 0.9635 - val_f1: 0.9685 - val_acc: 0.9635\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.11854\n",
            "Epoch 35/100\n",
            " - 65s - loss: 0.0666 - precision: 0.9783 - recall: 0.9748 - f1: 0.9765 - acc: 0.9760 - val_loss: 1.0974 - val_precision: 0.6842 - val_recall: 0.6719 - val_f1: 0.6780 - val_acc: 0.6771\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.11854\n",
            "Epoch 36/100\n",
            " - 65s - loss: 0.0688 - precision: 0.9718 - recall: 0.9718 - f1: 0.9718 - acc: 0.9718 - val_loss: 1.1166 - val_precision: 0.7344 - val_recall: 0.7344 - val_f1: 0.7344 - val_acc: 0.7344\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.11854\n",
            "Epoch 37/100\n",
            " - 65s - loss: 0.0930 - precision: 0.9675 - recall: 0.9669 - f1: 0.9672 - acc: 0.9675 - val_loss: 7.0787 - val_precision: 0.4726 - val_recall: 0.4714 - val_f1: 0.4720 - val_acc: 0.4714\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.11854\n",
            "Epoch 38/100\n",
            " - 64s - loss: 0.0921 - precision: 0.9675 - recall: 0.9651 - f1: 0.9663 - acc: 0.9669 - val_loss: 0.4272 - val_precision: 0.8984 - val_recall: 0.8984 - val_f1: 0.8984 - val_acc: 0.8984\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.11854\n",
            "Epoch 39/100\n",
            " - 65s - loss: 0.0624 - precision: 0.9784 - recall: 0.9766 - f1: 0.9774 - acc: 0.9778 - val_loss: 0.3336 - val_precision: 0.8978 - val_recall: 0.8906 - val_f1: 0.8942 - val_acc: 0.8958\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.11854\n",
            "Epoch 40/100\n",
            " - 65s - loss: 0.0592 - precision: 0.9813 - recall: 0.9796 - f1: 0.9804 - acc: 0.9802 - val_loss: 0.1934 - val_precision: 0.9401 - val_recall: 0.9401 - val_f1: 0.9401 - val_acc: 0.9401\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.11854\n",
            "Epoch 41/100\n",
            " - 65s - loss: 0.0597 - precision: 0.9801 - recall: 0.9790 - f1: 0.9795 - acc: 0.9790 - val_loss: 0.1363 - val_precision: 0.9609 - val_recall: 0.9609 - val_f1: 0.9609 - val_acc: 0.9609\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.11854\n",
            "Epoch 42/100\n",
            " - 65s - loss: 0.0587 - precision: 0.9777 - recall: 0.9772 - f1: 0.9774 - acc: 0.9772 - val_loss: 0.5724 - val_precision: 0.8411 - val_recall: 0.8411 - val_f1: 0.8411 - val_acc: 0.8411\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.11854\n",
            "Epoch 43/100\n",
            " - 65s - loss: 0.1093 - precision: 0.9548 - recall: 0.9525 - f1: 0.9537 - acc: 0.9531 - val_loss: 3.4002 - val_precision: 0.5234 - val_recall: 0.5234 - val_f1: 0.5234 - val_acc: 0.5234\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.11854\n",
            "Epoch 44/100\n",
            " - 65s - loss: 0.0756 - precision: 0.9729 - recall: 0.9706 - f1: 0.9717 - acc: 0.9712 - val_loss: 0.8003 - val_precision: 0.7604 - val_recall: 0.7604 - val_f1: 0.7604 - val_acc: 0.7604\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.11854\n",
            "Epoch 45/100\n",
            " - 65s - loss: 0.0544 - precision: 0.9802 - recall: 0.9790 - f1: 0.9796 - acc: 0.9796 - val_loss: 0.3285 - val_precision: 0.8710 - val_recall: 0.8620 - val_f1: 0.8664 - val_acc: 0.8672\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.11854\n",
            "Epoch 46/100\n",
            " - 65s - loss: 0.0497 - precision: 0.9813 - recall: 0.9790 - f1: 0.9801 - acc: 0.9796 - val_loss: 0.2679 - val_precision: 0.9035 - val_recall: 0.9010 - val_f1: 0.9023 - val_acc: 0.9036\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.11854\n",
            "Epoch 47/100\n",
            " - 65s - loss: 0.0496 - precision: 0.9820 - recall: 0.9820 - f1: 0.9820 - acc: 0.9820 - val_loss: 0.1503 - val_precision: 0.9634 - val_recall: 0.9557 - val_f1: 0.9595 - val_acc: 0.9583\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.11854\n",
            "Epoch 48/100\n",
            " - 65s - loss: 0.0629 - precision: 0.9784 - recall: 0.9778 - f1: 0.9781 - acc: 0.9784 - val_loss: 0.2066 - val_precision: 0.9105 - val_recall: 0.9036 - val_f1: 0.9070 - val_acc: 0.9036\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.11854\n",
            "Epoch 49/100\n",
            " - 65s - loss: 0.0749 - precision: 0.9717 - recall: 0.9694 - f1: 0.9705 - acc: 0.9712 - val_loss: 0.1807 - val_precision: 0.9060 - val_recall: 0.9036 - val_f1: 0.9048 - val_acc: 0.9062\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.11854\n",
            "Epoch 50/100\n",
            " - 65s - loss: 0.0613 - precision: 0.9801 - recall: 0.9784 - f1: 0.9792 - acc: 0.9784 - val_loss: 0.9619 - val_precision: 0.7656 - val_recall: 0.7656 - val_f1: 0.7656 - val_acc: 0.7656\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.11854\n",
            "Epoch 51/100\n",
            " - 65s - loss: 0.0424 - precision: 0.9874 - recall: 0.9862 - f1: 0.9868 - acc: 0.9874 - val_loss: 0.1739 - val_precision: 0.9316 - val_recall: 0.9193 - val_f1: 0.9254 - val_acc: 0.9193\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.11854\n",
            "Epoch 52/100\n",
            " - 65s - loss: 0.0494 - precision: 0.9838 - recall: 0.9832 - f1: 0.9835 - acc: 0.9832 - val_loss: 0.1367 - val_precision: 0.9556 - val_recall: 0.9531 - val_f1: 0.9544 - val_acc: 0.9531\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.11854\n",
            "Epoch 53/100\n",
            " - 65s - loss: 0.0434 - precision: 0.9874 - recall: 0.9856 - f1: 0.9865 - acc: 0.9862 - val_loss: 0.4075 - val_precision: 0.8490 - val_recall: 0.8490 - val_f1: 0.8490 - val_acc: 0.8490\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.11854\n",
            "Epoch 54/100\n",
            " - 65s - loss: 0.0478 - precision: 0.9862 - recall: 0.9844 - f1: 0.9853 - acc: 0.9850 - val_loss: 0.1830 - val_precision: 0.9243 - val_recall: 0.9219 - val_f1: 0.9231 - val_acc: 0.9219\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.11854\n",
            "Epoch 55/100\n",
            " - 65s - loss: 0.0578 - precision: 0.9795 - recall: 0.9778 - f1: 0.9786 - acc: 0.9784 - val_loss: 0.3547 - val_precision: 0.8849 - val_recall: 0.8828 - val_f1: 0.8839 - val_acc: 0.8828\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.11854\n",
            "Epoch 56/100\n",
            " - 66s - loss: 0.0740 - precision: 0.9711 - recall: 0.9700 - f1: 0.9705 - acc: 0.9712 - val_loss: 0.3694 - val_precision: 0.8660 - val_recall: 0.8568 - val_f1: 0.8613 - val_acc: 0.8646\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.11854\n",
            "Epoch 57/100\n",
            " - 65s - loss: 0.0745 - precision: 0.9735 - recall: 0.9730 - f1: 0.9732 - acc: 0.9736 - val_loss: 0.1097 - val_precision: 0.9632 - val_recall: 0.9531 - val_f1: 0.9581 - val_acc: 0.9635\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.11854 to 0.10973, saving model to BRAIN_TUMOR_FOLD_1.h5\n",
            "Epoch 58/100\n",
            " - 65s - loss: 0.0373 - precision: 0.9856 - recall: 0.9844 - f1: 0.9850 - acc: 0.9856 - val_loss: 0.0834 - val_precision: 0.9712 - val_recall: 0.9661 - val_f1: 0.9687 - val_acc: 0.9714\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.10973 to 0.08341, saving model to BRAIN_TUMOR_FOLD_1.h5\n",
            "Epoch 59/100\n",
            " - 65s - loss: 0.0460 - precision: 0.9856 - recall: 0.9850 - f1: 0.9853 - acc: 0.9850 - val_loss: 0.1203 - val_precision: 0.9505 - val_recall: 0.9505 - val_f1: 0.9505 - val_acc: 0.9505\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.08341\n",
            "Epoch 60/100\n",
            " - 65s - loss: 0.0399 - precision: 0.9843 - recall: 0.9838 - f1: 0.9841 - acc: 0.9844 - val_loss: 0.1730 - val_precision: 0.9401 - val_recall: 0.9401 - val_f1: 0.9401 - val_acc: 0.9401\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.08341\n",
            "Epoch 61/100\n",
            " - 65s - loss: 0.0636 - precision: 0.9771 - recall: 0.9760 - f1: 0.9765 - acc: 0.9766 - val_loss: 2.0691 - val_precision: 0.6432 - val_recall: 0.6432 - val_f1: 0.6432 - val_acc: 0.6432\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.08341\n",
            "Epoch 62/100\n",
            " - 65s - loss: 0.0902 - precision: 0.9662 - recall: 0.9639 - f1: 0.9651 - acc: 0.9651 - val_loss: 0.1641 - val_precision: 0.9609 - val_recall: 0.9609 - val_f1: 0.9609 - val_acc: 0.9609\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.08341\n",
            "Epoch 63/100\n",
            " - 65s - loss: 0.0657 - precision: 0.9760 - recall: 0.9748 - f1: 0.9754 - acc: 0.9754 - val_loss: 0.6342 - val_precision: 0.7998 - val_recall: 0.7917 - val_f1: 0.7957 - val_acc: 0.7995\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.08341\n",
            "Epoch 64/100\n",
            " - 65s - loss: 0.0561 - precision: 0.9808 - recall: 0.9802 - f1: 0.9805 - acc: 0.9802 - val_loss: 0.3006 - val_precision: 0.9239 - val_recall: 0.9193 - val_f1: 0.9216 - val_acc: 0.9193\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.08341\n",
            "Epoch 65/100\n",
            " - 66s - loss: 0.0545 - precision: 0.9814 - recall: 0.9808 - f1: 0.9811 - acc: 0.9808 - val_loss: 0.1656 - val_precision: 0.9478 - val_recall: 0.9453 - val_f1: 0.9465 - val_acc: 0.9453\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.08341\n",
            "Epoch 66/100\n",
            " - 66s - loss: 0.0402 - precision: 0.9850 - recall: 0.9832 - f1: 0.9841 - acc: 0.9838 - val_loss: 0.1628 - val_precision: 0.9427 - val_recall: 0.9427 - val_f1: 0.9427 - val_acc: 0.9427\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.08341\n",
            "Epoch 67/100\n",
            " - 66s - loss: 0.0846 - precision: 0.9735 - recall: 0.9706 - f1: 0.9720 - acc: 0.9730 - val_loss: 2.9525 - val_precision: 0.5731 - val_recall: 0.5703 - val_f1: 0.5717 - val_acc: 0.5703\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.08341\n",
            "Epoch 68/100\n",
            " - 66s - loss: 0.1068 - precision: 0.9639 - recall: 0.9639 - f1: 0.9639 - acc: 0.9639 - val_loss: 4.2492 - val_precision: 0.5651 - val_recall: 0.5651 - val_f1: 0.5651 - val_acc: 0.5651\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.08341\n",
            "Epoch 69/100\n",
            " - 65s - loss: 0.0719 - precision: 0.9765 - recall: 0.9742 - f1: 0.9753 - acc: 0.9748 - val_loss: 1.8835 - val_precision: 0.6852 - val_recall: 0.6797 - val_f1: 0.6824 - val_acc: 0.6875\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.08341\n",
            "Epoch 70/100\n",
            " - 66s - loss: 0.0466 - precision: 0.9844 - recall: 0.9832 - f1: 0.9838 - acc: 0.9844 - val_loss: 1.2220 - val_precision: 0.7350 - val_recall: 0.7292 - val_f1: 0.7320 - val_acc: 0.7344\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.08341\n",
            "Epoch 71/100\n",
            " - 66s - loss: 0.0536 - precision: 0.9795 - recall: 0.9784 - f1: 0.9789 - acc: 0.9784 - val_loss: 5.0533 - val_precision: 0.5104 - val_recall: 0.5104 - val_f1: 0.5104 - val_acc: 0.5104\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.08341\n",
            "Epoch 72/100\n",
            " - 66s - loss: 0.0590 - precision: 0.9802 - recall: 0.9796 - f1: 0.9799 - acc: 0.9802 - val_loss: 1.0025 - val_precision: 0.7938 - val_recall: 0.7917 - val_f1: 0.7927 - val_acc: 0.7917\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.08341\n",
            "Epoch 73/100\n",
            " - 65s - loss: 0.0592 - precision: 0.9807 - recall: 0.9802 - f1: 0.9805 - acc: 0.9802 - val_loss: 0.7054 - val_precision: 0.7757 - val_recall: 0.7734 - val_f1: 0.7745 - val_acc: 0.7760\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.08341\n",
            "Epoch 74/100\n",
            " - 65s - loss: 0.0332 - precision: 0.9868 - recall: 0.9868 - f1: 0.9868 - acc: 0.9868 - val_loss: 0.5033 - val_precision: 0.8799 - val_recall: 0.8776 - val_f1: 0.8788 - val_acc: 0.8776\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.08341\n",
            "Epoch 75/100\n",
            " - 65s - loss: 0.0360 - precision: 0.9874 - recall: 0.9868 - f1: 0.9871 - acc: 0.9868 - val_loss: 0.1830 - val_precision: 0.9424 - val_recall: 0.9375 - val_f1: 0.9399 - val_acc: 0.9375\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.08341\n",
            "Epoch 76/100\n",
            " - 65s - loss: 0.0308 - precision: 0.9916 - recall: 0.9892 - f1: 0.9904 - acc: 0.9904 - val_loss: 0.1739 - val_precision: 0.9479 - val_recall: 0.9453 - val_f1: 0.9466 - val_acc: 0.9453\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.08341\n",
            "Epoch 77/100\n",
            " - 65s - loss: 0.0256 - precision: 0.9904 - recall: 0.9898 - f1: 0.9901 - acc: 0.9898 - val_loss: 0.1435 - val_precision: 0.9505 - val_recall: 0.9505 - val_f1: 0.9505 - val_acc: 0.9505\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.08341\n",
            "Epoch 78/100\n",
            " - 65s - loss: 0.0345 - precision: 0.9874 - recall: 0.9868 - f1: 0.9871 - acc: 0.9874 - val_loss: 0.1496 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.08341\n",
            "Epoch 79/100\n",
            " - 65s - loss: 0.0254 - precision: 0.9958 - recall: 0.9946 - f1: 0.9952 - acc: 0.9952 - val_loss: 0.1256 - val_precision: 0.9609 - val_recall: 0.9609 - val_f1: 0.9609 - val_acc: 0.9609\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.08341\n",
            "Epoch 80/100\n",
            " - 65s - loss: 0.0332 - precision: 0.9880 - recall: 0.9880 - f1: 0.9880 - acc: 0.9880 - val_loss: 0.3145 - val_precision: 0.9062 - val_recall: 0.9062 - val_f1: 0.9062 - val_acc: 0.9062\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.08341\n",
            "Epoch 81/100\n",
            " - 65s - loss: 0.0373 - precision: 0.9868 - recall: 0.9856 - f1: 0.9862 - acc: 0.9868 - val_loss: 0.1393 - val_precision: 0.9477 - val_recall: 0.9453 - val_f1: 0.9465 - val_acc: 0.9479\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.08341\n",
            "Epoch 82/100\n",
            " - 65s - loss: 0.0781 - precision: 0.9717 - recall: 0.9706 - f1: 0.9711 - acc: 0.9712 - val_loss: 0.9943 - val_precision: 0.7637 - val_recall: 0.7578 - val_f1: 0.7607 - val_acc: 0.7604\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.08341\n",
            "Epoch 83/100\n",
            " - 65s - loss: 0.0998 - precision: 0.9632 - recall: 0.9615 - f1: 0.9624 - acc: 0.9633 - val_loss: 1.1221 - val_precision: 0.6618 - val_recall: 0.6562 - val_f1: 0.6590 - val_acc: 0.6615\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.08341\n",
            "Epoch 84/100\n",
            " - 65s - loss: 0.0822 - precision: 0.9729 - recall: 0.9712 - f1: 0.9720 - acc: 0.9724 - val_loss: 1.1730 - val_precision: 0.7218 - val_recall: 0.7083 - val_f1: 0.7149 - val_acc: 0.7214\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.08341\n",
            "Epoch 85/100\n",
            " - 65s - loss: 0.0497 - precision: 0.9849 - recall: 0.9826 - f1: 0.9837 - acc: 0.9838 - val_loss: 0.2905 - val_precision: 0.9010 - val_recall: 0.9010 - val_f1: 0.9010 - val_acc: 0.9010\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.08341\n",
            "Epoch 86/100\n",
            " - 65s - loss: 0.0305 - precision: 0.9886 - recall: 0.9868 - f1: 0.9877 - acc: 0.9868 - val_loss: 0.4098 - val_precision: 0.8672 - val_recall: 0.8672 - val_f1: 0.8672 - val_acc: 0.8672\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.08341\n",
            "Epoch 87/100\n",
            " - 65s - loss: 0.0375 - precision: 0.9868 - recall: 0.9850 - f1: 0.9859 - acc: 0.9862 - val_loss: 0.0910 - val_precision: 0.9714 - val_recall: 0.9714 - val_f1: 0.9714 - val_acc: 0.9714\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.08341\n",
            "Epoch 88/100\n",
            " - 65s - loss: 0.0278 - precision: 0.9910 - recall: 0.9910 - f1: 0.9910 - acc: 0.9910 - val_loss: 0.1367 - val_precision: 0.9557 - val_recall: 0.9557 - val_f1: 0.9557 - val_acc: 0.9557\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.08341\n",
            "Epoch 89/100\n",
            " - 65s - loss: 0.0400 - precision: 0.9880 - recall: 0.9874 - f1: 0.9877 - acc: 0.9880 - val_loss: 0.1288 - val_precision: 0.9557 - val_recall: 0.9557 - val_f1: 0.9557 - val_acc: 0.9557\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.08341\n",
            "Epoch 90/100\n",
            " - 65s - loss: 0.0281 - precision: 0.9892 - recall: 0.9886 - f1: 0.9889 - acc: 0.9886 - val_loss: 0.1115 - val_precision: 0.9632 - val_recall: 0.9583 - val_f1: 0.9608 - val_acc: 0.9635\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.08341\n",
            "Epoch 91/100\n",
            " - 65s - loss: 0.0140 - precision: 0.9970 - recall: 0.9970 - f1: 0.9970 - acc: 0.9970 - val_loss: 0.1407 - val_precision: 0.9555 - val_recall: 0.9505 - val_f1: 0.9530 - val_acc: 0.9531\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.08341\n",
            "Epoch 92/100\n",
            " - 65s - loss: 0.0356 - precision: 0.9880 - recall: 0.9868 - f1: 0.9874 - acc: 0.9868 - val_loss: 0.1970 - val_precision: 0.9478 - val_recall: 0.9453 - val_f1: 0.9465 - val_acc: 0.9453\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.08341\n",
            "Epoch 93/100\n",
            " - 65s - loss: 0.0499 - precision: 0.9838 - recall: 0.9820 - f1: 0.9829 - acc: 0.9826 - val_loss: 0.7702 - val_precision: 0.8325 - val_recall: 0.8281 - val_f1: 0.8303 - val_acc: 0.8281\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.08341\n",
            "Epoch 94/100\n",
            " - 65s - loss: 0.0488 - precision: 0.9831 - recall: 0.9814 - f1: 0.9822 - acc: 0.9820 - val_loss: 1.1250 - val_precision: 0.8121 - val_recall: 0.8099 - val_f1: 0.8110 - val_acc: 0.8125\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.08341\n",
            "Epoch 95/100\n",
            " - 65s - loss: 0.0714 - precision: 0.9783 - recall: 0.9772 - f1: 0.9777 - acc: 0.9778 - val_loss: 0.3018 - val_precision: 0.9086 - val_recall: 0.9062 - val_f1: 0.9074 - val_acc: 0.9062\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.08341\n",
            "Epoch 96/100\n",
            " - 65s - loss: 0.0464 - precision: 0.9856 - recall: 0.9838 - f1: 0.9847 - acc: 0.9850 - val_loss: 0.3888 - val_precision: 0.8815 - val_recall: 0.8698 - val_f1: 0.8756 - val_acc: 0.8698\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.08341\n",
            "Epoch 97/100\n",
            " - 65s - loss: 0.0533 - precision: 0.9832 - recall: 0.9826 - f1: 0.9829 - acc: 0.9832 - val_loss: 0.4059 - val_precision: 0.8898 - val_recall: 0.8828 - val_f1: 0.8863 - val_acc: 0.8854\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.08341\n",
            "Epoch 98/100\n",
            " - 65s - loss: 0.0441 - precision: 0.9861 - recall: 0.9844 - f1: 0.9853 - acc: 0.9856 - val_loss: 0.3419 - val_precision: 0.9029 - val_recall: 0.8958 - val_f1: 0.8994 - val_acc: 0.8984\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.08341\n",
            "Epoch 99/100\n",
            " - 65s - loss: 0.0411 - precision: 0.9844 - recall: 0.9838 - f1: 0.9841 - acc: 0.9838 - val_loss: 0.2556 - val_precision: 0.9089 - val_recall: 0.9089 - val_f1: 0.9089 - val_acc: 0.9089\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.08341\n",
            "Epoch 100/100\n",
            " - 65s - loss: 0.0270 - precision: 0.9940 - recall: 0.9934 - f1: 0.9937 - acc: 0.9940 - val_loss: 0.0776 - val_precision: 0.9740 - val_recall: 0.9740 - val_f1: 0.9740 - val_acc: 0.9740\n",
            "\n",
            "Epoch 00100: val_loss improved from 0.08341 to 0.07765, saving model to BRAIN_TUMOR_FOLD_1.h5\n",
            "Epoch 1/100\n",
            " - 76s - loss: 0.1056 - precision: 0.9633 - recall: 0.9615 - f1: 0.9624 - acc: 0.9621 - val_loss: 4.8017 - val_precision: 0.4922 - val_recall: 0.4922 - val_f1: 0.4922 - val_acc: 0.4922\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 4.80165, saving model to BRAIN_TUMOR_FOLD_2.h5\n",
            "Epoch 2/100\n",
            " - 62s - loss: 0.1116 - precision: 0.9615 - recall: 0.9609 - f1: 0.9612 - acc: 0.9615 - val_loss: 0.4207 - val_precision: 0.8381 - val_recall: 0.8359 - val_f1: 0.8370 - val_acc: 0.8385\n",
            "\n",
            "Epoch 00002: val_loss improved from 4.80165 to 0.42065, saving model to BRAIN_TUMOR_FOLD_2.h5\n",
            "Epoch 3/100\n",
            " - 65s - loss: 0.0800 - precision: 0.9723 - recall: 0.9706 - f1: 0.9714 - acc: 0.9712 - val_loss: 1.3045 - val_precision: 0.6562 - val_recall: 0.6562 - val_f1: 0.6562 - val_acc: 0.6562\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.42065\n",
            "Epoch 4/100\n",
            " - 67s - loss: 0.0722 - precision: 0.9741 - recall: 0.9730 - f1: 0.9735 - acc: 0.9736 - val_loss: 1.0809 - val_precision: 0.6849 - val_recall: 0.6849 - val_f1: 0.6849 - val_acc: 0.6849\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.42065\n",
            "Epoch 5/100\n",
            " - 68s - loss: 0.0507 - precision: 0.9844 - recall: 0.9838 - f1: 0.9841 - acc: 0.9838 - val_loss: 1.9654 - val_precision: 0.6815 - val_recall: 0.6797 - val_f1: 0.6806 - val_acc: 0.6797\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.42065\n",
            "Epoch 6/100\n",
            " - 66s - loss: 0.0541 - precision: 0.9820 - recall: 0.9808 - f1: 0.9814 - acc: 0.9808 - val_loss: 0.3144 - val_precision: 0.8851 - val_recall: 0.8828 - val_f1: 0.8840 - val_acc: 0.8828\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.42065 to 0.31437, saving model to BRAIN_TUMOR_FOLD_2.h5\n",
            "Epoch 7/100\n",
            " - 66s - loss: 0.0629 - precision: 0.9789 - recall: 0.9772 - f1: 0.9780 - acc: 0.9778 - val_loss: 0.8860 - val_precision: 0.7838 - val_recall: 0.7734 - val_f1: 0.7786 - val_acc: 0.7786\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.31437\n",
            "Epoch 8/100\n",
            " - 66s - loss: 0.0577 - precision: 0.9819 - recall: 0.9808 - f1: 0.9814 - acc: 0.9808 - val_loss: 2.9504 - val_precision: 0.5286 - val_recall: 0.5286 - val_f1: 0.5286 - val_acc: 0.5286\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.31437\n",
            "Epoch 9/100\n",
            " - 66s - loss: 0.0671 - precision: 0.9760 - recall: 0.9748 - f1: 0.9754 - acc: 0.9748 - val_loss: 0.1805 - val_precision: 0.9115 - val_recall: 0.9115 - val_f1: 0.9115 - val_acc: 0.9115\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.31437 to 0.18055, saving model to BRAIN_TUMOR_FOLD_2.h5\n",
            "Epoch 10/100\n",
            " - 66s - loss: 0.0501 - precision: 0.9795 - recall: 0.9784 - f1: 0.9789 - acc: 0.9784 - val_loss: 0.2666 - val_precision: 0.8828 - val_recall: 0.8828 - val_f1: 0.8828 - val_acc: 0.8828\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.18055\n",
            "Epoch 11/100\n",
            " - 65s - loss: 0.0814 - precision: 0.9712 - recall: 0.9712 - f1: 0.9712 - acc: 0.9712 - val_loss: 1.1108 - val_precision: 0.7552 - val_recall: 0.7552 - val_f1: 0.7552 - val_acc: 0.7552\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.18055\n",
            "Epoch 12/100\n",
            " - 66s - loss: 0.0722 - precision: 0.9765 - recall: 0.9754 - f1: 0.9759 - acc: 0.9754 - val_loss: 0.2229 - val_precision: 0.9316 - val_recall: 0.9245 - val_f1: 0.9280 - val_acc: 0.9323\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.18055\n",
            "Epoch 13/100\n",
            " - 66s - loss: 0.0485 - precision: 0.9808 - recall: 0.9796 - f1: 0.9802 - acc: 0.9802 - val_loss: 0.8494 - val_precision: 0.7441 - val_recall: 0.7422 - val_f1: 0.7431 - val_acc: 0.7448\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.18055\n",
            "Epoch 14/100\n",
            " - 65s - loss: 0.0606 - precision: 0.9754 - recall: 0.9754 - f1: 0.9754 - acc: 0.9754 - val_loss: 0.1145 - val_precision: 0.9766 - val_recall: 0.9688 - val_f1: 0.9726 - val_acc: 0.9766\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.18055 to 0.11450, saving model to BRAIN_TUMOR_FOLD_2.h5\n",
            "Epoch 15/100\n",
            " - 65s - loss: 0.0447 - precision: 0.9850 - recall: 0.9844 - f1: 0.9847 - acc: 0.9850 - val_loss: 0.1144 - val_precision: 0.9531 - val_recall: 0.9531 - val_f1: 0.9531 - val_acc: 0.9531\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.11450 to 0.11444, saving model to BRAIN_TUMOR_FOLD_2.h5\n",
            "Epoch 16/100\n",
            " - 65s - loss: 0.0529 - precision: 0.9819 - recall: 0.9802 - f1: 0.9810 - acc: 0.9814 - val_loss: 0.4275 - val_precision: 0.8747 - val_recall: 0.8724 - val_f1: 0.8735 - val_acc: 0.8724\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.11444\n",
            "Epoch 17/100\n",
            " - 67s - loss: 0.0825 - precision: 0.9741 - recall: 0.9730 - f1: 0.9735 - acc: 0.9736 - val_loss: 1.0407 - val_precision: 0.7250 - val_recall: 0.7214 - val_f1: 0.7232 - val_acc: 0.7240\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.11444\n",
            "Epoch 18/100\n",
            " - 66s - loss: 0.0404 - precision: 0.9874 - recall: 0.9868 - f1: 0.9871 - acc: 0.9868 - val_loss: 0.3424 - val_precision: 0.8669 - val_recall: 0.8646 - val_f1: 0.8657 - val_acc: 0.8672\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.11444\n",
            "Epoch 19/100\n",
            " - 66s - loss: 0.0378 - precision: 0.9886 - recall: 0.9862 - f1: 0.9874 - acc: 0.9868 - val_loss: 0.0849 - val_precision: 0.9581 - val_recall: 0.9505 - val_f1: 0.9543 - val_acc: 0.9583\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.11444 to 0.08487, saving model to BRAIN_TUMOR_FOLD_2.h5\n",
            "Epoch 20/100\n",
            " - 68s - loss: 0.0456 - precision: 0.9844 - recall: 0.9844 - f1: 0.9844 - acc: 0.9844 - val_loss: 0.9345 - val_precision: 0.7752 - val_recall: 0.7708 - val_f1: 0.7730 - val_acc: 0.7708\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.08487\n",
            "Epoch 21/100\n",
            " - 68s - loss: 0.0397 - precision: 0.9874 - recall: 0.9874 - f1: 0.9874 - acc: 0.9874 - val_loss: 0.2769 - val_precision: 0.9107 - val_recall: 0.9062 - val_f1: 0.9085 - val_acc: 0.9062\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.08487\n",
            "Epoch 22/100\n",
            " - 66s - loss: 0.0396 - precision: 0.9856 - recall: 0.9856 - f1: 0.9856 - acc: 0.9856 - val_loss: 0.1551 - val_precision: 0.9398 - val_recall: 0.9323 - val_f1: 0.9360 - val_acc: 0.9323\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.08487\n",
            "Epoch 23/100\n",
            " - 66s - loss: 0.0531 - precision: 0.9820 - recall: 0.9814 - f1: 0.9817 - acc: 0.9814 - val_loss: 1.3475 - val_precision: 0.6886 - val_recall: 0.6849 - val_f1: 0.6867 - val_acc: 0.6849\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.08487\n",
            "Epoch 24/100\n",
            " - 66s - loss: 0.0701 - precision: 0.9777 - recall: 0.9766 - f1: 0.9771 - acc: 0.9766 - val_loss: 1.9741 - val_precision: 0.6484 - val_recall: 0.6484 - val_f1: 0.6484 - val_acc: 0.6484\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.08487\n",
            "Epoch 25/100\n",
            " - 66s - loss: 0.0485 - precision: 0.9826 - recall: 0.9820 - f1: 0.9823 - acc: 0.9820 - val_loss: 0.3044 - val_precision: 0.9089 - val_recall: 0.9089 - val_f1: 0.9089 - val_acc: 0.9089\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.08487\n",
            "Epoch 26/100\n",
            " - 65s - loss: 0.0727 - precision: 0.9741 - recall: 0.9724 - f1: 0.9732 - acc: 0.9730 - val_loss: 0.1899 - val_precision: 0.9505 - val_recall: 0.9505 - val_f1: 0.9505 - val_acc: 0.9505\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.08487\n",
            "Epoch 27/100\n",
            " - 65s - loss: 0.0445 - precision: 0.9837 - recall: 0.9832 - f1: 0.9835 - acc: 0.9832 - val_loss: 0.2951 - val_precision: 0.8958 - val_recall: 0.8958 - val_f1: 0.8958 - val_acc: 0.8958\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.08487\n",
            "Epoch 28/100\n",
            " - 66s - loss: 0.0468 - precision: 0.9838 - recall: 0.9838 - f1: 0.9838 - acc: 0.9838 - val_loss: 0.5516 - val_precision: 0.8567 - val_recall: 0.8438 - val_f1: 0.8501 - val_acc: 0.8568\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.08487\n",
            "Epoch 29/100\n",
            " - 65s - loss: 0.0629 - precision: 0.9753 - recall: 0.9736 - f1: 0.9744 - acc: 0.9748 - val_loss: 5.2527 - val_precision: 0.4712 - val_recall: 0.4688 - val_f1: 0.4700 - val_acc: 0.4714\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.08487\n",
            "Epoch 30/100\n",
            " - 66s - loss: 0.0660 - precision: 0.9790 - recall: 0.9778 - f1: 0.9784 - acc: 0.9790 - val_loss: 0.3985 - val_precision: 0.8750 - val_recall: 0.8750 - val_f1: 0.8750 - val_acc: 0.8750\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.08487\n",
            "Epoch 31/100\n",
            " - 65s - loss: 0.0443 - precision: 0.9844 - recall: 0.9838 - f1: 0.9841 - acc: 0.9844 - val_loss: 0.4976 - val_precision: 0.8521 - val_recall: 0.8385 - val_f1: 0.8453 - val_acc: 0.8464\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.08487\n",
            "Epoch 32/100\n",
            " - 66s - loss: 0.0459 - precision: 0.9862 - recall: 0.9862 - f1: 0.9862 - acc: 0.9862 - val_loss: 4.0876 - val_precision: 0.5260 - val_recall: 0.5260 - val_f1: 0.5260 - val_acc: 0.5260\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.08487\n",
            "Epoch 33/100\n",
            " - 66s - loss: 0.0404 - precision: 0.9868 - recall: 0.9856 - f1: 0.9862 - acc: 0.9856 - val_loss: 0.0933 - val_precision: 0.9505 - val_recall: 0.9505 - val_f1: 0.9505 - val_acc: 0.9505\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.08487\n",
            "Epoch 34/100\n",
            " - 65s - loss: 0.0403 - precision: 0.9826 - recall: 0.9820 - f1: 0.9823 - acc: 0.9826 - val_loss: 0.0541 - val_precision: 0.9661 - val_recall: 0.9661 - val_f1: 0.9661 - val_acc: 0.9661\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.08487 to 0.05410, saving model to BRAIN_TUMOR_FOLD_2.h5\n",
            "Epoch 35/100\n",
            " - 65s - loss: 0.0374 - precision: 0.9880 - recall: 0.9874 - f1: 0.9877 - acc: 0.9874 - val_loss: 0.0510 - val_precision: 0.9792 - val_recall: 0.9792 - val_f1: 0.9792 - val_acc: 0.9792\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.05410 to 0.05099, saving model to BRAIN_TUMOR_FOLD_2.h5\n",
            "Epoch 36/100\n",
            " - 66s - loss: 0.0544 - precision: 0.9802 - recall: 0.9802 - f1: 0.9802 - acc: 0.9802 - val_loss: 0.2939 - val_precision: 0.9245 - val_recall: 0.9245 - val_f1: 0.9245 - val_acc: 0.9245\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.05099\n",
            "Epoch 37/100\n",
            " - 65s - loss: 0.0745 - precision: 0.9706 - recall: 0.9706 - f1: 0.9706 - acc: 0.9706 - val_loss: 1.7133 - val_precision: 0.6630 - val_recall: 0.6615 - val_f1: 0.6622 - val_acc: 0.6641\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.05099\n",
            "Epoch 38/100\n",
            " - 65s - loss: 0.1032 - precision: 0.9609 - recall: 0.9591 - f1: 0.9600 - acc: 0.9609 - val_loss: 0.4032 - val_precision: 0.8721 - val_recall: 0.8698 - val_f1: 0.8709 - val_acc: 0.8698\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.05099\n",
            "Epoch 39/100\n",
            " - 66s - loss: 0.0637 - precision: 0.9777 - recall: 0.9766 - f1: 0.9771 - acc: 0.9766 - val_loss: 0.0371 - val_precision: 0.9870 - val_recall: 0.9870 - val_f1: 0.9870 - val_acc: 0.9870\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.05099 to 0.03706, saving model to BRAIN_TUMOR_FOLD_2.h5\n",
            "Epoch 40/100\n",
            " - 65s - loss: 0.0371 - precision: 0.9904 - recall: 0.9904 - f1: 0.9904 - acc: 0.9904 - val_loss: 0.0311 - val_precision: 0.9922 - val_recall: 0.9922 - val_f1: 0.9922 - val_acc: 0.9922\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.03706 to 0.03113, saving model to BRAIN_TUMOR_FOLD_2.h5\n",
            "Epoch 41/100\n",
            " - 65s - loss: 0.0481 - precision: 0.9838 - recall: 0.9832 - f1: 0.9835 - acc: 0.9838 - val_loss: 0.0716 - val_precision: 0.9766 - val_recall: 0.9766 - val_f1: 0.9766 - val_acc: 0.9766\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.03113\n",
            "Epoch 42/100\n",
            " - 65s - loss: 0.0600 - precision: 0.9789 - recall: 0.9772 - f1: 0.9780 - acc: 0.9784 - val_loss: 0.3601 - val_precision: 0.8464 - val_recall: 0.8464 - val_f1: 0.8464 - val_acc: 0.8464\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.03113\n",
            "Epoch 43/100\n",
            " - 65s - loss: 0.0801 - precision: 0.9705 - recall: 0.9694 - f1: 0.9699 - acc: 0.9706 - val_loss: 0.1627 - val_precision: 0.9349 - val_recall: 0.9349 - val_f1: 0.9349 - val_acc: 0.9349\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.03113\n",
            "Epoch 44/100\n",
            " - 66s - loss: 0.0613 - precision: 0.9778 - recall: 0.9778 - f1: 0.9778 - acc: 0.9778 - val_loss: 0.3308 - val_precision: 0.9187 - val_recall: 0.9115 - val_f1: 0.9150 - val_acc: 0.9141\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.03113\n",
            "Epoch 45/100\n",
            " - 65s - loss: 0.0359 - precision: 0.9910 - recall: 0.9898 - f1: 0.9904 - acc: 0.9898 - val_loss: 0.1497 - val_precision: 0.9501 - val_recall: 0.9453 - val_f1: 0.9477 - val_acc: 0.9479\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.03113\n",
            "Epoch 46/100\n",
            " - 66s - loss: 0.0246 - precision: 0.9940 - recall: 0.9928 - f1: 0.9934 - acc: 0.9934 - val_loss: 0.1072 - val_precision: 0.9609 - val_recall: 0.9583 - val_f1: 0.9596 - val_acc: 0.9609\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.03113\n",
            "Epoch 47/100\n",
            " - 66s - loss: 0.0354 - precision: 0.9874 - recall: 0.9868 - f1: 0.9871 - acc: 0.9874 - val_loss: 0.1872 - val_precision: 0.9323 - val_recall: 0.9323 - val_f1: 0.9323 - val_acc: 0.9323\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.03113\n",
            "Epoch 48/100\n",
            " - 65s - loss: 0.0468 - precision: 0.9837 - recall: 0.9820 - f1: 0.9828 - acc: 0.9832 - val_loss: 0.1503 - val_precision: 0.9453 - val_recall: 0.9453 - val_f1: 0.9453 - val_acc: 0.9453\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.03113\n",
            "Epoch 49/100\n",
            " - 65s - loss: 0.0520 - precision: 0.9813 - recall: 0.9802 - f1: 0.9808 - acc: 0.9802 - val_loss: 0.2147 - val_precision: 0.9165 - val_recall: 0.9115 - val_f1: 0.9139 - val_acc: 0.9115\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.03113\n",
            "Epoch 50/100\n",
            " - 66s - loss: 0.0692 - precision: 0.9754 - recall: 0.9742 - f1: 0.9748 - acc: 0.9754 - val_loss: 0.3287 - val_precision: 0.8977 - val_recall: 0.8906 - val_f1: 0.8941 - val_acc: 0.8932\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.03113\n",
            "Epoch 51/100\n",
            " - 66s - loss: 0.0487 - precision: 0.9844 - recall: 0.9844 - f1: 0.9844 - acc: 0.9844 - val_loss: 0.2028 - val_precision: 0.9477 - val_recall: 0.9427 - val_f1: 0.9452 - val_acc: 0.9427\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.03113\n",
            "Epoch 52/100\n",
            " - 66s - loss: 0.0458 - precision: 0.9850 - recall: 0.9850 - f1: 0.9850 - acc: 0.9850 - val_loss: 0.1336 - val_precision: 0.9580 - val_recall: 0.9479 - val_f1: 0.9529 - val_acc: 0.9479\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.03113\n",
            "Epoch 53/100\n",
            " - 66s - loss: 0.0287 - precision: 0.9892 - recall: 0.9880 - f1: 0.9886 - acc: 0.9886 - val_loss: 0.1047 - val_precision: 0.9635 - val_recall: 0.9635 - val_f1: 0.9635 - val_acc: 0.9635\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.03113\n",
            "Epoch 54/100\n",
            " - 66s - loss: 0.0375 - precision: 0.9874 - recall: 0.9856 - f1: 0.9865 - acc: 0.9862 - val_loss: 0.4679 - val_precision: 0.8911 - val_recall: 0.8750 - val_f1: 0.8829 - val_acc: 0.8750\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.03113\n",
            "Epoch 55/100\n",
            " - 66s - loss: 0.0338 - precision: 0.9874 - recall: 0.9868 - f1: 0.9871 - acc: 0.9868 - val_loss: 4.7340 - val_precision: 0.5339 - val_recall: 0.5339 - val_f1: 0.5339 - val_acc: 0.5339\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.03113\n",
            "Epoch 56/100\n",
            " - 66s - loss: 0.0461 - precision: 0.9861 - recall: 0.9844 - f1: 0.9853 - acc: 0.9844 - val_loss: 1.0104 - val_precision: 0.7865 - val_recall: 0.7865 - val_f1: 0.7865 - val_acc: 0.7865\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.03113\n",
            "Epoch 57/100\n",
            " - 66s - loss: 0.0313 - precision: 0.9904 - recall: 0.9898 - f1: 0.9901 - acc: 0.9904 - val_loss: 0.1698 - val_precision: 0.9502 - val_recall: 0.9453 - val_f1: 0.9477 - val_acc: 0.9505\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.03113\n",
            "Epoch 58/100\n",
            " - 66s - loss: 0.0364 - precision: 0.9874 - recall: 0.9874 - f1: 0.9874 - acc: 0.9874 - val_loss: 0.0788 - val_precision: 0.9766 - val_recall: 0.9766 - val_f1: 0.9766 - val_acc: 0.9766\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.03113\n",
            "Epoch 59/100\n",
            " - 66s - loss: 0.0301 - precision: 0.9898 - recall: 0.9898 - f1: 0.9898 - acc: 0.9898 - val_loss: 0.1243 - val_precision: 0.9633 - val_recall: 0.9557 - val_f1: 0.9595 - val_acc: 0.9609\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.03113\n",
            "Epoch 60/100\n",
            " - 66s - loss: 0.0318 - precision: 0.9886 - recall: 0.9880 - f1: 0.9883 - acc: 0.9880 - val_loss: 0.0734 - val_precision: 0.9688 - val_recall: 0.9688 - val_f1: 0.9687 - val_acc: 0.9688\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.03113\n",
            "Epoch 61/100\n",
            " - 66s - loss: 0.0486 - precision: 0.9796 - recall: 0.9790 - f1: 0.9793 - acc: 0.9790 - val_loss: 2.9857 - val_precision: 0.5797 - val_recall: 0.5781 - val_f1: 0.5789 - val_acc: 0.5781\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.03113\n",
            "Epoch 62/100\n",
            " - 66s - loss: 0.0473 - precision: 0.9807 - recall: 0.9796 - f1: 0.9801 - acc: 0.9802 - val_loss: 0.4102 - val_precision: 0.8640 - val_recall: 0.8594 - val_f1: 0.8616 - val_acc: 0.8646\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.03113\n",
            "Epoch 63/100\n",
            " - 66s - loss: 0.0367 - precision: 0.9862 - recall: 0.9850 - f1: 0.9856 - acc: 0.9862 - val_loss: 0.0950 - val_precision: 0.9766 - val_recall: 0.9766 - val_f1: 0.9766 - val_acc: 0.9766\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.03113\n",
            "Epoch 64/100\n",
            " - 66s - loss: 0.0281 - precision: 0.9904 - recall: 0.9904 - f1: 0.9904 - acc: 0.9904 - val_loss: 0.1001 - val_precision: 0.9740 - val_recall: 0.9740 - val_f1: 0.9740 - val_acc: 0.9740\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.03113\n",
            "Epoch 65/100\n",
            " - 66s - loss: 0.0372 - precision: 0.9868 - recall: 0.9868 - f1: 0.9868 - acc: 0.9868 - val_loss: 0.0887 - val_precision: 0.9688 - val_recall: 0.9688 - val_f1: 0.9687 - val_acc: 0.9688\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.03113\n",
            "Epoch 66/100\n",
            " - 66s - loss: 0.0284 - precision: 0.9898 - recall: 0.9898 - f1: 0.9898 - acc: 0.9898 - val_loss: 0.0897 - val_precision: 0.9661 - val_recall: 0.9661 - val_f1: 0.9661 - val_acc: 0.9661\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.03113\n",
            "Epoch 67/100\n",
            " - 65s - loss: 0.0307 - precision: 0.9880 - recall: 0.9874 - f1: 0.9877 - acc: 0.9874 - val_loss: 0.5173 - val_precision: 0.8470 - val_recall: 0.8359 - val_f1: 0.8414 - val_acc: 0.8438\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.03113\n",
            "Epoch 68/100\n",
            " - 66s - loss: 0.0424 - precision: 0.9874 - recall: 0.9862 - f1: 0.9868 - acc: 0.9862 - val_loss: 1.8102 - val_precision: 0.7135 - val_recall: 0.7135 - val_f1: 0.7135 - val_acc: 0.7135\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.03113\n",
            "Epoch 69/100\n",
            " - 66s - loss: 0.0458 - precision: 0.9862 - recall: 0.9844 - f1: 0.9853 - acc: 0.9850 - val_loss: 0.4064 - val_precision: 0.9036 - val_recall: 0.9036 - val_f1: 0.9036 - val_acc: 0.9036\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.03113\n",
            "Epoch 70/100\n",
            " - 66s - loss: 0.0334 - precision: 0.9844 - recall: 0.9844 - f1: 0.9844 - acc: 0.9844 - val_loss: 0.2676 - val_precision: 0.9245 - val_recall: 0.9245 - val_f1: 0.9245 - val_acc: 0.9245\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.03113\n",
            "Epoch 71/100\n",
            " - 66s - loss: 0.0393 - precision: 0.9826 - recall: 0.9814 - f1: 0.9820 - acc: 0.9820 - val_loss: 2.4674 - val_precision: 0.5874 - val_recall: 0.5859 - val_f1: 0.5867 - val_acc: 0.5859\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.03113\n",
            "Epoch 72/100\n",
            " - 66s - loss: 0.0728 - precision: 0.9766 - recall: 0.9760 - f1: 0.9763 - acc: 0.9766 - val_loss: 0.8141 - val_precision: 0.8256 - val_recall: 0.8125 - val_f1: 0.8189 - val_acc: 0.8125\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.03113\n",
            "Epoch 73/100\n",
            " - 66s - loss: 0.0569 - precision: 0.9807 - recall: 0.9790 - f1: 0.9798 - acc: 0.9802 - val_loss: 0.2757 - val_precision: 0.9160 - val_recall: 0.9089 - val_f1: 0.9124 - val_acc: 0.9167\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.03113\n",
            "Epoch 74/100\n",
            " - 66s - loss: 0.0545 - precision: 0.9802 - recall: 0.9802 - f1: 0.9802 - acc: 0.9802 - val_loss: 0.1662 - val_precision: 0.9475 - val_recall: 0.9401 - val_f1: 0.9438 - val_acc: 0.9401\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.03113\n",
            "Epoch 75/100\n",
            " - 65s - loss: 0.0391 - precision: 0.9856 - recall: 0.9856 - f1: 0.9856 - acc: 0.9856 - val_loss: 0.2253 - val_precision: 0.9219 - val_recall: 0.9219 - val_f1: 0.9219 - val_acc: 0.9219\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.03113\n",
            "Epoch 76/100\n",
            " - 65s - loss: 0.0251 - precision: 0.9940 - recall: 0.9940 - f1: 0.9940 - acc: 0.9940 - val_loss: 0.0630 - val_precision: 0.9740 - val_recall: 0.9740 - val_f1: 0.9740 - val_acc: 0.9740\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.03113\n",
            "Epoch 77/100\n",
            " - 66s - loss: 0.0202 - precision: 0.9934 - recall: 0.9922 - f1: 0.9928 - acc: 0.9934 - val_loss: 0.0885 - val_precision: 0.9635 - val_recall: 0.9635 - val_f1: 0.9635 - val_acc: 0.9635\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.03113\n",
            "Epoch 78/100\n",
            " - 65s - loss: 0.0246 - precision: 0.9904 - recall: 0.9904 - f1: 0.9904 - acc: 0.9904 - val_loss: 0.0684 - val_precision: 0.9688 - val_recall: 0.9688 - val_f1: 0.9687 - val_acc: 0.9688\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.03113\n",
            "Epoch 79/100\n",
            " - 66s - loss: 0.0279 - precision: 0.9898 - recall: 0.9886 - f1: 0.9892 - acc: 0.9892 - val_loss: 0.1213 - val_precision: 0.9530 - val_recall: 0.9505 - val_f1: 0.9517 - val_acc: 0.9505\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.03113\n",
            "Epoch 80/100\n",
            " - 66s - loss: 0.0262 - precision: 0.9880 - recall: 0.9880 - f1: 0.9880 - acc: 0.9880 - val_loss: 0.2753 - val_precision: 0.9167 - val_recall: 0.9167 - val_f1: 0.9167 - val_acc: 0.9167\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.03113\n",
            "Epoch 81/100\n",
            " - 66s - loss: 0.0267 - precision: 0.9916 - recall: 0.9910 - f1: 0.9913 - acc: 0.9916 - val_loss: 0.2832 - val_precision: 0.9191 - val_recall: 0.9167 - val_f1: 0.9179 - val_acc: 0.9167\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.03113\n",
            "Epoch 82/100\n",
            " - 65s - loss: 0.0396 - precision: 0.9850 - recall: 0.9850 - f1: 0.9850 - acc: 0.9850 - val_loss: 0.3589 - val_precision: 0.8903 - val_recall: 0.8880 - val_f1: 0.8892 - val_acc: 0.8880\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.03113\n",
            "Epoch 83/100\n",
            " - 66s - loss: 0.0408 - precision: 0.9856 - recall: 0.9856 - f1: 0.9856 - acc: 0.9856 - val_loss: 5.4097 - val_precision: 0.5286 - val_recall: 0.5286 - val_f1: 0.5286 - val_acc: 0.5286\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.03113\n",
            "Epoch 84/100\n",
            " - 65s - loss: 0.0406 - precision: 0.9844 - recall: 0.9844 - f1: 0.9844 - acc: 0.9844 - val_loss: 1.8868 - val_precision: 0.6813 - val_recall: 0.6797 - val_f1: 0.6805 - val_acc: 0.6797\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.03113\n",
            "Epoch 85/100\n",
            " - 65s - loss: 0.0693 - precision: 0.9747 - recall: 0.9736 - f1: 0.9741 - acc: 0.9742 - val_loss: 3.5958 - val_precision: 0.5990 - val_recall: 0.5990 - val_f1: 0.5990 - val_acc: 0.5990\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.03113\n",
            "Epoch 86/100\n",
            " - 65s - loss: 0.0513 - precision: 0.9849 - recall: 0.9826 - f1: 0.9837 - acc: 0.9844 - val_loss: 0.1802 - val_precision: 0.9450 - val_recall: 0.9375 - val_f1: 0.9412 - val_acc: 0.9453\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.03113\n",
            "Epoch 87/100\n",
            " - 65s - loss: 0.0343 - precision: 0.9892 - recall: 0.9892 - f1: 0.9892 - acc: 0.9892 - val_loss: 0.5207 - val_precision: 0.8385 - val_recall: 0.8385 - val_f1: 0.8385 - val_acc: 0.8385\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.03113\n",
            "Epoch 88/100\n",
            " - 66s - loss: 0.0336 - precision: 0.9874 - recall: 0.9868 - f1: 0.9871 - acc: 0.9868 - val_loss: 0.2358 - val_precision: 0.9219 - val_recall: 0.9193 - val_f1: 0.9206 - val_acc: 0.9193\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.03113\n",
            "Epoch 89/100\n",
            " - 65s - loss: 0.0169 - precision: 0.9952 - recall: 0.9946 - f1: 0.9949 - acc: 0.9952 - val_loss: 0.1181 - val_precision: 0.9607 - val_recall: 0.9557 - val_f1: 0.9582 - val_acc: 0.9609\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.03113\n",
            "Epoch 90/100\n",
            " - 65s - loss: 0.0193 - precision: 0.9934 - recall: 0.9922 - f1: 0.9928 - acc: 0.9928 - val_loss: 0.0761 - val_precision: 0.9817 - val_recall: 0.9792 - val_f1: 0.9804 - val_acc: 0.9792\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.03113\n",
            "Epoch 91/100\n",
            " - 65s - loss: 0.0205 - precision: 0.9940 - recall: 0.9940 - f1: 0.9940 - acc: 0.9940 - val_loss: 0.0780 - val_precision: 0.9740 - val_recall: 0.9740 - val_f1: 0.9740 - val_acc: 0.9740\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.03113\n",
            "Epoch 92/100\n",
            " - 65s - loss: 0.0239 - precision: 0.9898 - recall: 0.9898 - f1: 0.9898 - acc: 0.9898 - val_loss: 0.1620 - val_precision: 0.9635 - val_recall: 0.9635 - val_f1: 0.9635 - val_acc: 0.9635\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.03113\n",
            "Epoch 93/100\n",
            " - 65s - loss: 0.0338 - precision: 0.9886 - recall: 0.9880 - f1: 0.9883 - acc: 0.9886 - val_loss: 0.4051 - val_precision: 0.8736 - val_recall: 0.8646 - val_f1: 0.8691 - val_acc: 0.8646\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.03113\n",
            "Epoch 94/100\n",
            " - 65s - loss: 0.0288 - precision: 0.9874 - recall: 0.9874 - f1: 0.9874 - acc: 0.9874 - val_loss: 1.3742 - val_precision: 0.7330 - val_recall: 0.7292 - val_f1: 0.7311 - val_acc: 0.7292\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.03113\n",
            "Epoch 95/100\n",
            " - 65s - loss: 0.0379 - precision: 0.9886 - recall: 0.9874 - f1: 0.9880 - acc: 0.9874 - val_loss: 1.2593 - val_precision: 0.7846 - val_recall: 0.7786 - val_f1: 0.7816 - val_acc: 0.7839\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.03113\n",
            "Epoch 96/100\n",
            " - 65s - loss: 0.0537 - precision: 0.9807 - recall: 0.9796 - f1: 0.9801 - acc: 0.9802 - val_loss: 3.7784 - val_precision: 0.4844 - val_recall: 0.4844 - val_f1: 0.4844 - val_acc: 0.4844\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.03113\n",
            "Epoch 97/100\n",
            " - 65s - loss: 0.0610 - precision: 0.9814 - recall: 0.9802 - f1: 0.9808 - acc: 0.9808 - val_loss: 4.7410 - val_precision: 0.5078 - val_recall: 0.5078 - val_f1: 0.5078 - val_acc: 0.5078\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.03113\n",
            "Epoch 98/100\n",
            " - 66s - loss: 0.0409 - precision: 0.9861 - recall: 0.9844 - f1: 0.9852 - acc: 0.9844 - val_loss: 1.5383 - val_precision: 0.6886 - val_recall: 0.6849 - val_f1: 0.6867 - val_acc: 0.6901\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.03113\n",
            "Epoch 99/100\n",
            " - 65s - loss: 0.0269 - precision: 0.9898 - recall: 0.9892 - f1: 0.9895 - acc: 0.9898 - val_loss: 0.5664 - val_precision: 0.8172 - val_recall: 0.8151 - val_f1: 0.8161 - val_acc: 0.8177\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.03113\n",
            "Epoch 100/100\n",
            " - 65s - loss: 0.0229 - precision: 0.9928 - recall: 0.9928 - f1: 0.9928 - acc: 0.9928 - val_loss: 0.3786 - val_precision: 0.8743 - val_recall: 0.8698 - val_f1: 0.8720 - val_acc: 0.8750\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.03113\n",
            "Epoch 1/100\n",
            " - 76s - loss: 0.0740 - precision: 0.9795 - recall: 0.9772 - f1: 0.9783 - acc: 0.9784 - val_loss: 5.6187 - val_precision: 0.3961 - val_recall: 0.3932 - val_f1: 0.3947 - val_acc: 0.4010\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 5.61873, saving model to BRAIN_TUMOR_FOLD_3.h5\n",
            "Epoch 2/100\n",
            " - 61s - loss: 0.0542 - precision: 0.9813 - recall: 0.9790 - f1: 0.9801 - acc: 0.9790 - val_loss: 1.4403 - val_precision: 0.6971 - val_recall: 0.6953 - val_f1: 0.6962 - val_acc: 0.6953\n",
            "\n",
            "Epoch 00002: val_loss improved from 5.61873 to 1.44029, saving model to BRAIN_TUMOR_FOLD_3.h5\n",
            "Epoch 3/100\n",
            " - 65s - loss: 0.0477 - precision: 0.9832 - recall: 0.9826 - f1: 0.9829 - acc: 0.9826 - val_loss: 0.2308 - val_precision: 0.9089 - val_recall: 0.9089 - val_f1: 0.9089 - val_acc: 0.9089\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.44029 to 0.23077, saving model to BRAIN_TUMOR_FOLD_3.h5\n",
            "Epoch 4/100\n",
            " - 65s - loss: 0.0517 - precision: 0.9826 - recall: 0.9820 - f1: 0.9823 - acc: 0.9826 - val_loss: 0.4559 - val_precision: 0.8594 - val_recall: 0.8594 - val_f1: 0.8594 - val_acc: 0.8594\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.23077\n",
            "Epoch 5/100\n",
            " - 65s - loss: 0.0420 - precision: 0.9850 - recall: 0.9844 - f1: 0.9847 - acc: 0.9850 - val_loss: 1.1611 - val_precision: 0.7010 - val_recall: 0.6953 - val_f1: 0.6981 - val_acc: 0.7005\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.23077\n",
            "Epoch 6/100\n",
            " - 65s - loss: 0.0599 - precision: 0.9801 - recall: 0.9784 - f1: 0.9792 - acc: 0.9802 - val_loss: 0.0817 - val_precision: 0.9659 - val_recall: 0.9609 - val_f1: 0.9634 - val_acc: 0.9609\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.23077 to 0.08171, saving model to BRAIN_TUMOR_FOLD_3.h5\n",
            "Epoch 7/100\n",
            " - 65s - loss: 0.0546 - precision: 0.9790 - recall: 0.9784 - f1: 0.9787 - acc: 0.9784 - val_loss: 0.5991 - val_precision: 0.8542 - val_recall: 0.8542 - val_f1: 0.8542 - val_acc: 0.8542\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.08171\n",
            "Epoch 8/100\n",
            " - 65s - loss: 0.0550 - precision: 0.9807 - recall: 0.9790 - f1: 0.9798 - acc: 0.9796 - val_loss: 0.2251 - val_precision: 0.9101 - val_recall: 0.8958 - val_f1: 0.9029 - val_acc: 0.8958\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.08171\n",
            "Epoch 9/100\n",
            " - 65s - loss: 0.0428 - precision: 0.9868 - recall: 0.9868 - f1: 0.9868 - acc: 0.9868 - val_loss: 1.2581 - val_precision: 0.7319 - val_recall: 0.7240 - val_f1: 0.7279 - val_acc: 0.7266\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.08171\n",
            "Epoch 10/100\n",
            " - 65s - loss: 0.0318 - precision: 0.9850 - recall: 0.9844 - f1: 0.9847 - acc: 0.9850 - val_loss: 1.4674 - val_precision: 0.6579 - val_recall: 0.6562 - val_f1: 0.6571 - val_acc: 0.6562\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.08171\n",
            "Epoch 11/100\n",
            " - 65s - loss: 0.0302 - precision: 0.9898 - recall: 0.9898 - f1: 0.9898 - acc: 0.9898 - val_loss: 0.1087 - val_precision: 0.9609 - val_recall: 0.9609 - val_f1: 0.9609 - val_acc: 0.9609\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.08171\n",
            "Epoch 12/100\n",
            " - 66s - loss: 0.0428 - precision: 0.9862 - recall: 0.9850 - f1: 0.9856 - acc: 0.9862 - val_loss: 0.0501 - val_precision: 0.9896 - val_recall: 0.9896 - val_f1: 0.9896 - val_acc: 0.9896\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.08171 to 0.05006, saving model to BRAIN_TUMOR_FOLD_3.h5\n",
            "Epoch 13/100\n",
            " - 65s - loss: 0.0221 - precision: 0.9928 - recall: 0.9922 - f1: 0.9925 - acc: 0.9922 - val_loss: 0.8449 - val_precision: 0.7512 - val_recall: 0.7474 - val_f1: 0.7493 - val_acc: 0.7474\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.05006\n",
            "Epoch 14/100\n",
            " - 65s - loss: 0.0391 - precision: 0.9886 - recall: 0.9868 - f1: 0.9877 - acc: 0.9886 - val_loss: 0.2413 - val_precision: 0.9153 - val_recall: 0.9010 - val_f1: 0.9080 - val_acc: 0.9010\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.05006\n",
            "Epoch 15/100\n",
            " - 65s - loss: 0.0361 - precision: 0.9880 - recall: 0.9868 - f1: 0.9874 - acc: 0.9874 - val_loss: 0.0859 - val_precision: 0.9557 - val_recall: 0.9557 - val_f1: 0.9557 - val_acc: 0.9557\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.05006\n",
            "Epoch 16/100\n",
            " - 65s - loss: 0.0418 - precision: 0.9849 - recall: 0.9838 - f1: 0.9843 - acc: 0.9838 - val_loss: 0.0641 - val_precision: 0.9843 - val_recall: 0.9818 - val_f1: 0.9830 - val_acc: 0.9818\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.05006\n",
            "Epoch 17/100\n",
            " - 65s - loss: 0.0537 - precision: 0.9796 - recall: 0.9784 - f1: 0.9790 - acc: 0.9796 - val_loss: 1.9732 - val_precision: 0.6016 - val_recall: 0.6016 - val_f1: 0.6016 - val_acc: 0.6016\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.05006\n",
            "Epoch 18/100\n",
            " - 65s - loss: 0.0575 - precision: 0.9789 - recall: 0.9772 - f1: 0.9780 - acc: 0.9784 - val_loss: 0.1847 - val_precision: 0.9268 - val_recall: 0.9245 - val_f1: 0.9256 - val_acc: 0.9271\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.05006\n",
            "Epoch 19/100\n",
            " - 65s - loss: 0.0412 - precision: 0.9844 - recall: 0.9838 - f1: 0.9841 - acc: 0.9838 - val_loss: 0.0802 - val_precision: 0.9714 - val_recall: 0.9714 - val_f1: 0.9714 - val_acc: 0.9714\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.05006\n",
            "Epoch 20/100\n",
            " - 65s - loss: 0.0350 - precision: 0.9898 - recall: 0.9892 - f1: 0.9895 - acc: 0.9898 - val_loss: 0.1078 - val_precision: 0.9555 - val_recall: 0.9479 - val_f1: 0.9516 - val_acc: 0.9479\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.05006\n",
            "Epoch 21/100\n",
            " - 65s - loss: 0.0277 - precision: 0.9916 - recall: 0.9898 - f1: 0.9907 - acc: 0.9904 - val_loss: 0.1040 - val_precision: 0.9687 - val_recall: 0.9661 - val_f1: 0.9674 - val_acc: 0.9661\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.05006\n",
            "Epoch 22/100\n",
            " - 65s - loss: 0.0304 - precision: 0.9874 - recall: 0.9874 - f1: 0.9874 - acc: 0.9874 - val_loss: 0.1593 - val_precision: 0.9375 - val_recall: 0.9375 - val_f1: 0.9375 - val_acc: 0.9375\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.05006\n",
            "Epoch 23/100\n",
            " - 65s - loss: 0.0410 - precision: 0.9844 - recall: 0.9832 - f1: 0.9838 - acc: 0.9832 - val_loss: 0.1768 - val_precision: 0.9349 - val_recall: 0.9349 - val_f1: 0.9349 - val_acc: 0.9349\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.05006\n",
            "Epoch 24/100\n",
            " - 65s - loss: 0.0372 - precision: 0.9850 - recall: 0.9838 - f1: 0.9844 - acc: 0.9838 - val_loss: 0.5038 - val_precision: 0.8460 - val_recall: 0.8438 - val_f1: 0.8449 - val_acc: 0.8438\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.05006\n",
            "Epoch 25/100\n",
            " - 65s - loss: 0.0338 - precision: 0.9880 - recall: 0.9874 - f1: 0.9877 - acc: 0.9874 - val_loss: 0.8338 - val_precision: 0.7877 - val_recall: 0.7812 - val_f1: 0.7845 - val_acc: 0.7865\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.05006\n",
            "Epoch 26/100\n",
            " - 65s - loss: 0.0645 - precision: 0.9766 - recall: 0.9760 - f1: 0.9763 - acc: 0.9760 - val_loss: 3.0036 - val_precision: 0.5495 - val_recall: 0.5495 - val_f1: 0.5495 - val_acc: 0.5495\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.05006\n",
            "Epoch 27/100\n",
            " - 65s - loss: 0.0429 - precision: 0.9886 - recall: 0.9880 - f1: 0.9883 - acc: 0.9886 - val_loss: 1.4568 - val_precision: 0.7422 - val_recall: 0.7422 - val_f1: 0.7422 - val_acc: 0.7422\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.05006\n",
            "Epoch 28/100\n",
            " - 65s - loss: 0.0414 - precision: 0.9874 - recall: 0.9874 - f1: 0.9874 - acc: 0.9874 - val_loss: 0.4255 - val_precision: 0.8616 - val_recall: 0.8594 - val_f1: 0.8605 - val_acc: 0.8594\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.05006\n",
            "Epoch 29/100\n",
            " - 65s - loss: 0.0396 - precision: 0.9844 - recall: 0.9832 - f1: 0.9838 - acc: 0.9838 - val_loss: 0.7948 - val_precision: 0.7514 - val_recall: 0.7344 - val_f1: 0.7427 - val_acc: 0.7396\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.05006\n",
            "Epoch 30/100\n",
            " - 65s - loss: 0.0400 - precision: 0.9868 - recall: 0.9868 - f1: 0.9868 - acc: 0.9868 - val_loss: 1.8512 - val_precision: 0.6096 - val_recall: 0.6016 - val_f1: 0.6055 - val_acc: 0.6068\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.05006\n",
            "Epoch 31/100\n",
            " - 65s - loss: 0.0873 - precision: 0.9802 - recall: 0.9802 - f1: 0.9802 - acc: 0.9802 - val_loss: 1.6806 - val_precision: 0.6484 - val_recall: 0.6484 - val_f1: 0.6484 - val_acc: 0.6484\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.05006\n",
            "Epoch 32/100\n",
            " - 65s - loss: 0.0820 - precision: 0.9723 - recall: 0.9712 - f1: 0.9717 - acc: 0.9718 - val_loss: 0.4555 - val_precision: 0.8383 - val_recall: 0.8359 - val_f1: 0.8371 - val_acc: 0.8385\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.05006\n",
            "Epoch 33/100\n",
            " - 65s - loss: 0.0479 - precision: 0.9832 - recall: 0.9820 - f1: 0.9826 - acc: 0.9826 - val_loss: 0.1165 - val_precision: 0.9375 - val_recall: 0.9375 - val_f1: 0.9375 - val_acc: 0.9375\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.05006\n",
            "Epoch 34/100\n",
            " - 65s - loss: 0.0408 - precision: 0.9862 - recall: 0.9850 - f1: 0.9856 - acc: 0.9862 - val_loss: 0.0983 - val_precision: 0.9661 - val_recall: 0.9661 - val_f1: 0.9661 - val_acc: 0.9661\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.05006\n",
            "Epoch 35/100\n",
            " - 65s - loss: 0.0279 - precision: 0.9928 - recall: 0.9928 - f1: 0.9928 - acc: 0.9928 - val_loss: 0.0340 - val_precision: 0.9948 - val_recall: 0.9948 - val_f1: 0.9948 - val_acc: 0.9948\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.05006 to 0.03404, saving model to BRAIN_TUMOR_FOLD_3.h5\n",
            "Epoch 36/100\n",
            " - 65s - loss: 0.0242 - precision: 0.9910 - recall: 0.9910 - f1: 0.9910 - acc: 0.9910 - val_loss: 1.0697 - val_precision: 0.7344 - val_recall: 0.7344 - val_f1: 0.7344 - val_acc: 0.7344\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.03404\n",
            "Epoch 37/100\n",
            " - 65s - loss: 0.0309 - precision: 0.9910 - recall: 0.9910 - f1: 0.9910 - acc: 0.9910 - val_loss: 2.1298 - val_precision: 0.5703 - val_recall: 0.5703 - val_f1: 0.5703 - val_acc: 0.5703\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.03404\n",
            "Epoch 38/100\n",
            " - 65s - loss: 0.0401 - precision: 0.9844 - recall: 0.9838 - f1: 0.9841 - acc: 0.9838 - val_loss: 0.7144 - val_precision: 0.8027 - val_recall: 0.7969 - val_f1: 0.7997 - val_acc: 0.7969\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.03404\n",
            "Epoch 39/100\n",
            " - 65s - loss: 0.0233 - precision: 0.9922 - recall: 0.9916 - f1: 0.9919 - acc: 0.9916 - val_loss: 0.0646 - val_precision: 0.9843 - val_recall: 0.9818 - val_f1: 0.9830 - val_acc: 0.9844\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.03404\n",
            "Epoch 40/100\n",
            " - 66s - loss: 0.0229 - precision: 0.9928 - recall: 0.9922 - f1: 0.9925 - acc: 0.9928 - val_loss: 0.0211 - val_precision: 0.9948 - val_recall: 0.9948 - val_f1: 0.9948 - val_acc: 0.9948\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.03404 to 0.02108, saving model to BRAIN_TUMOR_FOLD_3.h5\n",
            "Epoch 41/100\n",
            " - 65s - loss: 0.0243 - precision: 0.9910 - recall: 0.9910 - f1: 0.9910 - acc: 0.9910 - val_loss: 0.0117 - val_precision: 0.9974 - val_recall: 0.9974 - val_f1: 0.9974 - val_acc: 0.9974\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.02108 to 0.01166, saving model to BRAIN_TUMOR_FOLD_3.h5\n",
            "Epoch 42/100\n",
            " - 65s - loss: 0.0631 - precision: 0.9753 - recall: 0.9742 - f1: 0.9747 - acc: 0.9748 - val_loss: 6.1742 - val_precision: 0.4323 - val_recall: 0.4323 - val_f1: 0.4323 - val_acc: 0.4323\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.01166\n",
            "Epoch 43/100\n",
            " - 65s - loss: 0.0492 - precision: 0.9844 - recall: 0.9844 - f1: 0.9844 - acc: 0.9844 - val_loss: 0.2004 - val_precision: 0.9343 - val_recall: 0.9271 - val_f1: 0.9306 - val_acc: 0.9271\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.01166\n",
            "Epoch 44/100\n",
            " - 65s - loss: 0.0521 - precision: 0.9868 - recall: 0.9868 - f1: 0.9868 - acc: 0.9868 - val_loss: 0.3141 - val_precision: 0.9086 - val_recall: 0.9062 - val_f1: 0.9074 - val_acc: 0.9062\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.01166\n",
            "Epoch 45/100\n",
            " - 65s - loss: 0.0423 - precision: 0.9844 - recall: 0.9838 - f1: 0.9841 - acc: 0.9844 - val_loss: 0.1399 - val_precision: 0.9609 - val_recall: 0.9609 - val_f1: 0.9609 - val_acc: 0.9609\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.01166\n",
            "Epoch 46/100\n",
            " - 65s - loss: 0.0287 - precision: 0.9892 - recall: 0.9892 - f1: 0.9892 - acc: 0.9892 - val_loss: 0.0462 - val_precision: 0.9818 - val_recall: 0.9818 - val_f1: 0.9818 - val_acc: 0.9818\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.01166\n",
            "Epoch 47/100\n",
            " - 65s - loss: 0.0211 - precision: 0.9946 - recall: 0.9940 - f1: 0.9943 - acc: 0.9946 - val_loss: 0.0211 - val_precision: 0.9922 - val_recall: 0.9922 - val_f1: 0.9922 - val_acc: 0.9922\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.01166\n",
            "Epoch 48/100\n",
            " - 65s - loss: 0.0285 - precision: 0.9898 - recall: 0.9892 - f1: 0.9895 - acc: 0.9898 - val_loss: 0.1065 - val_precision: 0.9609 - val_recall: 0.9609 - val_f1: 0.9609 - val_acc: 0.9609\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.01166\n",
            "Epoch 49/100\n",
            " - 65s - loss: 0.0332 - precision: 0.9886 - recall: 0.9886 - f1: 0.9886 - acc: 0.9886 - val_loss: 0.7728 - val_precision: 0.8005 - val_recall: 0.7943 - val_f1: 0.7973 - val_acc: 0.8021\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.01166\n",
            "Epoch 50/100\n",
            " - 65s - loss: 0.0417 - precision: 0.9868 - recall: 0.9862 - f1: 0.9865 - acc: 0.9868 - val_loss: 0.3640 - val_precision: 0.8427 - val_recall: 0.8359 - val_f1: 0.8393 - val_acc: 0.8359\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.01166\n",
            "Epoch 51/100\n",
            " - 65s - loss: 0.0191 - precision: 0.9922 - recall: 0.9922 - f1: 0.9922 - acc: 0.9922 - val_loss: 0.0876 - val_precision: 0.9661 - val_recall: 0.9661 - val_f1: 0.9661 - val_acc: 0.9661\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.01166\n",
            "Epoch 52/100\n",
            " - 66s - loss: 0.0394 - precision: 0.9886 - recall: 0.9886 - f1: 0.9886 - acc: 0.9886 - val_loss: 0.0400 - val_precision: 0.9870 - val_recall: 0.9844 - val_f1: 0.9857 - val_acc: 0.9844\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.01166\n",
            "Epoch 53/100\n",
            " - 66s - loss: 0.0198 - precision: 0.9946 - recall: 0.9934 - f1: 0.9940 - acc: 0.9946 - val_loss: 0.0060 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.01166 to 0.00598, saving model to BRAIN_TUMOR_FOLD_3.h5\n",
            "Epoch 54/100\n",
            " - 65s - loss: 0.0251 - precision: 0.9916 - recall: 0.9916 - f1: 0.9916 - acc: 0.9916 - val_loss: 0.2474 - val_precision: 0.9141 - val_recall: 0.9141 - val_f1: 0.9141 - val_acc: 0.9141\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.00598\n",
            "Epoch 55/100\n",
            " - 66s - loss: 0.0333 - precision: 0.9868 - recall: 0.9862 - f1: 0.9865 - acc: 0.9862 - val_loss: 0.1034 - val_precision: 0.9740 - val_recall: 0.9740 - val_f1: 0.9740 - val_acc: 0.9740\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.00598\n",
            "Epoch 56/100\n",
            " - 65s - loss: 0.0519 - precision: 0.9838 - recall: 0.9826 - f1: 0.9832 - acc: 0.9826 - val_loss: 0.2492 - val_precision: 0.9000 - val_recall: 0.8906 - val_f1: 0.8953 - val_acc: 0.9010\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.00598\n",
            "Epoch 57/100\n",
            " - 65s - loss: 0.0351 - precision: 0.9861 - recall: 0.9832 - f1: 0.9846 - acc: 0.9844 - val_loss: 0.4149 - val_precision: 0.8402 - val_recall: 0.8229 - val_f1: 0.8314 - val_acc: 0.8333\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.00598\n",
            "Epoch 58/100\n",
            " - 66s - loss: 0.0221 - precision: 0.9928 - recall: 0.9928 - f1: 0.9928 - acc: 0.9928 - val_loss: 0.1063 - val_precision: 0.9635 - val_recall: 0.9635 - val_f1: 0.9635 - val_acc: 0.9635\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.00598\n",
            "Epoch 59/100\n",
            " - 66s - loss: 0.0287 - precision: 0.9898 - recall: 0.9898 - f1: 0.9898 - acc: 0.9898 - val_loss: 0.0269 - val_precision: 0.9870 - val_recall: 0.9870 - val_f1: 0.9870 - val_acc: 0.9870\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.00598\n",
            "Epoch 60/100\n",
            " - 66s - loss: 0.0230 - precision: 0.9898 - recall: 0.9898 - f1: 0.9898 - acc: 0.9898 - val_loss: 0.1700 - val_precision: 0.9401 - val_recall: 0.9401 - val_f1: 0.9401 - val_acc: 0.9401\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.00598\n",
            "Epoch 61/100\n",
            " - 65s - loss: 0.0217 - precision: 0.9940 - recall: 0.9940 - f1: 0.9940 - acc: 0.9940 - val_loss: 0.4073 - val_precision: 0.8875 - val_recall: 0.8828 - val_f1: 0.8852 - val_acc: 0.8828\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.00598\n",
            "Epoch 62/100\n",
            " - 65s - loss: 0.0262 - precision: 0.9898 - recall: 0.9898 - f1: 0.9898 - acc: 0.9898 - val_loss: 0.0601 - val_precision: 0.9870 - val_recall: 0.9870 - val_f1: 0.9870 - val_acc: 0.9870\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.00598\n",
            "Epoch 63/100\n",
            " - 65s - loss: 0.0293 - precision: 0.9904 - recall: 0.9904 - f1: 0.9904 - acc: 0.9904 - val_loss: 0.0318 - val_precision: 0.9870 - val_recall: 0.9870 - val_f1: 0.9870 - val_acc: 0.9870\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.00598\n",
            "Epoch 64/100\n",
            " - 65s - loss: 0.0205 - precision: 0.9928 - recall: 0.9928 - f1: 0.9928 - acc: 0.9928 - val_loss: 0.0257 - val_precision: 0.9922 - val_recall: 0.9922 - val_f1: 0.9922 - val_acc: 0.9922\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.00598\n",
            "Epoch 65/100\n",
            " - 66s - loss: 0.0207 - precision: 0.9916 - recall: 0.9910 - f1: 0.9913 - acc: 0.9916 - val_loss: 0.0168 - val_precision: 0.9948 - val_recall: 0.9948 - val_f1: 0.9948 - val_acc: 0.9948\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.00598\n",
            "Epoch 66/100\n",
            " - 65s - loss: 0.0220 - precision: 0.9928 - recall: 0.9928 - f1: 0.9928 - acc: 0.9928 - val_loss: 0.1890 - val_precision: 0.9426 - val_recall: 0.9401 - val_f1: 0.9413 - val_acc: 0.9401\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.00598\n",
            "Epoch 67/100\n",
            " - 65s - loss: 0.0372 - precision: 0.9862 - recall: 0.9856 - f1: 0.9859 - acc: 0.9856 - val_loss: 1.0787 - val_precision: 0.7288 - val_recall: 0.7214 - val_f1: 0.7250 - val_acc: 0.7214\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.00598\n",
            "Epoch 68/100\n",
            " - 66s - loss: 0.0445 - precision: 0.9862 - recall: 0.9856 - f1: 0.9859 - acc: 0.9856 - val_loss: 0.5820 - val_precision: 0.8301 - val_recall: 0.8281 - val_f1: 0.8291 - val_acc: 0.8281\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.00598\n",
            "Epoch 69/100\n",
            " - 66s - loss: 0.0402 - precision: 0.9898 - recall: 0.9898 - f1: 0.9898 - acc: 0.9898 - val_loss: 0.2674 - val_precision: 0.9265 - val_recall: 0.9193 - val_f1: 0.9229 - val_acc: 0.9219\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.00598\n",
            "Epoch 70/100\n",
            " - 66s - loss: 0.0194 - precision: 0.9916 - recall: 0.9916 - f1: 0.9916 - acc: 0.9916 - val_loss: 0.0718 - val_precision: 0.9766 - val_recall: 0.9766 - val_f1: 0.9766 - val_acc: 0.9766\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.00598\n",
            "Epoch 71/100\n",
            " - 65s - loss: 0.0207 - precision: 0.9940 - recall: 0.9934 - f1: 0.9937 - acc: 0.9940 - val_loss: 1.1388 - val_precision: 0.7748 - val_recall: 0.7708 - val_f1: 0.7728 - val_acc: 0.7708\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.00598\n",
            "Epoch 72/100\n",
            " - 65s - loss: 0.0212 - precision: 0.9928 - recall: 0.9922 - f1: 0.9925 - acc: 0.9928 - val_loss: 0.3568 - val_precision: 0.8917 - val_recall: 0.8776 - val_f1: 0.8846 - val_acc: 0.8854\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.00598\n",
            "Epoch 73/100\n",
            " - 66s - loss: 0.0270 - precision: 0.9934 - recall: 0.9928 - f1: 0.9931 - acc: 0.9934 - val_loss: 0.2971 - val_precision: 0.8984 - val_recall: 0.8984 - val_f1: 0.8984 - val_acc: 0.8984\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.00598\n",
            "Epoch 74/100\n",
            " - 66s - loss: 0.0172 - precision: 0.9940 - recall: 0.9940 - f1: 0.9940 - acc: 0.9940 - val_loss: 0.0696 - val_precision: 0.9818 - val_recall: 0.9818 - val_f1: 0.9818 - val_acc: 0.9818\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.00598\n",
            "Epoch 75/100\n",
            " - 66s - loss: 0.0154 - precision: 0.9964 - recall: 0.9958 - f1: 0.9961 - acc: 0.9958 - val_loss: 0.0062 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.00598\n",
            "Epoch 76/100\n",
            " - 66s - loss: 0.0046 - precision: 0.9994 - recall: 0.9988 - f1: 0.9991 - acc: 0.9994 - val_loss: 0.0116 - val_precision: 0.9948 - val_recall: 0.9948 - val_f1: 0.9948 - val_acc: 0.9948\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.00598\n",
            "Epoch 77/100\n",
            " - 65s - loss: 0.0155 - precision: 0.9940 - recall: 0.9940 - f1: 0.9940 - acc: 0.9940 - val_loss: 0.0070 - val_precision: 0.9948 - val_recall: 0.9948 - val_f1: 0.9948 - val_acc: 0.9948\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.00598\n",
            "Epoch 78/100\n",
            " - 67s - loss: 0.0078 - precision: 0.9982 - recall: 0.9982 - f1: 0.9982 - acc: 0.9982 - val_loss: 0.0172 - val_precision: 0.9948 - val_recall: 0.9948 - val_f1: 0.9948 - val_acc: 0.9948\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.00598\n",
            "Epoch 79/100\n",
            " - 66s - loss: 0.0152 - precision: 0.9958 - recall: 0.9958 - f1: 0.9958 - acc: 0.9958 - val_loss: 0.2074 - val_precision: 0.9375 - val_recall: 0.9375 - val_f1: 0.9375 - val_acc: 0.9375\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.00598\n",
            "Epoch 80/100\n",
            " - 66s - loss: 0.0295 - precision: 0.9898 - recall: 0.9898 - f1: 0.9898 - acc: 0.9898 - val_loss: 0.0953 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.00598\n",
            "Epoch 81/100\n",
            " - 66s - loss: 0.0176 - precision: 0.9934 - recall: 0.9934 - f1: 0.9934 - acc: 0.9934 - val_loss: 0.1509 - val_precision: 0.9193 - val_recall: 0.9193 - val_f1: 0.9193 - val_acc: 0.9193\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.00598\n",
            "Epoch 82/100\n",
            " - 66s - loss: 0.0496 - precision: 0.9838 - recall: 0.9832 - f1: 0.9835 - acc: 0.9832 - val_loss: 0.8441 - val_precision: 0.8255 - val_recall: 0.8255 - val_f1: 0.8255 - val_acc: 0.8255\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.00598\n",
            "Epoch 83/100\n",
            " - 66s - loss: 0.0451 - precision: 0.9844 - recall: 0.9844 - f1: 0.9844 - acc: 0.9844 - val_loss: 0.1522 - val_precision: 0.9740 - val_recall: 0.9740 - val_f1: 0.9740 - val_acc: 0.9740\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.00598\n",
            "Epoch 84/100\n",
            " - 66s - loss: 0.0526 - precision: 0.9802 - recall: 0.9802 - f1: 0.9802 - acc: 0.9802 - val_loss: 0.2162 - val_precision: 0.9289 - val_recall: 0.9193 - val_f1: 0.9240 - val_acc: 0.9219\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.00598\n",
            "Epoch 85/100\n",
            " - 65s - loss: 0.0369 - precision: 0.9874 - recall: 0.9868 - f1: 0.9871 - acc: 0.9874 - val_loss: 0.8480 - val_precision: 0.8307 - val_recall: 0.8307 - val_f1: 0.8307 - val_acc: 0.8307\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.00598\n",
            "Epoch 86/100\n",
            " - 66s - loss: 0.0428 - precision: 0.9850 - recall: 0.9844 - f1: 0.9847 - acc: 0.9844 - val_loss: 0.6575 - val_precision: 0.8182 - val_recall: 0.8099 - val_f1: 0.8140 - val_acc: 0.8151\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.00598\n",
            "Epoch 87/100\n",
            " - 67s - loss: 0.0249 - precision: 0.9922 - recall: 0.9922 - f1: 0.9922 - acc: 0.9922 - val_loss: 0.5717 - val_precision: 0.8255 - val_recall: 0.8255 - val_f1: 0.8255 - val_acc: 0.8255\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.00598\n",
            "Epoch 88/100\n",
            " - 66s - loss: 0.0215 - precision: 0.9946 - recall: 0.9946 - f1: 0.9946 - acc: 0.9946 - val_loss: 0.1712 - val_precision: 0.9375 - val_recall: 0.9375 - val_f1: 0.9375 - val_acc: 0.9375\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.00598\n",
            "Epoch 89/100\n",
            " - 66s - loss: 0.0179 - precision: 0.9946 - recall: 0.9946 - f1: 0.9946 - acc: 0.9946 - val_loss: 0.0335 - val_precision: 0.9844 - val_recall: 0.9844 - val_f1: 0.9844 - val_acc: 0.9844\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.00598\n",
            "Epoch 90/100\n",
            " - 66s - loss: 0.0264 - precision: 0.9898 - recall: 0.9898 - f1: 0.9898 - acc: 0.9898 - val_loss: 0.0540 - val_precision: 0.9764 - val_recall: 0.9714 - val_f1: 0.9739 - val_acc: 0.9714\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.00598\n",
            "Epoch 91/100\n",
            " - 66s - loss: 0.0167 - precision: 0.9928 - recall: 0.9928 - f1: 0.9928 - acc: 0.9928 - val_loss: 0.0365 - val_precision: 0.9948 - val_recall: 0.9948 - val_f1: 0.9948 - val_acc: 0.9948\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.00598\n",
            "Epoch 92/100\n",
            " - 66s - loss: 0.0136 - precision: 0.9970 - recall: 0.9964 - f1: 0.9967 - acc: 0.9970 - val_loss: 0.2397 - val_precision: 0.9314 - val_recall: 0.9193 - val_f1: 0.9253 - val_acc: 0.9219\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.00598\n",
            "Epoch 93/100\n",
            " - 65s - loss: 0.0111 - precision: 0.9976 - recall: 0.9976 - f1: 0.9976 - acc: 0.9976 - val_loss: 0.1206 - val_precision: 0.9583 - val_recall: 0.9583 - val_f1: 0.9583 - val_acc: 0.9583\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.00598\n",
            "Epoch 94/100\n",
            " - 65s - loss: 0.0256 - precision: 0.9904 - recall: 0.9898 - f1: 0.9901 - acc: 0.9904 - val_loss: 0.2570 - val_precision: 0.9208 - val_recall: 0.9089 - val_f1: 0.9148 - val_acc: 0.9089\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.00598\n",
            "Epoch 95/100\n",
            " - 65s - loss: 0.0334 - precision: 0.9892 - recall: 0.9892 - f1: 0.9892 - acc: 0.9892 - val_loss: 0.0983 - val_precision: 0.9659 - val_recall: 0.9583 - val_f1: 0.9621 - val_acc: 0.9609\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.00598\n",
            "Epoch 96/100\n",
            " - 66s - loss: 0.0430 - precision: 0.9856 - recall: 0.9850 - f1: 0.9853 - acc: 0.9850 - val_loss: 0.6074 - val_precision: 0.7980 - val_recall: 0.7917 - val_f1: 0.7948 - val_acc: 0.7917\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.00598\n",
            "Epoch 97/100\n",
            " - 66s - loss: 0.0323 - precision: 0.9892 - recall: 0.9892 - f1: 0.9892 - acc: 0.9892 - val_loss: 1.8704 - val_precision: 0.6630 - val_recall: 0.6615 - val_f1: 0.6622 - val_acc: 0.6641\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.00598\n",
            "Epoch 98/100\n",
            " - 65s - loss: 0.0236 - precision: 0.9928 - recall: 0.9928 - f1: 0.9928 - acc: 0.9928 - val_loss: 0.4145 - val_precision: 0.8825 - val_recall: 0.8802 - val_f1: 0.8814 - val_acc: 0.8828\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.00598\n",
            "Epoch 99/100\n",
            " - 65s - loss: 0.0315 - precision: 0.9904 - recall: 0.9904 - f1: 0.9904 - acc: 0.9904 - val_loss: 0.1559 - val_precision: 0.9453 - val_recall: 0.9453 - val_f1: 0.9453 - val_acc: 0.9453\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.00598\n",
            "Epoch 100/100\n",
            " - 65s - loss: 0.0263 - precision: 0.9898 - recall: 0.9898 - f1: 0.9898 - acc: 0.9898 - val_loss: 0.0566 - val_precision: 0.9844 - val_recall: 0.9844 - val_f1: 0.9844 - val_acc: 0.9844\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.00598\n",
            "Epoch 1/100\n",
            " - 76s - loss: 0.0559 - precision: 0.9796 - recall: 0.9796 - f1: 0.9796 - acc: 0.9796 - val_loss: 1.4382 - val_precision: 0.6583 - val_recall: 0.6484 - val_f1: 0.6533 - val_acc: 0.6536\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.43816, saving model to BRAIN_TUMOR_FOLD_4.h5\n",
            "Epoch 2/100\n",
            " - 62s - loss: 0.0571 - precision: 0.9807 - recall: 0.9790 - f1: 0.9798 - acc: 0.9796 - val_loss: 3.0493 - val_precision: 0.5260 - val_recall: 0.5260 - val_f1: 0.5260 - val_acc: 0.5260\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 1.43816\n",
            "Epoch 3/100\n",
            " - 65s - loss: 0.0398 - precision: 0.9862 - recall: 0.9850 - f1: 0.9856 - acc: 0.9856 - val_loss: 0.0307 - val_precision: 0.9896 - val_recall: 0.9896 - val_f1: 0.9896 - val_acc: 0.9896\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.43816 to 0.03070, saving model to BRAIN_TUMOR_FOLD_4.h5\n",
            "Epoch 4/100\n",
            " - 65s - loss: 0.0288 - precision: 0.9898 - recall: 0.9886 - f1: 0.9892 - acc: 0.9892 - val_loss: 0.2364 - val_precision: 0.9193 - val_recall: 0.9193 - val_f1: 0.9193 - val_acc: 0.9193\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.03070\n",
            "Epoch 5/100\n",
            " - 65s - loss: 0.0325 - precision: 0.9862 - recall: 0.9856 - f1: 0.9859 - acc: 0.9862 - val_loss: 0.0371 - val_precision: 0.9896 - val_recall: 0.9896 - val_f1: 0.9896 - val_acc: 0.9896\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.03070\n",
            "Epoch 6/100\n",
            " - 65s - loss: 0.0299 - precision: 0.9880 - recall: 0.9868 - f1: 0.9874 - acc: 0.9868 - val_loss: 0.0558 - val_precision: 0.9817 - val_recall: 0.9740 - val_f1: 0.9778 - val_acc: 0.9766\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.03070\n",
            "Epoch 7/100\n",
            " - 65s - loss: 0.0388 - precision: 0.9856 - recall: 0.9850 - f1: 0.9853 - acc: 0.9856 - val_loss: 0.0629 - val_precision: 0.9844 - val_recall: 0.9844 - val_f1: 0.9844 - val_acc: 0.9844\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.03070\n",
            "Epoch 8/100\n",
            " - 65s - loss: 0.0272 - precision: 0.9934 - recall: 0.9934 - f1: 0.9934 - acc: 0.9934 - val_loss: 0.4630 - val_precision: 0.8464 - val_recall: 0.8464 - val_f1: 0.8464 - val_acc: 0.8464\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.03070\n",
            "Epoch 9/100\n",
            " - 65s - loss: 0.0392 - precision: 0.9868 - recall: 0.9868 - f1: 0.9868 - acc: 0.9868 - val_loss: 0.4284 - val_precision: 0.8776 - val_recall: 0.8776 - val_f1: 0.8776 - val_acc: 0.8776\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.03070\n",
            "Epoch 10/100\n",
            " - 65s - loss: 0.0400 - precision: 0.9850 - recall: 0.9850 - f1: 0.9850 - acc: 0.9850 - val_loss: 0.4090 - val_precision: 0.8594 - val_recall: 0.8594 - val_f1: 0.8594 - val_acc: 0.8594\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.03070\n",
            "Epoch 11/100\n",
            " - 66s - loss: 0.0427 - precision: 0.9856 - recall: 0.9844 - f1: 0.9850 - acc: 0.9844 - val_loss: 5.8492 - val_precision: 0.4635 - val_recall: 0.4635 - val_f1: 0.4635 - val_acc: 0.4635\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.03070\n",
            "Epoch 12/100\n",
            " - 66s - loss: 0.0261 - precision: 0.9910 - recall: 0.9898 - f1: 0.9904 - acc: 0.9904 - val_loss: 1.6519 - val_precision: 0.6641 - val_recall: 0.6641 - val_f1: 0.6641 - val_acc: 0.6641\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.03070\n",
            "Epoch 13/100\n",
            " - 65s - loss: 0.0218 - precision: 0.9904 - recall: 0.9904 - f1: 0.9904 - acc: 0.9904 - val_loss: 0.0296 - val_precision: 0.9948 - val_recall: 0.9922 - val_f1: 0.9935 - val_acc: 0.9948\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.03070 to 0.02955, saving model to BRAIN_TUMOR_FOLD_4.h5\n",
            "Epoch 14/100\n",
            " - 65s - loss: 0.0389 - precision: 0.9856 - recall: 0.9850 - f1: 0.9853 - acc: 0.9856 - val_loss: 0.3487 - val_precision: 0.9162 - val_recall: 0.9115 - val_f1: 0.9138 - val_acc: 0.9141\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.02955\n",
            "Epoch 15/100\n",
            " - 65s - loss: 0.0270 - precision: 0.9904 - recall: 0.9898 - f1: 0.9901 - acc: 0.9904 - val_loss: 0.0847 - val_precision: 0.9581 - val_recall: 0.9505 - val_f1: 0.9543 - val_acc: 0.9583\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.02955\n",
            "Epoch 16/100\n",
            " - 65s - loss: 0.0173 - precision: 0.9934 - recall: 0.9934 - f1: 0.9934 - acc: 0.9934 - val_loss: 0.3058 - val_precision: 0.9010 - val_recall: 0.9010 - val_f1: 0.9010 - val_acc: 0.9010\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.02955\n",
            "Epoch 17/100\n",
            " - 65s - loss: 0.0260 - precision: 0.9916 - recall: 0.9916 - f1: 0.9916 - acc: 0.9916 - val_loss: 1.1144 - val_precision: 0.7594 - val_recall: 0.7552 - val_f1: 0.7573 - val_acc: 0.7552\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.02955\n",
            "Epoch 18/100\n",
            " - 65s - loss: 0.0289 - precision: 0.9922 - recall: 0.9916 - f1: 0.9919 - acc: 0.9922 - val_loss: 0.4685 - val_precision: 0.8802 - val_recall: 0.8802 - val_f1: 0.8802 - val_acc: 0.8802\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.02955\n",
            "Epoch 19/100\n",
            " - 65s - loss: 0.0217 - precision: 0.9922 - recall: 0.9916 - f1: 0.9919 - acc: 0.9922 - val_loss: 0.7420 - val_precision: 0.8177 - val_recall: 0.8177 - val_f1: 0.8177 - val_acc: 0.8177\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.02955\n",
            "Epoch 20/100\n",
            " - 65s - loss: 0.0238 - precision: 0.9904 - recall: 0.9892 - f1: 0.9898 - acc: 0.9898 - val_loss: 0.2732 - val_precision: 0.9375 - val_recall: 0.9375 - val_f1: 0.9375 - val_acc: 0.9375\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.02955\n",
            "Epoch 21/100\n",
            " - 66s - loss: 0.0150 - precision: 0.9946 - recall: 0.9946 - f1: 0.9946 - acc: 0.9946 - val_loss: 0.1345 - val_precision: 0.9556 - val_recall: 0.9505 - val_f1: 0.9530 - val_acc: 0.9557\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.02955\n",
            "Epoch 22/100\n",
            " - 65s - loss: 0.0162 - precision: 0.9940 - recall: 0.9934 - f1: 0.9937 - acc: 0.9934 - val_loss: 0.2566 - val_precision: 0.9216 - val_recall: 0.9193 - val_f1: 0.9204 - val_acc: 0.9219\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.02955\n",
            "Epoch 23/100\n",
            " - 65s - loss: 0.0791 - precision: 0.9760 - recall: 0.9760 - f1: 0.9760 - acc: 0.9760 - val_loss: 2.7706 - val_precision: 0.6354 - val_recall: 0.6354 - val_f1: 0.6354 - val_acc: 0.6354\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.02955\n",
            "Epoch 24/100\n",
            " - 66s - loss: 0.0399 - precision: 0.9850 - recall: 0.9850 - f1: 0.9850 - acc: 0.9850 - val_loss: 1.1692 - val_precision: 0.7786 - val_recall: 0.7786 - val_f1: 0.7786 - val_acc: 0.7786\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.02955\n",
            "Epoch 25/100\n",
            " - 66s - loss: 0.0360 - precision: 0.9886 - recall: 0.9886 - f1: 0.9886 - acc: 0.9886 - val_loss: 0.6301 - val_precision: 0.8073 - val_recall: 0.8073 - val_f1: 0.8073 - val_acc: 0.8073\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.02955\n",
            "Epoch 26/100\n",
            " - 65s - loss: 0.0440 - precision: 0.9843 - recall: 0.9838 - f1: 0.9841 - acc: 0.9844 - val_loss: 0.9483 - val_precision: 0.7155 - val_recall: 0.7135 - val_f1: 0.7145 - val_acc: 0.7135\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.02955\n",
            "Epoch 27/100\n",
            " - 65s - loss: 0.0227 - precision: 0.9934 - recall: 0.9910 - f1: 0.9922 - acc: 0.9928 - val_loss: 0.4636 - val_precision: 0.8480 - val_recall: 0.8438 - val_f1: 0.8458 - val_acc: 0.8464\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.02955\n",
            "Epoch 28/100\n",
            " - 65s - loss: 0.0162 - precision: 0.9946 - recall: 0.9946 - f1: 0.9946 - acc: 0.9946 - val_loss: 0.3671 - val_precision: 0.8997 - val_recall: 0.8880 - val_f1: 0.8938 - val_acc: 0.8880\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.02955\n",
            "Epoch 29/100\n",
            " - 66s - loss: 0.0487 - precision: 0.9796 - recall: 0.9790 - f1: 0.9793 - acc: 0.9790 - val_loss: 1.0299 - val_precision: 0.7604 - val_recall: 0.7604 - val_f1: 0.7604 - val_acc: 0.7604\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.02955\n",
            "Epoch 30/100\n",
            " - 65s - loss: 0.0315 - precision: 0.9892 - recall: 0.9892 - f1: 0.9892 - acc: 0.9892 - val_loss: 0.5277 - val_precision: 0.8873 - val_recall: 0.8828 - val_f1: 0.8850 - val_acc: 0.8828\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.02955\n",
            "Epoch 31/100\n",
            " - 65s - loss: 0.0453 - precision: 0.9831 - recall: 0.9814 - f1: 0.9823 - acc: 0.9820 - val_loss: 2.2653 - val_precision: 0.6598 - val_recall: 0.6562 - val_f1: 0.6580 - val_acc: 0.6615\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.02955\n",
            "Epoch 32/100\n",
            " - 65s - loss: 0.0444 - precision: 0.9868 - recall: 0.9862 - f1: 0.9865 - acc: 0.9862 - val_loss: 0.2033 - val_precision: 0.9297 - val_recall: 0.9297 - val_f1: 0.9297 - val_acc: 0.9297\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.02955\n",
            "Epoch 33/100\n",
            " - 65s - loss: 0.0184 - precision: 0.9940 - recall: 0.9940 - f1: 0.9940 - acc: 0.9940 - val_loss: 0.0193 - val_precision: 0.9922 - val_recall: 0.9922 - val_f1: 0.9922 - val_acc: 0.9922\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.02955 to 0.01931, saving model to BRAIN_TUMOR_FOLD_4.h5\n",
            "Epoch 34/100\n",
            " - 65s - loss: 0.0216 - precision: 0.9940 - recall: 0.9934 - f1: 0.9937 - acc: 0.9934 - val_loss: 0.0354 - val_precision: 0.9844 - val_recall: 0.9844 - val_f1: 0.9844 - val_acc: 0.9844\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.01931\n",
            "Epoch 35/100\n",
            " - 65s - loss: 0.0222 - precision: 0.9928 - recall: 0.9928 - f1: 0.9928 - acc: 0.9928 - val_loss: 0.2706 - val_precision: 0.9009 - val_recall: 0.8984 - val_f1: 0.8996 - val_acc: 0.8984\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.01931\n",
            "Epoch 36/100\n",
            " - 65s - loss: 0.0187 - precision: 0.9922 - recall: 0.9922 - f1: 0.9922 - acc: 0.9922 - val_loss: 0.3257 - val_precision: 0.9089 - val_recall: 0.9089 - val_f1: 0.9089 - val_acc: 0.9089\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.01931\n",
            "Epoch 37/100\n",
            " - 65s - loss: 0.0343 - precision: 0.9886 - recall: 0.9886 - f1: 0.9886 - acc: 0.9886 - val_loss: 0.4400 - val_precision: 0.8609 - val_recall: 0.8542 - val_f1: 0.8575 - val_acc: 0.8568\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.01931\n",
            "Epoch 38/100\n",
            " - 66s - loss: 0.0258 - precision: 0.9898 - recall: 0.9892 - f1: 0.9895 - acc: 0.9898 - val_loss: 0.5668 - val_precision: 0.8483 - val_recall: 0.8438 - val_f1: 0.8460 - val_acc: 0.8438\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.01931\n",
            "Epoch 39/100\n",
            " - 65s - loss: 0.0214 - precision: 0.9916 - recall: 0.9916 - f1: 0.9916 - acc: 0.9916 - val_loss: 0.2454 - val_precision: 0.9062 - val_recall: 0.9062 - val_f1: 0.9062 - val_acc: 0.9062\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.01931\n",
            "Epoch 40/100\n",
            " - 66s - loss: 0.0169 - precision: 0.9952 - recall: 0.9952 - f1: 0.9952 - acc: 0.9952 - val_loss: 0.1168 - val_precision: 0.9661 - val_recall: 0.9635 - val_f1: 0.9648 - val_acc: 0.9635\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.01931\n",
            "Epoch 41/100\n",
            " - 65s - loss: 0.0135 - precision: 0.9952 - recall: 0.9952 - f1: 0.9952 - acc: 0.9952 - val_loss: 0.0606 - val_precision: 0.9948 - val_recall: 0.9896 - val_f1: 0.9921 - val_acc: 0.9896\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.01931\n",
            "Epoch 42/100\n",
            " - 65s - loss: 0.0133 - precision: 0.9958 - recall: 0.9958 - f1: 0.9958 - acc: 0.9958 - val_loss: 0.1342 - val_precision: 0.9659 - val_recall: 0.9609 - val_f1: 0.9634 - val_acc: 0.9609\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.01931\n",
            "Epoch 43/100\n",
            " - 66s - loss: 0.0179 - precision: 0.9958 - recall: 0.9958 - f1: 0.9958 - acc: 0.9958 - val_loss: 0.2208 - val_precision: 0.9505 - val_recall: 0.9505 - val_f1: 0.9505 - val_acc: 0.9505\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.01931\n",
            "Epoch 44/100\n",
            " - 66s - loss: 0.0326 - precision: 0.9874 - recall: 0.9874 - f1: 0.9874 - acc: 0.9874 - val_loss: 0.2213 - val_precision: 0.9239 - val_recall: 0.9141 - val_f1: 0.9189 - val_acc: 0.9245\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.01931\n",
            "Epoch 45/100\n",
            " - 66s - loss: 0.0187 - precision: 0.9940 - recall: 0.9940 - f1: 0.9940 - acc: 0.9940 - val_loss: 0.0507 - val_precision: 0.9922 - val_recall: 0.9922 - val_f1: 0.9922 - val_acc: 0.9922\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.01931\n",
            "Epoch 46/100\n",
            " - 66s - loss: 0.0227 - precision: 0.9940 - recall: 0.9940 - f1: 0.9940 - acc: 0.9940 - val_loss: 0.0108 - val_precision: 0.9974 - val_recall: 0.9974 - val_f1: 0.9974 - val_acc: 0.9974\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.01931 to 0.01078, saving model to BRAIN_TUMOR_FOLD_4.h5\n",
            "Epoch 47/100\n",
            " - 65s - loss: 0.0088 - precision: 0.9970 - recall: 0.9970 - f1: 0.9970 - acc: 0.9970 - val_loss: 0.0314 - val_precision: 0.9974 - val_recall: 0.9922 - val_f1: 0.9948 - val_acc: 0.9922\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.01078\n",
            "Epoch 48/100\n",
            " - 65s - loss: 0.0264 - precision: 0.9904 - recall: 0.9898 - f1: 0.9901 - acc: 0.9904 - val_loss: 1.0777 - val_precision: 0.7646 - val_recall: 0.7604 - val_f1: 0.7625 - val_acc: 0.7656\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.01078\n",
            "Epoch 49/100\n",
            " - 66s - loss: 0.0398 - precision: 0.9832 - recall: 0.9832 - f1: 0.9832 - acc: 0.9832 - val_loss: 0.5863 - val_precision: 0.8800 - val_recall: 0.8776 - val_f1: 0.8788 - val_acc: 0.8776\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.01078\n",
            "Epoch 50/100\n",
            " - 65s - loss: 0.0270 - precision: 0.9934 - recall: 0.9934 - f1: 0.9934 - acc: 0.9934 - val_loss: 0.2007 - val_precision: 0.9219 - val_recall: 0.9219 - val_f1: 0.9219 - val_acc: 0.9219\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.01078\n",
            "Epoch 51/100\n",
            " - 65s - loss: 0.0144 - precision: 0.9958 - recall: 0.9952 - f1: 0.9955 - acc: 0.9958 - val_loss: 0.6365 - val_precision: 0.8151 - val_recall: 0.8151 - val_f1: 0.8151 - val_acc: 0.8151\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.01078\n",
            "Epoch 52/100\n",
            " - 65s - loss: 0.0150 - precision: 0.9940 - recall: 0.9940 - f1: 0.9940 - acc: 0.9940 - val_loss: 0.2039 - val_precision: 0.9293 - val_recall: 0.9245 - val_f1: 0.9269 - val_acc: 0.9245\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.01078\n",
            "Epoch 53/100\n",
            " - 65s - loss: 0.0108 - precision: 0.9976 - recall: 0.9976 - f1: 0.9976 - acc: 0.9976 - val_loss: 0.1834 - val_precision: 0.9240 - val_recall: 0.9193 - val_f1: 0.9216 - val_acc: 0.9193\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.01078\n",
            "Epoch 54/100\n",
            " - 65s - loss: 0.0176 - precision: 0.9958 - recall: 0.9946 - f1: 0.9952 - acc: 0.9958 - val_loss: 0.0397 - val_precision: 0.9870 - val_recall: 0.9870 - val_f1: 0.9870 - val_acc: 0.9870\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.01078\n",
            "Epoch 55/100\n",
            " - 65s - loss: 0.0308 - precision: 0.9874 - recall: 0.9868 - f1: 0.9871 - acc: 0.9874 - val_loss: 0.4947 - val_precision: 0.8693 - val_recall: 0.8646 - val_f1: 0.8669 - val_acc: 0.8646\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.01078\n",
            "Epoch 56/100\n",
            " - 65s - loss: 0.0198 - precision: 0.9940 - recall: 0.9940 - f1: 0.9940 - acc: 0.9940 - val_loss: 2.5226 - val_precision: 0.6016 - val_recall: 0.6016 - val_f1: 0.6016 - val_acc: 0.6016\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.01078\n",
            "Epoch 57/100\n",
            " - 65s - loss: 0.0262 - precision: 0.9928 - recall: 0.9916 - f1: 0.9922 - acc: 0.9928 - val_loss: 0.4153 - val_precision: 0.8590 - val_recall: 0.8568 - val_f1: 0.8579 - val_acc: 0.8568\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.01078\n",
            "Epoch 58/100\n",
            " - 65s - loss: 0.0139 - precision: 0.9946 - recall: 0.9946 - f1: 0.9946 - acc: 0.9946 - val_loss: 0.0665 - val_precision: 0.9818 - val_recall: 0.9818 - val_f1: 0.9818 - val_acc: 0.9818\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.01078\n",
            "Epoch 59/100\n",
            " - 65s - loss: 0.0173 - precision: 0.9946 - recall: 0.9946 - f1: 0.9946 - acc: 0.9946 - val_loss: 0.0087 - val_precision: 0.9948 - val_recall: 0.9948 - val_f1: 0.9948 - val_acc: 0.9948\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.01078 to 0.00871, saving model to BRAIN_TUMOR_FOLD_4.h5\n",
            "Epoch 60/100\n",
            " - 65s - loss: 0.0177 - precision: 0.9946 - recall: 0.9946 - f1: 0.9946 - acc: 0.9946 - val_loss: 0.0319 - val_precision: 0.9948 - val_recall: 0.9948 - val_f1: 0.9948 - val_acc: 0.9948\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.00871\n",
            "Epoch 61/100\n",
            " - 65s - loss: 0.0386 - precision: 0.9861 - recall: 0.9850 - f1: 0.9856 - acc: 0.9856 - val_loss: 2.9677 - val_precision: 0.6354 - val_recall: 0.6354 - val_f1: 0.6354 - val_acc: 0.6354\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.00871\n",
            "Epoch 62/100\n",
            " - 65s - loss: 0.0314 - precision: 0.9886 - recall: 0.9886 - f1: 0.9886 - acc: 0.9886 - val_loss: 1.4491 - val_precision: 0.7107 - val_recall: 0.7057 - val_f1: 0.7082 - val_acc: 0.7057\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.00871\n",
            "Epoch 63/100\n",
            " - 65s - loss: 0.0195 - precision: 0.9922 - recall: 0.9916 - f1: 0.9919 - acc: 0.9922 - val_loss: 0.0533 - val_precision: 0.9766 - val_recall: 0.9766 - val_f1: 0.9766 - val_acc: 0.9766\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.00871\n",
            "Epoch 64/100\n",
            " - 65s - loss: 0.0216 - precision: 0.9934 - recall: 0.9928 - f1: 0.9931 - acc: 0.9928 - val_loss: 0.0323 - val_precision: 0.9974 - val_recall: 0.9974 - val_f1: 0.9974 - val_acc: 0.9974\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.00871\n",
            "Epoch 65/100\n",
            " - 65s - loss: 0.0200 - precision: 0.9928 - recall: 0.9928 - f1: 0.9928 - acc: 0.9928 - val_loss: 0.0043 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1: 1.0000 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00065: val_loss improved from 0.00871 to 0.00432, saving model to BRAIN_TUMOR_FOLD_4.h5\n",
            "Epoch 66/100\n",
            " - 65s - loss: 0.0062 - precision: 0.9988 - recall: 0.9988 - f1: 0.9988 - acc: 0.9988 - val_loss: 0.0663 - val_precision: 0.9870 - val_recall: 0.9870 - val_f1: 0.9870 - val_acc: 0.9870\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.00432\n",
            "Epoch 67/100\n",
            " - 65s - loss: 0.0182 - precision: 0.9928 - recall: 0.9928 - f1: 0.9928 - acc: 0.9928 - val_loss: 0.2245 - val_precision: 0.9193 - val_recall: 0.9193 - val_f1: 0.9193 - val_acc: 0.9193\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.00432\n",
            "Epoch 68/100\n",
            " - 65s - loss: 0.0133 - precision: 0.9940 - recall: 0.9940 - f1: 0.9940 - acc: 0.9940 - val_loss: 2.8040 - val_precision: 0.6094 - val_recall: 0.6094 - val_f1: 0.6094 - val_acc: 0.6094\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.00432\n",
            "Epoch 69/100\n",
            " - 65s - loss: 0.0167 - precision: 0.9934 - recall: 0.9934 - f1: 0.9934 - acc: 0.9934 - val_loss: 6.8985 - val_precision: 0.3672 - val_recall: 0.3672 - val_f1: 0.3672 - val_acc: 0.3672\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.00432\n",
            "Epoch 70/100\n",
            " - 65s - loss: 0.0191 - precision: 0.9934 - recall: 0.9928 - f1: 0.9931 - acc: 0.9928 - val_loss: 2.8718 - val_precision: 0.6077 - val_recall: 0.6016 - val_f1: 0.6046 - val_acc: 0.6016\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.00432\n",
            "Epoch 71/100\n",
            " - 65s - loss: 0.0243 - precision: 0.9916 - recall: 0.9916 - f1: 0.9916 - acc: 0.9916 - val_loss: 0.6561 - val_precision: 0.8177 - val_recall: 0.8177 - val_f1: 0.8177 - val_acc: 0.8177\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.00432\n",
            "Epoch 72/100\n",
            " - 65s - loss: 0.0334 - precision: 0.9898 - recall: 0.9898 - f1: 0.9898 - acc: 0.9898 - val_loss: 0.6284 - val_precision: 0.8212 - val_recall: 0.8151 - val_f1: 0.8181 - val_acc: 0.8177\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.00432\n",
            "Epoch 73/100\n",
            " - 65s - loss: 0.0237 - precision: 0.9904 - recall: 0.9904 - f1: 0.9904 - acc: 0.9904 - val_loss: 0.0752 - val_precision: 0.9818 - val_recall: 0.9818 - val_f1: 0.9818 - val_acc: 0.9818\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.00432\n",
            "Epoch 74/100\n",
            " - 65s - loss: 0.0128 - precision: 0.9958 - recall: 0.9952 - f1: 0.9955 - acc: 0.9952 - val_loss: 0.0386 - val_precision: 0.9974 - val_recall: 0.9974 - val_f1: 0.9974 - val_acc: 0.9974\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.00432\n",
            "Epoch 75/100\n",
            " - 65s - loss: 0.0126 - precision: 0.9958 - recall: 0.9958 - f1: 0.9958 - acc: 0.9958 - val_loss: 0.0395 - val_precision: 0.9896 - val_recall: 0.9896 - val_f1: 0.9896 - val_acc: 0.9896\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.00432\n",
            "Epoch 76/100\n",
            " - 65s - loss: 0.0104 - precision: 0.9964 - recall: 0.9964 - f1: 0.9964 - acc: 0.9964 - val_loss: 0.0115 - val_precision: 0.9922 - val_recall: 0.9922 - val_f1: 0.9922 - val_acc: 0.9922\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.00432\n",
            "Epoch 77/100\n",
            " - 65s - loss: 0.0174 - precision: 0.9940 - recall: 0.9928 - f1: 0.9934 - acc: 0.9934 - val_loss: 0.0510 - val_precision: 0.9844 - val_recall: 0.9844 - val_f1: 0.9844 - val_acc: 0.9844\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.00432\n",
            "Epoch 78/100\n",
            " - 65s - loss: 0.0170 - precision: 0.9940 - recall: 0.9940 - f1: 0.9940 - acc: 0.9940 - val_loss: 0.0418 - val_precision: 0.9896 - val_recall: 0.9896 - val_f1: 0.9896 - val_acc: 0.9896\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.00432\n",
            "Epoch 79/100\n",
            " - 65s - loss: 0.0093 - precision: 0.9970 - recall: 0.9970 - f1: 0.9970 - acc: 0.9970 - val_loss: 0.0254 - val_precision: 0.9870 - val_recall: 0.9870 - val_f1: 0.9870 - val_acc: 0.9870\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.00432\n",
            "Epoch 80/100\n",
            " - 65s - loss: 0.0136 - precision: 0.9940 - recall: 0.9940 - f1: 0.9940 - acc: 0.9940 - val_loss: 0.0179 - val_precision: 0.9948 - val_recall: 0.9922 - val_f1: 0.9935 - val_acc: 0.9948\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.00432\n",
            "Epoch 81/100\n",
            " - 65s - loss: 0.0092 - precision: 0.9970 - recall: 0.9964 - f1: 0.9967 - acc: 0.9970 - val_loss: 0.1647 - val_precision: 0.9557 - val_recall: 0.9531 - val_f1: 0.9544 - val_acc: 0.9557\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.00432\n",
            "Epoch 82/100\n",
            " - 65s - loss: 0.0206 - precision: 0.9922 - recall: 0.9922 - f1: 0.9922 - acc: 0.9922 - val_loss: 0.4936 - val_precision: 0.8509 - val_recall: 0.8464 - val_f1: 0.8486 - val_acc: 0.8464\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.00432\n",
            "Epoch 83/100\n",
            " - 65s - loss: 0.0297 - precision: 0.9898 - recall: 0.9892 - f1: 0.9895 - acc: 0.9892 - val_loss: 0.3375 - val_precision: 0.9349 - val_recall: 0.9349 - val_f1: 0.9349 - val_acc: 0.9349\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.00432\n",
            "Epoch 84/100\n",
            " - 65s - loss: 0.0322 - precision: 0.9868 - recall: 0.9868 - f1: 0.9868 - acc: 0.9868 - val_loss: 2.0545 - val_precision: 0.7083 - val_recall: 0.7083 - val_f1: 0.7083 - val_acc: 0.7083\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.00432\n",
            "Epoch 85/100\n",
            " - 65s - loss: 0.0393 - precision: 0.9868 - recall: 0.9856 - f1: 0.9862 - acc: 0.9862 - val_loss: 0.1224 - val_precision: 0.9505 - val_recall: 0.9505 - val_f1: 0.9505 - val_acc: 0.9505\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.00432\n",
            "Epoch 86/100\n",
            " - 65s - loss: 0.0234 - precision: 0.9922 - recall: 0.9922 - f1: 0.9922 - acc: 0.9922 - val_loss: 0.3527 - val_precision: 0.8750 - val_recall: 0.8750 - val_f1: 0.8750 - val_acc: 0.8750\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.00432\n",
            "Epoch 87/100\n",
            " - 65s - loss: 0.0083 - precision: 0.9976 - recall: 0.9976 - f1: 0.9976 - acc: 0.9976 - val_loss: 0.1033 - val_precision: 0.9818 - val_recall: 0.9818 - val_f1: 0.9818 - val_acc: 0.9818\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.00432\n",
            "Epoch 88/100\n",
            " - 65s - loss: 0.0073 - precision: 0.9976 - recall: 0.9976 - f1: 0.9976 - acc: 0.9976 - val_loss: 0.0347 - val_precision: 0.9922 - val_recall: 0.9922 - val_f1: 0.9922 - val_acc: 0.9922\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.00432\n",
            "Epoch 89/100\n",
            " - 65s - loss: 0.0079 - precision: 0.9976 - recall: 0.9976 - f1: 0.9976 - acc: 0.9976 - val_loss: 0.0141 - val_precision: 0.9948 - val_recall: 0.9948 - val_f1: 0.9948 - val_acc: 0.9948\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.00432\n",
            "Epoch 90/100\n",
            " - 65s - loss: 0.0090 - precision: 0.9982 - recall: 0.9982 - f1: 0.9982 - acc: 0.9982 - val_loss: 0.0349 - val_precision: 0.9948 - val_recall: 0.9948 - val_f1: 0.9948 - val_acc: 0.9948\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.00432\n",
            "Epoch 91/100\n",
            " - 65s - loss: 0.0148 - precision: 0.9952 - recall: 0.9952 - f1: 0.9952 - acc: 0.9952 - val_loss: 0.0758 - val_precision: 0.9792 - val_recall: 0.9792 - val_f1: 0.9792 - val_acc: 0.9792\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.00432\n",
            "Epoch 92/100\n",
            " - 65s - loss: 0.0073 - precision: 0.9982 - recall: 0.9982 - f1: 0.9982 - acc: 0.9982 - val_loss: 0.1110 - val_precision: 0.9688 - val_recall: 0.9688 - val_f1: 0.9687 - val_acc: 0.9688\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.00432\n",
            "Epoch 93/100\n",
            " - 65s - loss: 0.0094 - precision: 0.9970 - recall: 0.9970 - f1: 0.9970 - acc: 0.9970 - val_loss: 0.0614 - val_precision: 0.9714 - val_recall: 0.9714 - val_f1: 0.9714 - val_acc: 0.9714\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.00432\n",
            "Epoch 94/100\n",
            " - 65s - loss: 0.0108 - precision: 0.9976 - recall: 0.9970 - f1: 0.9973 - acc: 0.9976 - val_loss: 0.9189 - val_precision: 0.7596 - val_recall: 0.7552 - val_f1: 0.7574 - val_acc: 0.7578\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.00432\n",
            "Epoch 95/100\n",
            " - 65s - loss: 0.0216 - precision: 0.9946 - recall: 0.9946 - f1: 0.9946 - acc: 0.9946 - val_loss: 4.8151 - val_precision: 0.5521 - val_recall: 0.5521 - val_f1: 0.5521 - val_acc: 0.5521\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.00432\n",
            "Epoch 96/100\n",
            " - 65s - loss: 0.0230 - precision: 0.9940 - recall: 0.9940 - f1: 0.9940 - acc: 0.9940 - val_loss: 0.4435 - val_precision: 0.8646 - val_recall: 0.8646 - val_f1: 0.8646 - val_acc: 0.8646\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.00432\n",
            "Epoch 97/100\n",
            " - 65s - loss: 0.0319 - precision: 0.9898 - recall: 0.9898 - f1: 0.9898 - acc: 0.9898 - val_loss: 0.3702 - val_precision: 0.9032 - val_recall: 0.8984 - val_f1: 0.9008 - val_acc: 0.8984\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.00432\n",
            "Epoch 98/100\n",
            " - 65s - loss: 0.0264 - precision: 0.9904 - recall: 0.9904 - f1: 0.9904 - acc: 0.9904 - val_loss: 0.0845 - val_precision: 0.9635 - val_recall: 0.9609 - val_f1: 0.9622 - val_acc: 0.9635\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.00432\n",
            "Epoch 99/100\n",
            " - 65s - loss: 0.0123 - precision: 0.9952 - recall: 0.9946 - f1: 0.9949 - acc: 0.9952 - val_loss: 0.0800 - val_precision: 0.9844 - val_recall: 0.9844 - val_f1: 0.9844 - val_acc: 0.9844\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.00432\n",
            "Epoch 100/100\n",
            " - 65s - loss: 0.0155 - precision: 0.9946 - recall: 0.9940 - f1: 0.9943 - acc: 0.9946 - val_loss: 0.0364 - val_precision: 0.9844 - val_recall: 0.9844 - val_f1: 0.9844 - val_acc: 0.9844\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.00432\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hII8huG94jZT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3I83S5TU4jZX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_test_data():\n",
        "    gen = DATASET(SHAPE, BATCH_SIZE, range(1), BASE_DIR, SEED, TRAIN_TEST_RATIO, augment=False).split_train_test(\"test\")\n",
        "                       \n",
        "    x = np.empty((len(gen[0]),)+SHAPE, dtype=np.float32)\n",
        "    y = np.empty((len(gen[1]), 3), dtype=np.float32)\n",
        "    \n",
        "    for ix, path in tqdm(enumerate(gen[0])):\n",
        "        img = np.array(Image.open(gen[0][ix]))\n",
        "        img = resize(img, SHAPE)\n",
        "\n",
        "        label = gen[1][ix]\n",
        "\n",
        "        x[ix] = img\n",
        "        y[ix] = label\n",
        "        \n",
        "    return x, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDnun27N4jZa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "139911a7-68f3-4cd4-c1b7-ad3790f8b8b3"
      },
      "source": [
        "x, y = get_test_data()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "920it [10:59,  1.02it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCjjF4vm4jZd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Threshold predictions with THRESH_VAL\n",
        "def threshold_arr(array):\n",
        "    # Get all value from array\n",
        "    # Compare calue with THRESH_VAL \n",
        "    # IF value >= THRESH_VAL. round to 1\n",
        "    # ELSE. round to 0\n",
        "    new_arr = []\n",
        "    for ix, val in enumerate(array):\n",
        "        loc = np.array(val).argmax(axis=0)\n",
        "        k = list(np.zeros((len(val)), dtype=np.float32))\n",
        "        k[loc]=1\n",
        "        new_arr.append(k)\n",
        "        \n",
        "    return np.array(new_arr, dtype=np.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIOdFm1Y4jZg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "a60dbc4d-d6cd-49ad-de0a-d96faa358d89"
      },
      "source": [
        "models = []\n",
        "for i in range(5):\n",
        "    model = load_model(\"BRAIN_TUMOR_FOLD_{}.h5\".format(i), custom_objects={'f1': f1, 'precision': precision, 'recall': recall})\n",
        "    print(model.evaluate(x, y, verbose=0))\n",
        "    models.append(model)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.16864661425352095, 0.9477208966794222, 0.9456521739130435, 0.9466700683469358, 0.9467391304347826]\n",
            "[0.18482328825022865, 0.9467391299164813, 0.9467391299164813, 0.9467390786046567, 0.9467391299164813]\n",
            "[0.14188210562519404, 0.9500000005183012, 0.9489130439965622, 0.9494478464126587, 0.9500000005183012]\n",
            "[0.12046627562655055, 0.9630084141441013, 0.9619565217391305, 0.9624740813089454, 0.9619565217391305]\n",
            "[0.1357678721294455, 0.9652173913043478, 0.9630434782608696, 0.9641131364780924, 0.9641304347826087]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4K00nrl4jZj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVq6AwMb4jZn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_confusion_matrix(cm,\n",
        "                          target_names,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=None,\n",
        "                          normalize=True):\n",
        "    \"\"\"\n",
        "    given a sklearn confusion matrix (cm), make a nice plot\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
        "\n",
        "    target_names: given classification classes such as [0, 1, 2]\n",
        "                  the class names, for example: ['high', 'medium', 'low']\n",
        "\n",
        "    title:        the text to display at the top of the matrix\n",
        "\n",
        "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
        "                  see http://matplotlib.org/examples/color/colormaps_reference.html\n",
        "                  plt.get_cmap('jet') or plt.cm.Blues\n",
        "\n",
        "    normalize:    If False, plot the raw numbers\n",
        "                  If True, plot the proportions\n",
        "\n",
        "    Usage\n",
        "    -----\n",
        "    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n",
        "                                                              # sklearn.metrics.confusion_matrix\n",
        "                          normalize    = True,                # show proportions\n",
        "                          target_names = y_labels_vals,       # list of names of the classes\n",
        "                          title        = best_estimator_name) # title of graph\n",
        "\n",
        "    Citiation\n",
        "    ---------\n",
        "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
        "\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    import numpy as np\n",
        "    import itertools\n",
        "\n",
        "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
        "    misclass = 1 - accuracy\n",
        "\n",
        "    if cmap is None:\n",
        "        cmap = plt.get_cmap('Blues')\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "\n",
        "    if target_names is not None:\n",
        "        tick_marks = np.arange(len(target_names))\n",
        "        plt.xticks(tick_marks, target_names, rotation=45)\n",
        "        plt.yticks(tick_marks, target_names)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "\n",
        "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        if normalize:\n",
        "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
        "                     horizontalalignment=\"center\",\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "        else:\n",
        "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
        "                     horizontalalignment=\"center\",\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
        "    plt.savefig(\"confusion matrix_best.jpg\", dpi=150)\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57JHUoqj4jZr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "55a39430-58e5-411e-94ec-09cfe1979874"
      },
      "source": [
        "y_preds = threshold_arr(models[4].predict(x, verbose=0))\n",
        "\n",
        "results = precision_recall_fscore_support(y, y_preds ,average='macro')\n",
        "acc = accuracy_score(y, y_preds)\n",
        "\n",
        "print(\"Accuracy: {}, F1_Score: {}, Precision: {}, Recall: {}\".format(acc, results[2], results[0], results[1]))\n",
        "print(\"\\n\")\n",
        "print(classification_report(y, y_preds))\n",
        "print(\"\\n\")\n",
        "cnf_matrix = confusion_matrix(y.argmax(axis=1), y_preds.argmax(axis=1))\n",
        "\n",
        "plot_confusion_matrix(cm           = cnf_matrix, \n",
        "                      normalize    = False,\n",
        "                      target_names = ['Meningioma', 'Glioma','Pituitary'],\n",
        "                      title        = \"Confusion Matrix\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-e82da69b6c15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthreshold_arr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprecision_recall_fscore_support\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_preds\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'macro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'threshold_arr' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOJCV5BwxVtK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "2863ac53-e7b2-4cbb-a20b-cf6628b30b91"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3JGZIxiyWDJ",
        "colab_type": "text"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "125HfcFS4jZu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2v7WuVR4jZw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import auc, roc_curve\n",
        "\n",
        "y_pred = models[4].predict(x, verbose=0)\n",
        "y_proba = []\n",
        "for ix, i in enumerate(y_pred.argmax(axis=1)):\n",
        "    if i==0:\n",
        "        y_proba.append(1-y_pred[ix][i])\n",
        "    else:\n",
        "        y_proba.append(y_pred[ix][i])\n",
        "        \n",
        "fpr, tpr, thresholds = roc_curve(y.argmax(axis=1), y_proba)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure()\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.plot(fpr, tpr, label='Brain Tumor Classification (AUC = {})'.format(round(roc_auc,3)))\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.savefig(\"ROC_best.jpg\", dpi=150)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWpEUz0e4jZz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}